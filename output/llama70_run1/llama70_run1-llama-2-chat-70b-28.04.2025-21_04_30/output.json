{
    "128538.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Text",
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "page_number": 6,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1804
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "start_index": 1802,
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1511.08308.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "start_index": 0,
                    "page_number": 4,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "start_index": 3600,
                    "filename": "128538.pdf",
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "page_number": 6,
                    "start_index": 1804,
                    "type": "Text",
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "start_index": 1802,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "page_number": 4
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1603.01354.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "type": "Text",
                    "start_index": 1804
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1702.02098.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1804,
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "start_index": 1802,
                    "filename": "128538.pdf",
                    "page_number": 4
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1703.06345.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "type": "Text",
                    "page_number": 4,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 1804,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "start_index": 1802,
                    "type": "Text",
                    "page_number": 4,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1709.04109.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "start_index": 3600,
                    "type": "Text"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "page_number": 6,
                    "type": "Text",
                    "start_index": 1804,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "page_number": 4
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1711.05568.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 0,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "start_index": 3600,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1804,
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "type": "Text",
                    "page_number": 4
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1804.09849.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "page_number": 6,
                    "type": "Text",
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1806.03489.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "start_index": 3600,
                    "type": "Text"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 1804,
                    "page_number": 6,
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "start_index": 1802,
                    "page_number": 4,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1808.08745.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "type": "Text",
                    "start_index": 1804
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "start_index": 1802,
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1809.08370.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 3600,
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "start_index": 1804,
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1811.05021.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "type": "Text"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "start_index": 1804,
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 1802,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1811.09242.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "type": "Text",
                    "page_number": 6,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "start_index": 1804,
                    "page_number": 6,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1812.09471.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "start_index": 0,
                    "page_number": 4,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "type": "Text",
                    "page_number": 6,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "page_number": 6,
                    "start_index": 1804,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "type": "Text",
                    "page_number": 4
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1812.10235.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 0,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "start_index": 1802,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1901.10430.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 1804,
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "start_index": 1802,
                    "page_number": 4,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1903.07785.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "start_index": 0,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "type": "Text",
                    "start_index": 1804
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1905.03197.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "type": "Text",
                    "start_index": 3600,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "start_index": 1804,
                    "filename": "128538.pdf",
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 1802,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1905.12598.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 3600,
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "start_index": 1804,
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1906.05012.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "start_index": 1804,
                    "page_number": 6,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1907.00390.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 3600,
                    "page_number": 6,
                    "type": "Text"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "type": "Text",
                    "page_number": 6,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1908.08345.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 3600,
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "type": "Text",
                    "page_number": 6,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "type": "Text",
                    "page_number": 4
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1909.02188.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "start_index": 1804
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1910.13461.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "start_index": 1802,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1911.09483.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "start_index": 1802,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1911.10390.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "start_index": 3600,
                    "type": "Text"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1912.08777.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "start_index": 3600,
                    "page_number": 6,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "page_number": 6,
                    "type": "Text",
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 1802,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "2001.04063.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "start_index": 0,
                    "page_number": 4,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "type": "Text",
                    "page_number": 6,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "start_index": 1802,
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "2002.03184.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "type": "Text",
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 3600,
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "2008.07772.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "page_number": 6,
                    "type": "Text",
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "start_index": 1802,
                    "page_number": 4,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "2020.emnlp-main.152.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 3600,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "start_index": 1802,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 4
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "2020.emnlp-main.523.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 3600,
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "page_number": 6,
                    "start_index": 1804,
                    "type": "Text",
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "2020.findings-emnlp.378.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "type": "Text",
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 1804,
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "start_index": 1802,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "2105.03654.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Text",
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 3600,
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "start_index": 1804,
                    "filename": "128538.pdf",
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Text",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "2203.16804.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "type": "Text",
                    "page_number": 4,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 1804
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "C18-1139.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 1804,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "D18-1105.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 3600,
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 1804
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "start_index": 1802,
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "D18-1279.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "start_index": 3600,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 1804,
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "D18-1523.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "start_index": 3600,
                    "type": "Text",
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1804,
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "start_index": 1802,
                    "type": "Text",
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "D19-1367.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "page_number": 6,
                    "type": "Text",
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 1804
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "K19-1036.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 3600,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "start_index": 1802,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "N19-1373.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1804,
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "start_index": 1802,
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "wang19f.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "start_index": 3600,
                    "page_number": 6,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 1804
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "page_number": 4
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    }
}