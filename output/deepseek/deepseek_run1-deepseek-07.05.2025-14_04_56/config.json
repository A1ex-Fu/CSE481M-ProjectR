{
  "deployment_name": "local",
  "env_file_path": "config/env.json",
  "exp_id": "deepseek_run1",
  "few_shot_system_prompt": "\n    You are a highly accurate information extraction system. You are given snippets of text from research papers. Your task is to identify and extract tuples containing the name of the task addressed in the paper, utilized datasets, evaluation metrics, and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper, not baselines. Please use JSON format for each different tuple.\n\nExample format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}].\n\nYour answer will immediately start with the JSON object satisfying the given template and contain nothing else.\n\nHere are 5 examples to guide your extraction:\n\n**Example 1**\n\n**Input:**\n\n\"Table 1: Results on the WNUT 2016 dataset.\nModel  F1\nSystem A 52.41\nOurs 53.48\"\n\n**Output:**\n\n```json\n[{\"Task\": \"Named Entity Recognition\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": \"53.48\"}]\nExample 2\n\nInput:\n\n\"Table 4: POS tagging accuracy of our model on test data from WSJ proportion of PTB, together with top-performance systems. The neural net- work based models are marked with \u2021.\nModel Acc.\nToutanova et al. (2003) 97.27\nThis paper 97.55\"\n\nOutput:\n\nJSON\n\n[{\"Task\": \"POS Tagging\", \"Dataset\": \"WSJ\", \"Metric\": \"Accuracy\", \"Result\": \"97.55\"}]\nExample 3\n\nInput:\n\n\"Table 1: Results on WMT14 En\u2192Fr.\nThe num- bers before and after \u2019\u00b1\u2019 are the mean and stan- dard deviation of test BLEU score over an evalua- tion window.\nModel Test BLEU\n(Zhou et al. 2019) 53.43\nOurs 53.48\"\n\nOutput:\n\nJSON\n\n[{\"Task\": \"Machine Translation\", \"Dataset\": \"WMT14 En\u2192Fr\", \"Metric\": \"BLEU\", \"Result\": \"53.48\"}]\nExample 4\n\nInput:\n\n\"Table 5: Results of our models, with various feature sets, compared to other published results.\nModel CoNLL-2003 Prec. Recall F1\nBLSTM-CNN + emb + lex 91.39 91.85 91.62\"\n\nOutput:\n\nJSON\n\n[{\"Task\": \"Named Entity Recognition\", \"Dataset\": \"CoNLL-2003\", \"Metric\": \"F1\", \"Result\": \"91.62\"}]\nExample 5\n\nInput:\n\n\"Table 4: F1 score of models trained to predict document-at-a-time.\nModel F1\nBi-LSTM-CRF (sent) 90.43 \u00b1 0.12 \nID-CNN 90.65 \u00b1 0.15\"\n\nOutput:\n\nJSON\n\n[{\"Task\": \"Named Entity Recognition\", \"Dataset\": \"CoNLL\", \"Metric\": \"F1\", \"Result\": \"90.65\"}]\nNow, complete the extraction on the following input.\n\nInput:\n\n\"Table 4: POS tagging accuracy of our model on test data from WSJ proportion of PTB, together with top-performance systems. The neural net- work based models are marked with \u2021.\nModel Acc.\nCollobert et al. (2011)\u2021 97.29\nSantos and Zadrozny (2014)\u2021 97.32\nThis paper 97.55\"\n    ",
  "is_few_shot": false,
  "is_preprocessed_doc": true,
  "max_new_tokens": 2048,
  "model_path": "",
  "model_type": "deepseek",
  "model_version": "deepseek-r1",
  "output_path": "output/deepseek/deepseek_run1-deepseek-07.05.2025-14_04_56/",
  "papers_path": "paperDataset",
  "processed_docs_path": "processedPapers1-05.05.2025-18_38_20/processed_docs.pkl",
  "query_prompt": "Main task, datasets and evaluation metrics.",
  "seed": 42,
  "system_prompt": "You will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else."
}