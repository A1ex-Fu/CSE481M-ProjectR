{
    "1901.10430.pdf": {
        "normalized_output": [
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201914 EN-DE</s>",
                "Metric": "BLEU",
                "Result": "29.7"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201914 EN-FR",
                "Metric": "BLEU",
                "Result": "43.2"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "IWSLT\u201914 EN-DE</s>",
                "Metric": "BLEU",
                "Result": "35.2"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201917 EN-ZH",
                "Metric": "BLEU",
                "Result": "24.4"
            },
            {
                "Task": "Language Modeling",
                "Dataset": "Google Billion Word",
                "Metric": "Perplexity",
                "Result": "26.67"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-1</s>",
                "Result": "39.84"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-2</s>",
                "Result": "16.25"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-L",
                "Result": "36.73"
            }
        ],
        "source_documents": [
            {
                "content": "Machine Translation. We report results on four benchmarks: For WMT English to German (En- De) we replicate the setup of Vaswani et al. (2017), based on WMT\u201916 training data with 4.5M sen- tence pairs, we validate on newstest2013 and test on newstest2014.3 The vocabulary is a 32K joint source and target byte pair encoding (BPE; Sennrich et al. 2016). For WMT English to French (En- Fr), we borrow the setup of Gehring et al. (2017) with 36M training sentence pairs from WMT\u201914, validate on newstest2012+2013 and test on newstest2014. The 40K vocabulary is based on a joint source and target BPE factorization. For WMT English to Chinese (Zh-En), we pre-process the WMT\u201917 training data following Hassan et al. (2018) resulting in 20M sentence pairs. We develop on devtest2017 and test on newstest2017. For IWSLT\u201914 German-English (De-En) we replicate the setup of Edunov et al. (2018) for 160K training sentence pairs and 10K joint BPE vocabulary. For this benchmark only, data is lowercased. only we apply compound splitting similar to Vaswani et al. (2017). For WMT Zh-En we measure detokenized BLEU to be comparable to Hassan et al. (2018).5 We train three random initializations of a each con\ufb01guration and report test accuracy of the seed which resulted in the highest validation BLEU. Ablations are conducted on the validation set and we report the mean BLEU and standard deviation on this set. WMT En-De, WMT En-Fr are based on beam search with a beam width of 5, IWSLT uses beam 4, and WMT Zh-En beam 8 following Hassan et al. (2018). For all datasets, we tune a length penalty as well as the number of checkpoints to average on the validation set. Language Modeling. We evaluate on the large-scale Billion word dataset (Chelba et al., 2013) which contains 768M tokens and has a vocabulary of nearly 800K types. Sentences in this dataset are shuf\ufb02ed and we batch sentences independently of each other. Models are evaluated in terms of perplexity on the valid and test portions. Summarization.",
                "metadata": {
                    "filename": "1901.10430.pdf",
                    "type": "Text",
                    "start_index": 0,
                    "page_number": 5
                }
            },
            {
                "content": "nearly 800K types. Sentences in this dataset are shuf\ufb02ed and we batch sentences independently of each other. Models are evaluated in terms of perplexity on the valid and test portions. Summarization. We test the model\u2019s ability to process long documents on the CNN-DailyMail summarization task (Hermann et al., 2015; Nallapati et al., 2016) comprising over 280K news arti- cles paired with multi-sentence summaries. Articles are truncated to 400 tokens (See et al., 2017) and we use a BPE vocabulary of 30K types (Fan et al., 2017). We evaluate in terms of F1-Rouge, that is Rouge-1, Rouge-2 and Rouge-L (Lin, 2004).6 When generating summaries, we follow stan- dard practice in tuning the maximum output length, disallowing repeating the same trigram, and we apply a stepwise length penalty (Paulus et al., 2017; Fan et al., 2017; Wu et al., 2016). Translation. We use a dropout rate of 0.3 for WMT En-De and IWSLT De-En, 0.1 for WMT En- Fr, and 0.25 for WMT Zh-En. WMT models are optimized with Adam and a cosine learning rate schedule (Kingma & Ba, 2015; Loshchilov & Hutter, 2016) where the learning rate is \ufb01rst linearly warmed up for 10K steps from 10\u22127 to 10\u22123 and then annealed following a cosine rate with a single cycle. For IWSLT\u201914 De-En, we use a schedule based on the inverse square root of the current step (Vaswani et al., 2017). We train the WMT models on 8 NVIDIA V100 GPUs for a total of 30K steps on WMT En-De, 40K steps for WMT Zh-En and 80K steps for WMT En-Fr. For IWSLT De-En we train for 50K steps on a single GPU. We use \ufb02oating point 16 precision and accumulate the gradients for 16 batches before applying an update (Ott et al., 2018), except for IWSLT where we do not accumulate gradients. Batches contain up to 459K source tokens and the same number of target tokens for both WMT En-De and WMT Zh-En, 655K for En-Fr, and 4K for IWSLT De-En. We use label smoothing with 0.1 weight for the uniform prior distribution over the vocabulary (Szegedy et al., 2015; Pereyra et",
                "metadata": {
                    "start_index": 1801,
                    "filename": "1901.10430.pdf",
                    "page_number": 5,
                    "type": "Text"
                }
            },
            {
                "content": "REFERENCES",
                "metadata": {
                    "start_index": 0,
                    "type": "Text",
                    "filename": "1901.10430.pdf",
                    "page_number": 9
                }
            },
            {
                "content": "after the input projection. For comparison, we re-implemented averaged attention networks (AAN; Zhang et al. 2018) which compute a uniform average over past model states instead of a weighted average as in self-attention. Our re-implementation is ef\ufb01cient: we measure 129 sentences/sec for a base transformer-AAN on newstest2014 compared to 20 sentences/sec for Zhang et al. (2018). Table 3 shows that our models outperform this approach. Note that AANs still use self-attention in the encoder network while as our approach does away with self-attention both in the encoder and decoder. 6.3 LANGUAGE MODELING As second task we consider language modeling on the Billion word benchmark. The self-attention baseline has N = 16 blocks, each with a self-attention module and a feed-forward module using dff = 4096 and d = 1024. DynamicConv uses N = 17 blocks to assimilate the parameter count and we use kernel sizes 15x2, 31x4 and 63x11. Table 4 shows that DynamicConv achieves slightly better perplexity than our self-attention baseline which is very competitive.",
                "metadata": {
                    "type": "Text",
                    "page_number": 7,
                    "filename": "1901.10430.pdf",
                    "start_index": 1800
                }
            },
            {
                "content": "Table 1: Machine translation accuracy in terms of BLEU for WMT En-De and WMT En-Fr on newstest2014.\nModel Param (En-De) WMT En-De WMT En-Fr Gehring et al. (2017) 216M 25.2 40.5 Vaswani et al. (2017) 213M 28.4 41.0 Ahmed et al. (2017) 213M 28.9 41.4 Chen et al. (2018) 379M 28.5 41.0 Shaw et al. (2018) - 29.2 41.5 Ott et al. (2018) 210M 29.3 43.2 LightConv 202M 28.9 43.1 DynamicConv 213M 29.7 43.2",
                "metadata": {
                    "filename": "1901.10430.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Machine translation accuracy in terms of BLEU on IWSLT and WMT Zh-En.\nModel Param (Zh-En) IWSLT WMT Zh-En Deng et al. (2018) - 33.1 - Hassan et al. (2018) - - 24.2 Self-attention baseline 292M 34.4 23.8 LightConv 285M 34.8 24.3 DynamicConv 296M 35.2 24.4",
                "metadata": {
                    "filename": "1901.10430.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Ablation on WMT English-German newstest2013. (+) indicates that a result includes all preceding features. Speed results based on beam size 4, batch size 256 on an NVIDIA P100 GPU.\nModel Param BLEU Sent/sec Vaswani et al. (2017) 213M 26.4 - Self-attention baseline (k=inf, H=16) 210M 26.9 \u00b1 0.1 52.1 \u00b1 0.1 Self-attention baseline (k=3,7,15,31x3, H=16) 210M 26.9 \u00b1 0.3 54.9 \u00b1 0.2 CNN (k=3) 208M 25.9 \u00b1 0.2 68.1 \u00b1 0.3 CNN Depthwise (k=3, H=1024) 195M 26.1 \u00b1 0.2 67.1 \u00b1 1.0 + Increasing kernel (k=3,7,15,31x4, H=1024) 195M 26.4 \u00b1 0.2 63.3 \u00b1 0.1 + DropConnect (H=1024) 195M 26.5 \u00b1 0.2 63.3 \u00b1 0.1 + Weight sharing (H=16) 195M 26.5 \u00b1 0.1 63.7 \u00b1 0.4 + Softmax-normalized weights [LightConv] (H=16) 195M 26.6 \u00b1 0.2 63.6 \u00b1 0.1 + Dynamic weights [DynamicConv] (H=16) 200M 26.9 \u00b1 0.2 62.6 \u00b1 0.4 Note: DynamicConv(H=16) w/o softmax-normalization 200M diverges AAN decoder + self-attn encoder 260M 26.8 \u00b1 0.1 59.5 \u00b1 0.1 AAN decoder + AAN encoder 310M 22.5 \u00b1 0.1 59.2 \u00b1 2.1",
                "metadata": {
                    "filename": "1901.10430.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Model Param Valid Test 2-layer LSTM-8192-1024 (J\u00b4ozefowicz et al., 2016) \u2013 \u2013 30.6 Gated Convolutional Model (Dauphin et al., 2017) 428M \u2013 31.9 Mixture of Experts (Shazeer et al., 2017) 4371M \u2020 \u2013 28.0 Self-attention baseline 331M 26.67 26.73 DynamicConv 339M 26.60 26.67",
                "metadata": {
                    "filename": "1901.10430.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: Language modeling results on the Google Billion Word test set. \u2020does not include embedding and softmax layers\nModel Param Rouge-1 Rouge-2 Rouge-l LSTM (Paulus et al., 2017) - 38.30 14.81 35.49 CNN (Fan et al., 2017) - 39.06 15.38 35.77 Self-attention baseline 90M 39.26 15.98 36.35 LightConv 86M 39.52 15.97 36.51 DynamicConv 87M 39.84 16.25 36.73 RL (Celikyilmaz et al., 2018) - 41.69 19.47 37.92",
                "metadata": {
                    "filename": "1901.10430.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 6: Alternatives to softmax-normalization in DynamicConv on WMT English-German new- stest2013 (\u20ac = 10~\u00b0).\nMethod BLEU W (No normalization) diverges softmax(W) 26.9 + 0.2 o(W) 26.6 + 0.3 tanh(W) 25.6 + 0.2 wick diverges whe 26.8 + 0.2 power(W, 2) diverges abs(W) diverges wie: diverges wee 26.7 \u00a3 0.2",
                "metadata": {
                    "filename": "1901.10430.pdf",
                    "page_number": 13,
                    "type": "Table"
                }
            },
            {
                "content": "Table 7: Inference speed of non-autoregressive models and small decoder versions of DynamicConv on WMT English-German newstest2014. For some models, the decoding speed (sent/sec) is derived by taking the inverse of the sentence generation latency in the literature.\nModel (batch size = 1, beam size = 1) Param BLEU Sent/sec NAT (+ FT) (Gu et al., 2018) - 17.7 25.6 NAT (+ FT + NPD=10) (Gu et al., 2018) - 18.7 12.7 NAT (+ FT + NPD=100) (Gu et al., 2018) - 19.2 3.9 LT, Improved Semhash (Kaiser et al., 2018) - 19.8 9.5 IR idec = 1 (Lee et al., 2018) - 13.9 - IR idec = 2 (Lee et al., 2018) - 17.0 - IR idec = 5 (Lee et al., 2018) - 20.3 - IR idec = 10 (Lee et al., 2018) - 21.6 - IR Adaptive (Lee et al., 2018) - 21.5 - NART w/ hints (Li et al., 2019) - 21.1 38.5 NART w/ hints (B = 4, 9 candidates) (Li et al., 2019) - 25.2 22.7 ENAT Embedding Mapping (Guo et al., 2019) - 20.7 41.7 ENAT Embedding Mapping (rescoring 9 candidates) (Guo et al., 2019) - 24.3 20.4 Autoregressive (Gu et al., 2018) - 22.7 2.5 Autoregressive (Lee et al., 2018) - 23.8 - Transformer (Li et al., 2019) - 27.3 1.3 Transformer (Guo et al., 2019) - 27.4 1.6 DynamicConv (1-decoder layer (k=31)) 124M 26.1 15.2 DynamicConv (3-decoder layers (k=3,7,15)) 153M 27.7 7.2 DynamicConv (6-decoder layers (k=3,7,15,31,31,31)) 200M 28.5 3.9 Tok/sec - - - - 511.4 393.6 139.7 90.4 107.2 - - - - - 54.0 - - 423.0 202.3 110.9",
                "metadata": {
                    "filename": "1901.10430.pdf",
                    "page_number": 14,
                    "type": "Table"
                }
            }
        ]
    },
    "128538.pdf": {
        "normalized_output": [
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "WNUT-16 - English",
                "Metric": "F1</s>",
                "Result": "53.48"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "WNUT-17 - English",
                "Metric": "F1</s>",
                "Result": "50.52"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2003 - English",
                "Metric": "F-Score (F-S)",
                "Result": "93.76"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "Multimodal",
                "Metric": "F-Score (F-S)",
                "Result": "74.17"
            }
        ],
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector Cm by taking the sum of textual and visual context vectors as depicted in Figure 1 as given below: Cm = Ct + Cv Finally, the overall context vector Cm was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure 1 to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle over\ufb01t- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of \ufb01lters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential over\ufb01tting, we also apply an L2 weight regularization over convolutional and word-level BiLSTM layers with a rate of 10\u22123. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u2208 {10\u22123,10\u22122,10\u22121} and batch size \u2208 {10,20,50}. We then select the best model with the high- est F1-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our \ufb01nal",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 0
                }
            },
            {
                "content": "determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our \ufb01nal result. We used of\ufb01cial WNUT script to compute the results 2. Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs 2https://noisy-text.github.io/2017/emerging-rare-entities.html",
                "metadata": {
                    "page_number": 4,
                    "start_index": 1798,
                    "type": "Text",
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "in tweets. 1\u20134. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524\u20131534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from over\ufb01tting. Journal of Machine Learn- ing Research 15:1929\u20131958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138\u2013144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F., and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142\u2013147. von D\u00a8aniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166\u2013171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154\u20135163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3602,
                    "page_number": 6,
                    "type": "Text",
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "to, causing a considerable overlap with the geo-loc en- tity type. By contrast to earlier mentioned approaches, our system goes beyond the current state-of-the-art with 53.48% F1 score on WNUT 2016 without the need for external re- sources and additional data. Our benchmark with current systems is shown in Table 1. WNUT 2017 results. The challenge that WNUT 2017 data (Derczynski et al. 2017) poses is the more frequent use of punctuation (Derczynski et al. 2014). Often, part of the name is also a common word (e.g., Andrew little) and some location names are also common person names (e.g., Smith), thus, causing dif\ufb01culty (Derczynski et al. 2014). Entities like corporation, group, and product could easily be confused between each other. Current state-of-the-art, though do not rely on external resources, however, rely on the CRF based classi\ufb01er. For instance, Aguilar et al. (2019) emphasized the signi\ufb01cance of having a CRF prediction layer over softmax and reported an approximately 4% increase in F1 with CRF in comparison to a softmax. However, we found this claim not completely true since it may be dependent on the dataset. Consistent with our previous results, our model goes beyond the current state-of-the-art with an F1 score of 50.5% with- out relying on external resources and the CRF prediction layer. This improvement is not signi\ufb01cant; nevertheless, it gave us a consistent result. We also observe the improvement over hard to predict entities such as creative-work and product compared to the current state-of-the-art. Multimodal results. So far, we only consider the tex- tual context. In this experiment, we evaluate our multi- modal approach that considers the auxiliary context from the text\u2019s images. With segregated attention network, our model achieved an F1 score of 74.17% on this data which is a considerable improvement over the state-of-the-art model of (Zhang et al. 2018). In one of our experiments, we merged the textual and visual representation and observed",
                "metadata": {
                    "start_index": 0,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "page_number": 5
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von D\u00a8aniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1905.03197.pdf": {
        "normalized_output": [
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-L",
                "Result": "40.51"
            },
            {
                "Task": "Summarization",
                "Dataset": "Gigaword",
                "Metric": "ROGUE-L</s>",
                "Result": "35.75"
            },
            {
                "Task": "Question Answering",
                "Dataset": "CoQA",
                "Metric": "F1",
                "Result": "82.5"
            },
            {
                "Task": "Question Generation",
                "Dataset": "SQuAD 1.1",
                "Metric": "BLEU-4",
                "Result": "22.12"
            },
            {
                "Task": "Dialogue Generation</s>",
                "Dataset": "DSTC7",
                "Metric": "NIST-4",
                "Result": "2.67"
            }
        ],
        "source_documents": [
            {
                "content": "Appendix A Long Text Generation: A Case Study Our model can generate text samples using the left-to-right setting. We picked three text samples sampled from left to right using our model, as shown in Table 12. We use the top-40 truncating sampling strategy [32], and forbid duplicate 4-grams during generation. For each example, we sampled 10 times from the same input and we hand-picked the best one; as such, these samples should be considered to be better than the average model output. From the examples, we \ufb01nd that the model can produce \ufb02uent output with somewhat consistent contents which \ufb01ts the inputs\u2019 genres and topics. In the \ufb01rst example, given a modi\ufb01ed excerpt from the novel \u201c1984\u201d as input, the model\u2019s output is like a short paragraph in an fantasy novel; for the other two with input from Wikipedia, the model makes up \u201cfacts\u201d with plausible names, time stamps and events written in the Wikipedia style. Appendix B GLUE Benchmark As shown in Table 13, we summarize the data size and the evaluation metrics used for the General Language Understanding Evaluation (GLUE) benchmark.",
                "metadata": {
                    "filename": "1905.03197.pdf",
                    "start_index": 0,
                    "page_number": 13,
                    "type": "Text"
                }
            },
            {
                "content": "EM F1 UNILM QA Model (Section 3.2) + UNILM Generated Questions Table 9: Question generation based on UNILM improves question answering results on the Table 8: Question generation results on SQuAD. MTR is short for METEOR, and RG for ROUGE. Results in the groups use different data splits. SQuAD development set. Table 10: Response generation results. Div-1 and Div-2 indicate diversity of unigrams and bigrams, respectively. test sets, and keep the original development set. We also conduct experiments following the data split as in [51], which uses the reversed dev-test split. The question generation task is formulated as a sequence-to-sequence problem. The \ufb01rst segment is the concatenation of input passage and answer, while the second segment is the generated question. We \ufb01ne-tune UNILM on the training set for 10 epochs. We set batch size to 32, masking probability to 0.7, and learning rate to 2e-5. The rate of label smoothing is 0.1. The other hyper-parameters are the same as pre-training. During decoding, we truncate the input to 464 tokens by selecting a passage chunk which contains the answer. The evaluation metrics BLEU-4, METEOR, and ROUGE-L are computed by the same scripts as in [12]. The results3 are presented in Table 8. CorefNQG [11] is based on a sequence-to-sequence model with attention and a feature-rich encoder. MP-GSN [51] uses an attention-based sequence-to-sequence model with a gated self-attention encoder. SemQG [50] uses two semantics-enhanced rewards to regularize the generation. UNILM outperforms previous models and achieves a new state-of-the-art for question generation. Generated Questions Improve QA The question generation model can automatically harvest a large number of question-passage-answer examples from a text corpus. We show that the augmented data generated by question generation improves the question answering model. We generate \ufb01ve million answerable examples, and four million unanswerable examples by modifying the answerable ones. We",
                "metadata": {
                    "page_number": 8,
                    "type": "Text",
                    "filename": "1905.03197.pdf",
                    "start_index": 0
                }
            },
            {
                "content": "v arXiv i X r a Uni\ufb01ed Language Model Pre-training for Natural Language Understanding and Generation Li Dong\u2217 Nan Yang\u2217 Wenhui Wang\u2217 Furu Wei\u2217\u2020 Xiaodong Liu Yu Wang Jianfeng Gao Ming Zhou Hsiao-Wuen Hon Microsoft Research {lidong1,nanya,wenwan,fuwei}@microsoft.com {xiaodl,yuwan,jfgao,mingzhou,hon}@microsoft.com Abstract This paper presents a new UNI\ufb01ed pre-trained Language Model (UNILM) that can be \ufb01ne-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirec- tional, bidirectional, and sequence-to-sequence prediction. The uni\ufb01ed modeling is achieved by employing a shared Transformer network and utilizing speci\ufb01c self-attention masks to control what context the prediction conditions on. UNILM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UNILM achieves new state-of- the-art results on \ufb01ve natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https://github.com/microsoft/unilm. 1 Introduction Language model (LM) pre-training has substantially advanced the state of the art across a variety of natural language processing tasks [8, 29, 19, 31, 9, 1]. Pre-trained LMs learn contextualized text representations by predicting words based on their context using large amounts of text data, and can be \ufb01ne-tuned to adapt to downstream tasks. Different prediction tasks and training objectives have been used for pre-training LMs of different",
                "metadata": {
                    "start_index": 0,
                    "page_number": 1,
                    "type": "Text",
                    "filename": "1905.03197.pdf"
                }
            },
            {
                "content": "the \ufb01rst 40,000 steps and linear decay. The dropout rate is 0.1. The weight decay is 0.01. The batch size is 330. The pre-training procedure runs for about 770,000 steps. It takes about 7 hours for 10,000 steps using 8 Nvidia Telsa V100 32GB GPU cards with mixed precision training. 2.5 Fine-tuning on Downstream NLU and NLG Tasks For NLU tasks, we \ufb01ne-tune UNILM as a bidirectional Transformer encoder, like BERT. Take text classi\ufb01cation as an example. We use the encoding vector of [SOS] as the representation of input, denoted as hL 1 , and feed it to a randomly initialized softmax classi\ufb01er (i.e., the task-speci\ufb01c output 1WC), where WC \u2208 Rdh\u00d7C is layer), where the class probabilities are computed as softmax(hL a parameter matrix, and C the number of categories. We maximize the likelihood of the labeled training data by updating the parameters of the pre-trained LM and the added softmax classi\ufb01er. For NLG tasks, we take the sequence-to-sequence task as an example. The \ufb01ne-tuning procedure is similar to pre-training using the self-attention masks as in Section 2.3. Let S1 and S2 denote source and target sequences, respectively. We pack them together with special tokens, to form the input \u201c[SOS] S1 [EOS] S2 [EOS]\u201d. The model is \ufb01ne-tuned by masking some percentage of tokens in the target sequence at random, and learning to recover the masked words. The training objective is to maximize the likelihood of masked tokens given context. It is worth noting that [EOS], which marks the end of the target sequence, can also be masked during \ufb01ne-tuning, thus when this happens, the model learns when to emit [EOS] to terminate the generation process of the target sequence. 3 Experiments We have conducted experiments on both NLU (i.e., the GLUE benchmark, and extractive question answering) and NLG tasks (i.e., abstractive summarization, question generation, generative question answering, and dialog response generation). 3.1 Abstractive Summarization Automatic text summarization",
                "metadata": {
                    "filename": "1905.03197.pdf",
                    "start_index": 1800,
                    "type": "Text",
                    "page_number": 5
                }
            },
            {
                "content": "Table 1: Comparison between language model (LM) pre-training objectives.\nELMo GPT BERT UNILM Left-to-Right LM v v Right-to-Left LM v Bidirectional LM v Sequence-to-Sequence LM v v v",
                "metadata": {
                    "filename": "1905.03197.pdf",
                    "page_number": 2,
                    "type": "Table"
                }
            },
            {
                "content": "Network LM Objectives of Uni\ufb01ed Pre-training What Uni\ufb01ed LM Learns Example Downstream Tasks Bidirectional LM Bidirectional encoding GLUE benchmark Extractive question answering Unidirectional LM Unidirectional decoding Long text generation Unidirectional decoding Abstractive summarization Sequence-to-Sequence LM conditioned on Question generation bidirectional encoding Generative question answering",
                "metadata": {
                    "filename": "1905.03197.pdf",
                    "page_number": 2,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: Results on Gigaword abstractive summa-\nRG-1 RG-2 RG-L RG-1 RG-2 RG-L Extractive Summarization 10K Training Examples LEAD-3 40.42 17.62 36.67 Transformer [43] 10.97 2.23 10.42 Best Extractive [27] 43.25 20.24 39.63 MASS [39] 25.03 9.48 23.48 Abstractive Summarization UNILM 32.96 14.68 30.56 PGNet [37] 39.53 17.28 37.98 Full Training Set Bottom-Up [16] 41.22 18.68 38.34 OpenNMT [23] 36.73 17.86 33.68 S2S-ELMo [13] 41.56 18.94 38.47 Re3Sum [4] 37.04 19.03 34.46 UNILM 43.33 20.21 40.51 MASS [39] 37.66 18.53 34.89 Table 3: Evaluation results on CNN/DailyMail UNILM 38.45 19.45 35.75 summarization. Models in the \ufb01rst block are ex-",
                "metadata": {
                    "filename": "1905.03197.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "EM F1 F1 F1 RMR+ELMo [20] 71.4 73.7 DrQA+ELMo [35] 67.2 Seq2Seq [35] 27.5 78.9 81.8 BERTLARGE 82.7 PGNet [35] 45.4 UNILM 80.5 83.4 UNILM 84.9 UNILM 82.5",
                "metadata": {
                    "filename": "1905.03197.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "BLEU-4 MTR RG-L CorefNQG [11] SemQG [50] 15.16 18.37 19.12 22.65 - 46.68 UNILM 22.12 25.06 51.07 MP-GSN [51] 16.38 20.25 44.48 SemQG [50] UNILM 20.76 23.75 24.20 25.61 48.91 52.04",
                "metadata": {
                    "filename": "1905.03197.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 10: Response generation results. Div-1 and Div-2 indicate diversity of unigrams and bigrams, respectively.\nNIST-4 BLEU-4 METEOR Entropy-4 Div-1 Div-2 Avg len Best System in DSTC7 Shared Task 2.523 1.83 8.07 9.030 0.109 0.325 15.133 UNILM 2.669 4.39 8.27 9.195 0.120 0.391 14.807 Human Performance 2.650 3.13 8.31 10.445 0.167 0.670 18.76",
                "metadata": {
                    "filename": "1905.03197.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "CoLA SST-2 MRPC STS-B QQP MNLI-m/mm QNLI RTE WNLI AX Score MCC Acc F1 S Corr F1 Acc Acc Acc Acc Acc 45.4 91.3 82.3 80.0 70.3 82.1/81.4 87.4 56.0 53.4 29.8 94.9 89.3 86.5 72.1 86.7/85.9 92.7 70.1 65.1 39.6 61.1 94.5 90.0 87.7 71.7 87.0/85.9 92.7 70.9 65.1 38.4 72.8 80.5 80.8",
                "metadata": {
                    "filename": "1905.03197.pdf",
                    "page_number": 9,
                    "type": "Table"
                }
            },
            {
                "content": "Winston sat back. A Winston sat back. A sense of complete helplessness had descended sense of complete upon him. He was now helpless and unmoving. What would that helplessness had mean? To become a monk under her mother\u2019s rule. What would it descended upon him. mean for him to be put at the center of the most dangerous of all of demons, the demon known as Azrael? He would never know exactly what that was, or even how it would end, unless he could sense the strength of Azrael\u2019s mind and soul. It wasn\u2019t only the existence of his mother, it was all he knew, and every moment. He was trapped within her embrace, a trapped, torn, battered, crippling presence he only knew it would mean was eternity. And in the instant that he was lost his life changed forever, when Azraelis\u2019 spirit touched his mind, he was engulfed in this dark and mystical, so enchanting and beautiful force. (...) Bert is a yellow Bert is a yellow Muppet character on the long running children\u2019s Muppet character television show, Sesame Street. He would typically appear as a on the long running sidekick or as a villain in the show; he is usually portrayed as children\u2019s television a cowardly and untrustworthy \ufb01gure that tends to intrude on the show, Sesame Street. normal life of the street, such as \u201cThe Story of the Mole\u201d , \u201cThe Story from the Side\u201d, and \u201cBump the Bush\u201d. His voice was provided by Michael Combs. Bert was a regular cast member of Sesame Street, and his appearance was always played throughout the series by two of his regular cast members, Frank Laughner and Michael Combs. (...) Maurice Ravel (7 Maurice Ravel (7 March 1875 - 28 December 1937) was a French March 1875 - 28 composer, pianist and conductor. He was born in Paris, and studied December 1937) was piano with Eugene Massenet. In the early 1900s Ravel became a French composer, interested in the composition of music, and he composed much of pianist and conductor. his early music for the piano. Ravel\u2019s early work is marked by a marked sense of lyricism, but in the later years of his career he developed a strong preference for orchestral forms. His \ufb01rst work, \u201cle Grand Cours d\u2019 Auvergne\u201d, was composed in 1909 by Maurice Ravel, a student of Jules Massenet, and was published by A.F.A. in 1912. It was re-published in 1912 by the publisher J.S.D.M. de l\u2019Etablissement Musicale de la Musique Francaise. Ravel wrote the piano concerto \u201cla Tragedie et la Chanson Dans le Theatre des",
                "metadata": {
                    "filename": "1905.03197.pdf",
                    "page_number": 14,
                    "type": "Table"
                }
            },
            {
                "content": "Table 12: Text samples generated by our model using left-to-right generation.\nCorpus #Train/#Dev/#Test Metrics Single-Sentence Classi\ufb01cation CoLA (Acceptability) 8.5k/1k/1k Matthews corr SST-2 (Sentiment) 67k/872/1.8k Accuracy Pairwise Text Classi\ufb01cation MNLI (NLI) 393k/20k/20k Accuracy RTE (NLI) 2.5k/276/3k Accuracy QNLI (NLI) 108k/5.7k/5.7k Accuracy WNLI (NLI) 634/71/146 Accuracy QQP (Paraphrase) 364k/40k/391k F1 score MRPC (Paraphrase) 3.7k/408/1.7k F1 score Text Similarity STS-B (Similarity) 7k/1.5k/1.4k",
                "metadata": {
                    "filename": "1905.03197.pdf",
                    "page_number": 14,
                    "type": "Table"
                }
            }
        ]
    },
    "2020.emnlp-main.523.pdf": {
        "normalized_output": [
            {
                "Task": "Entity Typing",
                "Dataset": "Open Entity",
                "Metric": "F1",
                "Result": "78.2"
            },
            {
                "Task": "Relation Classification",
                "Dataset": "TACRED",
                "Metric": "F1",
                "Result": "72.7"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2003 - English",
                "Metric": "F1",
                "Result": "94.3"
            },
            {
                "Task": "Question Answering",
                "Dataset": "ReCoRD",
                "Metric": "Exact Match (EM)",
                "Result": "90.8"
            },
            {
                "Task": "Question Answering",
                "Dataset": "ReCoRD",
                "Metric": "F1",
                "Result": "91.4"
            },
            {
                "Task": "Question Answering",
                "Dataset": "SQuAD 1.1",
                "Metric": "Exact Match (EM)",
                "Result": "89.8"
            },
            {
                "Task": "Question Answering",
                "Dataset": "SQuAD 1.1",
                "Metric": "F1",
                "Result": "95.0"
            }
        ],
        "source_documents": [
            {
                "content": "Table 12: Common hyper-parameters used in our ex- periments. dataset is obtained from the LDC website.3 We compute the reported results using our code based on that of Zhang et al. (2019). B.3 CoNLL-2003 The CoNLL-2003 dataset comprises training, de- velopment, and test sets, containing 14,987, 3,466, and 3,684 sentences, respectively. Each sentence contains annotations of four entity types, namely person, location, organization, and miscellaneous. The dataset is downloaded from the relevant web- site.4 The reported results are computed using the conlleval script obtained from the website. We do not tune the hyper-parameters of the SQuAD dataset, and use the ones described in Liu et al. (2020). The hyper-parameters and other details, including the training time, number of GPUs used, and the best score on the development set, are shown in Table 11. For the other hyper-parameters, we simply follow Liu et al. (2020) (see Table 12). We optimize the model using AdamW with learn- ing rate warmup and linear decay of the learning rate. We also use early stopping based on perfor- mance on the development set. The details of the datasets used in our experiments are provided be- low. B.4 ReCoRD The ReCoRD dataset consists of 100,730 train- ing, 10,000 development, and 10,000 test ques- tions created based on 80,121 unique news articles. The dataset is obtained from the relevant website.5 We compute the performance on the development set using the of\ufb01cial evaluation script downloaded from the website. Performance on the test set is ob- tained by submitting our model to the leaderboard. B.5 SQuAD 1.1 B.1 Open Entity The Open Entity dataset used in Zhang et al. (2019) consists of training, development, and test sets, where each set contains 1,998 examples with labels of nine general entity types. The dataset is down- loaded from the website for Zhang et al. (2019).2 We compute the reported results using our code based on that of Zhang et al. (2019). The SQuAD 1.1 dataset contains",
                "metadata": {
                    "type": "Text",
                    "page_number": 12,
                    "start_index": 0,
                    "filename": "2020.emnlp-main.523.pdf"
                }
            },
            {
                "content": "points. Furthermore, it achieves a new state of the art on this competitive dataset by outperforming the previous state of the art reported in Baevski et al. (2019) by 0.8 F1 points. 4.4 Cloze-style Question Answering We evaluate our model on the ReCoRD dataset (Zhang et al., 2018a), a cloze-style QA dataset con- sisting of over 120K examples. An interesting char- acteristic of this dataset is that most of its questions cannot be solved without external knowledge. The following is an example question and its answer in the dataset: Question: According to claims in the suit, \u201cParts of \u2019Stairway to Heaven,\u2019 instantly recog- nizable to the music fans across the world, sound almost identical to signi\ufb01cant portions of \u2018X.\u201d\u2019 Answer: Taurus Given a question and a passage, the task is to \ufb01nd the entity mentioned in the passage that \ufb01ts the missing entity (denoted by X in the question above). In this dataset, annotations of entity spans (start and end positions) in a passage are provided, and the answer is contained in the provided entity spans one or multiple times. Following past work, we evaluate the models using exact match (EM) and token-level F1 on the development and test sets. Model We solve this task by assigning a relevance score to each entity in the pas- sage and selecting the entity with the high- est score as the answer. Following Liu et al. (2020), given a question q1,q2,...,qj, and a passage p1,p2,...,pl, the input word sequence is constructed as: [CLS]q1,q2,...,qj[SEP] [SEP]p1,p2,...,pl[SEP]. Further, we input [MASK] entities corresponding to the missing en- tity and all entities in the passage. We compute the relevance score of each entity in the passage using a linear classi\ufb01er with the concatenated representa- tion of the missing entity and the corresponding en- tity. We train the model using binary cross-entropy loss averaged over all entities in the passage, and select the entity with the highest score (logit) as the answer. Baselines DocQA+ELMo (Clark",
                "metadata": {
                    "filename": "2020.emnlp-main.523.pdf",
                    "page_number": 7,
                    "start_index": 0,
                    "type": "Text"
                }
            },
            {
                "content": "types. The dataset is down- loaded from the website for Zhang et al. (2019).2 We compute the reported results using our code based on that of Zhang et al. (2019). The SQuAD 1.1 dataset contains 87,599 training, 10,570 development, and 9,533 test questions cre- ated based on 536 Wikipedia articles. The dataset is downloaded from the relevant website.6 We com- pute performance on the development set using the of\ufb01cial evaluation script downloaded from the web- site. Performance on the test set is obtained by submitting our model to the leaderboard. B.2 TACRED The TACRED dataset contains 68,124 training ex- amples, 22,631 development examples, and 15,509 test examples with labels of their relation types. The total number of relation types is 42. The 2https://github.com/thunlp/ERNIE 3https://catalog.ldc.upenn.edu/ LDC2018T24 4https://www.clips.uantwerpen.be/ conll2003/ner 5https://sheng-z.github.io/ ReCoRD-explorer 6https://rajpurkar.github.io/ SQuAD-explorer",
                "metadata": {
                    "type": "Text",
                    "filename": "2020.emnlp-main.523.pdf",
                    "start_index": 1807,
                    "page_number": 12
                }
            },
            {
                "content": "Table 5: Results of extractive question answering on the SQuAD 1.1 dataset. Table 6: Ablation study of our entity representations. with these models using this set. To conduct a fair compassion with RoBERTa, we use the same model architecture and hyper-parameters as those of RoBERTa (Liu et al., 2020). Results The experimental results are presented in Table 5. LUKE outperforms our primary base- line, RoBERTa, by 0.9 EM points and 0.4 F1 points on the development set. Furthermore, it achieves a new state of the art on this competitive dataset by outperforming XLNet by 0.3 points both in terms of EM and F1. Note that XLNet uses a more so- phisticated model involving beam search than the other models considered here. 5 Analysis In this section, we provide a detailed analysis of LUKE by reporting three additional experiments. 5.1 Effects of Entity Representations To investigate how our entity representations in- \ufb02uence performance on downstream tasks, we per- form an ablation experiment by addressing NER on the CoNLL-2003 dataset and extractive QA on the SQuAD dataset without inputting any entities. In this setting, LUKE uses only the word sequence to compute the representation for each word. We ad- dress the tasks using the same model architectures as those for RoBERTa described in the correspond- ing sections. As shown in Table 6, this setting clearly degrades performance, i.e., 1.4 F1 points on the CoNLL-2003 dataset and 0.6 EM points on the SQuAD dataset, demonstrating the effectiveness of our entity representations on these two tasks. 5.2 Effects of Entity-aware Self-attention We conduct an ablation study of our entity-aware self-attention mechanism by comparing the perfor- mance of LUKE using our mechanism with that using the original mechanism of the transformer. As shown in Table 7, our entity-aware self-attention mechanism consistently outperforms the original mechanism across all tasks. Furthermore, we ob- serve signi\ufb01cant improvements on two kinds of tasks,",
                "metadata": {
                    "type": "Text",
                    "start_index": 0,
                    "filename": "2020.emnlp-main.523.pdf",
                    "page_number": 8
                }
            },
            {
                "content": "Table 1: Results of entity typing on the Open Entity dataset.\nName Prec. Rec. F1 UFET (Zhang et al., 2019) 77.4 60.6 68.0 BERT (Zhang et al., 2019) 76.4 71.0 73.6 ERNIE (Zhang et al., 2019) 78.4 72.9 75.6 KEPLER (Wang et al., 2019b) 77.2 74.2 75.7 KnowBERT (Peters et al., 2019) 78.6 73.7 76.1 K-Adapter (Wang et al., 2020) 79.3 75.8 77.5 RoBERTa (Wang et al., 2020) 77.6 75.0 76.2 LUKE 79.9 76.6 78.2",
                "metadata": {
                    "filename": "2020.emnlp-main.523.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Results of relation classi\ufb01cation on the TA- CRED dataset.\nName Prec. Rec. F1 BERT (Zhang et al., 2019) 67.2 64.8 66.0 C-GCN (Zhang et al., 2018b) 69.9 63.3 66.4 ERNIE (Zhang et al., 2019) 70.0 66.1 68.0 SpanBERT (Joshi et al., 2020) 70.8 70.9 70.8 MTB (Baldini Soares et al., 2019) - - 71.5 KnowBERT (Peters et al., 2019) 71.6 71.4 71.5 KEPLER (Wang et al., 2019b) 70.4 73.0 71.7 K-Adapter (Wang et al., 2020) 68.9 75.4 72.0 RoBERTa (Wang et al., 2020) 70.2 72.4 71.3 LUKE 70.4 75.1 72.7",
                "metadata": {
                    "filename": "2020.emnlp-main.523.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Results of named entity recognition on the CoNLL-2003 dataset.\nName F1 LSTM-CRF (Lample et al., 2016) 91.0 ELMo (Peters et al., 2018) 92.2 BERT (Devlin et al., 2019) 92.8 Akbik et al. (2018) 93.1 Baevski et al. (2019) 93.5 RoBERTa 92.4 LUKE 94.3",
                "metadata": {
                    "filename": "2020.emnlp-main.523.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: Results of cloze-style question answering on the ReCoRD dataset. All models except RoBERTa (en- semble) are based on a single model.\nName Dev EM Dev F1 Test EM Test F1 DocQA+ELMo (Zhang et al., 2018a) 44.1 45.4 45.4 46.7 BERT (Wang et al., 2019a) - - 71.3 72.0 XLNet+Veri\ufb01er (Li et al., 2019) 80.6 82.1 81.5 82.7 RoBERTa (Liu et al., 2020) 89.0 89.5 - - RoBERTa (ensemble) (Liu et al., 2020) - - 90.0 90.6 LUKE 90.8 91.4 90.6 91.2",
                "metadata": {
                    "filename": "2020.emnlp-main.523.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5: Results of extractive question answering on the SQuAD 1.1 dataset.\nName Dev EM Dev F1 Test EM Test F1 BERT (Devlin et al., 2019) 84.2 91.1 85.1 91.8 SpanBERT (Joshi et al., 2020) - - 88.8 94.6 XLNet (Yang et al., 2019) 89.0 94.5 89.9 95.1 ALBERT (Lan et al., 2020) 89.3 94.8 - - RoBERTa (Liu et al., 2020) 88.9 94.6 - - LUKE 89.8 95.0 90.2 95.4",
                "metadata": {
                    "filename": "2020.emnlp-main.523.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 6: Ablation study of our entity representations.\nName CoNLL-2003 (Test F1) SQuAD (Dev EM) SQuAD (Dev F1) LUKE w/o entity inputs 92.9 89.2 94.8 LUKE 94.3 89.8 95.0",
                "metadata": {
                    "filename": "2020.emnlp-main.523.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Open Entity (Test F1) TACRED (Test F1) CoNLL-2003 (Test F1) ReCoRD (Dev EM) ReCoRD (Dev F1) SQuAD (Dev EM) 77.9 72.2 94.1 90.1 90.7 89.2 78.2 72.7 94.3 90.8 91.4 89.8",
                "metadata": {
                    "filename": "2020.emnlp-main.523.pdf",
                    "page_number": 9,
                    "type": "Table"
                }
            },
            {
                "content": "Table 7: Ablation study of our entity-aware self-attention mechanism.\nName RoBERTa w/ extra training RoBERTa LUKE CoNLL-2003 (Test F1) 92.5 92.4 94.3 SQuAD (Dev EM) 89.1 88.9 89.8 SQuAD (Dev F1) 94.7 94.6 95.0",
                "metadata": {
                    "filename": "2020.emnlp-main.523.pdf",
                    "page_number": 9,
                    "type": "Table"
                }
            },
            {
                "content": "Table 9: Hyper-parameters used to pretrain LUKE.\nName Value Maximum word length 512 Batch size 2048 Peak learning rate le-5 Peak learning rate (first 100K steps) | 5e-4 Learning rate decay linear \u2018Warmup steps 2500 Mask probability for words 15% Mask probability for entities 15% Dropout 0.1 Weight decay 0.01 Gradient clipping none Adam (3; 0.9 Adam {32 0.999 Adam \u20ac le-6",
                "metadata": {
                    "filename": "2020.emnlp-main.523.pdf",
                    "page_number": 11,
                    "type": "Table"
                }
            },
            {
                "content": "Table 10: Hyper-parameters used for the extra pretrain- ing of RoBERTa on our Wikipedia corpus.\nName Value Maximum word length 512 Batch size 2048 Peak learning rate le-5 Learning rate decay linear Warmup steps 2500 Mask probability for words | 15% Dropout 0.1 Weight decay 0.01 Gradient clipping none Adam (3; 0.9 Adam 32 0.999 Adam \u20ac le-6",
                "metadata": {
                    "filename": "2020.emnlp-main.523.pdf",
                    "page_number": 11,
                    "type": "Table"
                }
            },
            {
                "content": "Table 11: Hyper-parameters and other details of our experiments.\nName Open Entity TACRED CoNLL-2003 ReCoRD SQuAD Learning rate 1e-5 1e-5 1e-5 1e-5 15e-6 Batch size 4 32 8 32 48 Training epochs 3 5 5 2 2 Training time 10min 190min 203min 92min 42min Number of GPUs 1 1 1 8 8 Dev score 78.5 F1 72.0 F1 97.1 F1 90.8 EM/91.4 F1 89.8 EM/95.0 F1",
                "metadata": {
                    "filename": "2020.emnlp-main.523.pdf",
                    "page_number": 12,
                    "type": "Table"
                }
            },
            {
                "content": "Table 12: Common hyper-parameters used in our ex- periments.\nName Value Maximum word length | 512 Learning rate decay linear Warmup ratio 0.06 Dropout 0.1 Weight decay 0.01 Gradient clipping none Adam 3; 0.9 \u201ccam Ba ee jam \u20ac ec",
                "metadata": {
                    "filename": "2020.emnlp-main.523.pdf",
                    "page_number": 12,
                    "type": "Table"
                }
            }
        ]
    },
    "1511.08308.pdf": {
        "normalized_output": [
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2003 - English",
                "Metric": "F1",
                "Result": "91.62 (\u00b1 0.33)"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "Ontonotes v5 - English",
                "Metric": "F1",
                "Result": "86.28 (\u00b1 0.26)"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2003 - English",
                "Metric": "Precision",
                "Result": "91.39"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2003 - English</s>",
                "Metric": "Recall",
                "Result": "91.85"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "Ontonotes v5 - English",
                "Metric": "Precision",
                "Result": "86.04"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "Ontonotes v5 - English",
                "Metric": "Recall",
                "Result": "86.53"
            }
        ],
        "source_documents": [
            {
                "content": "Table 6: F1 score results of BLSTM and BLSTM-CNN models with various additional features; emb = Collobert word embeddings, char = character type feature, caps = capitalization feature, lex = lexicon features. Note that starred results are repeated for ease of comparison. 3.1 Dataset Preprocessing 3.4 Hyper-parameter Optimization For all datasets, we performed the following pre- processing: We performed two rounds of hyper-parameter opti- mization and selected the best settings based on de- velopment set performance23. Table 3 shows the \ufb01- nal hyper-parameters, and Table 4 shows the dev set performance of the best models in each round. In addition, for the OntoNotes dataset, in order to handle the Date, Time, Money, Percent, Quantity, Ordinal, and Cardinal named en- tity tags, we split tokens before and after every digit. In the \ufb01rst round, we performed random search and selected the best hyper-parameters over the de- velopment set of the CoNLL-2003 data. We evalu- ated around 500 hyper-parameter settings. Then, we took the same settings and tuned the learning rate and epochs on the OntoNotes development set. 24 3.2 CoNLL 2003 Dataset The CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003) consists of newswire from the Reuters RCV1 corpus tagged with four types of named entities: location, organization, person, and miscellaneous. As the dataset is small compared to OntoNotes, we trained the model on both the train- ing and development sets after performing hyper- parameter optimization on the development set. For the second round, we performed independent hyper-parameter searches on each dataset using Op- tunity\u2019s implementation of particle swarm (Claesen et al., ), as there is some evidence that it is more ef\ufb01cient than random search (Clerc and Kennedy, 2002). We evaluated 500 hyper-parameter settings this round as well. As we later found out that train- ing fails occasionally (Section 3.5) as well as large variation from run to run, we ran the top 5 settings",
                "metadata": {
                    "type": "Text",
                    "start_index": 0,
                    "page_number": 7,
                    "filename": "1511.08308.pdf"
                }
            },
            {
                "content": "Table 5: Results of our models, with various feature sets, compared to other published results. The three sections are, in order, our models, published neural network models, and published non-neural network models. For the features, emb = Collobert word embeddings, caps = capitalization feature, lex = lexicon features from both SENNA and DBpedia lexicons. For F1 scores, standard deviations are in parentheses. Then, letting [y]T 1 be the true tag sequence, the sentence-level log-likelihood is obtained by normal- izing the above score over all possible tag-sequences [j]T 1 using a softmax: This objective function and its gradients can be ef- \ufb01ciently computed by dynamic programming (Col- lobert et al., 2011b). 2.6.4 Learning Algorithm Training is done by mini-batch stochastic gradi- ent descent (SGD) with a \ufb01xed learning rate. Each mini-batch consists of multiple sentences with the same number of tokens. We found applying dropout to the output nodes22 of each LSTM layer (Pham et al., 2014) was quite effective in reducing over\ufb01t- ting (Section 4.4). We explored other more sophis- ticated optimization algorithms such as momentum (Nesterov, 1983), AdaDelta (Zeiler, 2012), and RM- SProp (Hinton et al., 2012), and in preliminary ex- periments they did not improve upon plain SGD. At inference time, given neural network out- puts [fiz we use the Viterbi algorithm to find the tag sequence [i]/ that maximizes the score S((elf [T.0%. 3 Evaluation 2.6.3 Tagging Scheme The output tags are annotated with BIOES (which stand for Begin, Inside, Outside, End, Single, indicating the position of the token in the entity) as this scheme has been reported to outper- form others such as BIO (Ratinov and Roth, 2009). Evaluation was performed on the well-established CoNLL-2003 NER shared task dataset (Tjong Kim Sang and De Meulder, 2003) and the much larger but less-studied OntoNotes 5.0 dataset (Hovy et al., 2006; Pradhan et al., 2013). Table 2 gives an overview of these two different",
                "metadata": {
                    "filename": "1511.08308.pdf",
                    "start_index": 0,
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "dataset (Tjong Kim Sang and De Meulder, 2003) and the much larger but less-studied OntoNotes 5.0 dataset (Hovy et al., 2006; Pradhan et al., 2013). Table 2 gives an overview of these two different datasets. sion of the corpus with a different data split. 21Numbers taken from the original paper (Luo et al., 2015). While the precision, recall, and F1 scores are clearly inconsis- tent, it is unclear in which way they are incorrect. For each experiment, we report the average and standard deviation of 10 successful trials. 22Adding dropout to inputs seems to have an adverse effect.",
                "metadata": {
                    "filename": "1511.08308.pdf",
                    "page_number": 6,
                    "type": "Text",
                    "start_index": 1798
                }
            },
            {
                "content": "Table 7: F1 scores when the Collobert word vectors are replaced. We tried 50- and 300-dimensional random vec- tors (Random 50d, Random 300d); GloVe\u2019s released vec- tors trained on 6 billion words (GloVe 6B 50d, GloVe 6B 300d); Google\u2019s released 300-dimensional vectors trained on 100 billion words from Google News (Google 100B 300d); and 50-dimensional GloVe and word2vec skip-gram vectors that we trained on Wikipedia and Reuters RCV-1 (Our GloVe 50d, Our Skip-gram 50d). ber of epochs because we observed that the models did not exhibit overtraining and instead continued to slowly improve on the development set long af- ter reaching near 100% accuracy on the training set. In contrast, despite OntoNotes being much larger than CoNLL-2003, training for more than about 18 epochs causes performance on the development set to decline steadily due to over\ufb01tting. 3.5 Excluding Failed Trials On the CoNLL-2003 dataset, while BLSTM models completed training without dif\ufb01culty, the BLSTM- CNN models fail to converge around 5\u223c10% of the time depending on feature set. Similarly, on OntoNotes, 1.5% of trials fail. We found that using a lower learning rate reduces failure rate. We also tried clipping gradients and using AdaDelta and both of them were effective at eliminating such failures by themselves. AdaDelta, however, made training more expensive with no gain in model performance. In any case, for all experiments we excluded trials where the \ufb01nal F1 score on a subset of training data falls below a certain threshold, and continued to run trials until we obtained 10 successful ones. For CoNLL-2003, we excluded trials where the \ufb01nal F1 score on the development set was less than 95; there was no ambiguity in selecting the threshold as every trial scored either above 98 or below 90. For OntoNotes, the threshold was a F1 score of 80 on the last 5,000 sentences of the training set; every trial scored either above 80 or below 75. 3.6 Training and Tagging Speed On an Intel Xeon E5-2697",
                "metadata": {
                    "type": "Text",
                    "filename": "1511.08308.pdf",
                    "page_number": 8,
                    "start_index": 0
                }
            },
            {
                "content": "Table 1: Number of entries for each category in the SENNA lexicon and our DBpedia lexicon.\nCategory SENNA DBpedia Location 36,697 709,772 Miscellaneous 4,722 328,575 Organization 6,440 231,868 Person 123,283 1,074,363 Total 171,142 2,344,578",
                "metadata": {
                    "filename": "1511.08308.pdf",
                    "page_number": 3,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Dataset sizes in number of tokens (entities)\nDataset Train Dev Test CoNLL-2003 204,567 51,578 46,666 (23,499) (5,942) (5,648) OntoNotes 5.0 1,088,503 147,724 152,728 / CoNLL-2012 (81,828) (11,066) (11,257)",
                "metadata": {
                    "filename": "1511.08308.pdf",
                    "page_number": 3,
                    "type": "Table"
                }
            },
            {
                "content": "LOC - - - - - B I - S - - - - - S B B I S S S S ORG - - - - - B I B I I E B E - - - - - - S - -",
                "metadata": {
                    "filename": "1511.08308.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Hyper-parameter search space and \ufb01nal values used for all experiments\nHyper-parameter CoNLL-2003 (Round 2) OntoNotes 5.0 (Round 1) Final Range Final Range Convolution width 3 [3,7] 3 [3,9] CNN output size 53 [15,84] 20 [15,100] LSTM state size 275 [100,500] 200 [100,400]10 LSTM layers 1 [1,4] 2 [2,4] Learning rate 0.0105 [10\u22123,10\u22121.8] 0.008 [10\u22123.5,10\u22121.5] Epochs11 80 - 18 - Dropout12 0.68 [0.25,0.75] 0.63 [0,1] Mini-batch size 9 -13 9 [5,14]",
                "metadata": {
                    "filename": "1511.08308.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "Round CoNLL-2003 OntoNotes 5.0 1 2 93.82 (\u00b1 0.15) 94.03 (\u00b1 0.23) 84.57 (\u00b1 0.27) 84.47 (\u00b1 0.29)",
                "metadata": {
                    "filename": "1511.08308.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5: Results of our models, with various feature sets, compared to other published results. The three sections are, in order, our models, published neural network models, and published non-neural network models. For the features, emb = Collobert word embeddings, caps = capitalization feature, lex = lexicon features from both SENNA and DBpedia lexicons. For F1 scores, standard deviations are in parentheses.\nModel CoNLL-2003 Prec. Recall F1 OntoNotes 5.0 Prec. Recall F1 FFNN + emb + caps + lex 89.54 89.80 89.67 (\u00b1 0.24) 74.28 73.61 73.94 (\u00b1 0.43) BLSTM 80.14 72.81 76.29 (\u00b1 0.29) 79.68 75.97 77.77 (\u00b1 0.37) BLSTM-CNN 83.48 83.28 83.38 (\u00b1 0.20) 82.58 82.49 82.53 (\u00b1 0.40) BLSTM-CNN + emb 90.75 91.08 90.91 (\u00b1 0.20) 85.99 86.36 86.17 (\u00b1 0.22) BLSTM-CNN + emb + lex 91.39 91.85 91.62 (\u00b1 0.33) 86.04 86.53 86.28 (\u00b1 0.26) Collobert et al. (2011b) - - 88.67 - - - Collobert et al. (2011b) + lexicon - - 89.59 - - - Huang et al. (2015) - - 90.10 - - - Ratinov and Roth (2009)18 91.20 90.50 90.80 82.00 84.95 83.45 Lin and Wu (2009) - - 90.90 - - - Finkel and Manning (2009)19 - - - 84.04 80.86 82.42 Suzuki et al. (2011) - - 91.02 - - - Passos et al. (2014)20 - - 90.90 - - 82.24 Durrett and Klein (2014) - - - 85.22 82.89 84.04 Luo et al. (2015)21 91.50 91.40 91.20 - - -",
                "metadata": {
                    "filename": "1511.08308.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Features CoNLL BLSTM OntoNotes BLSTM-CNN CoNLL OntoNotes BLSTM-CNN + lex CoNLL OntoNotes none 76.29 (\u00b1 0.29) 77.77 (\u00b1 0.37) 83.38 (\u00b1 0.20) 82.53 (\u00b1 0.40) 87.77 (\u00b1 0.29) emb 88.23 (\u00b1 0.23) 82.72 (\u00b1 0.23) 90.91 (\u00b1 0.20) 86.17 (\u00b1 0.22) 91.62 (\u00b1 0.33) emb + caps 90.67 (\u00b1 0.16) 86.19 (\u00b1 0.25) 90.98 (\u00b1 0.18) 86.35 (\u00b1 0.28) 91.55 (\u00b1 0.19)* emb + caps + lex 91.43 (\u00b1 0.17) 86.21 (\u00b1 0.16) 91.55 (\u00b1 0.19)* 86.28 (\u00b1 0.32)* 91.55 (\u00b1 0.19)* emb + char - - 90.88 (\u00b1 0.48) 86.08 (\u00b1 0.40) 91.44 (\u00b1 0.23) emb + char + caps - - 90.88 (\u00b1 0.31) 86.41 (\u00b1 0.22) 91.48 (\u00b1 0.23)",
                "metadata": {
                    "filename": "1511.08308.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 7: F1 scores when the Collobert word vectors are replaced. We tried 50- and 300-dimensional random vec- tors (Random 50d, Random 300d); GloVe\u2019s released vec- tors trained on 6 billion words (GloVe 6B 50d, GloVe 6B 300d); Google\u2019s released 300-dimensional vectors trained on 100 billion words from Google News (Google 100B 300d); and 50-dimensional GloVe and word2vec skip-gram vectors that we trained on Wikipedia and Reuters RCV-1 (Our GloVe 50d, Our Skip-gram 50d).\nWord Embeddings CoNLL-2003 OntoNotes Random 50d 87.77 (\u00b1 0.29) 83.82 (\u00b1 0.19) Random 300d 87.84 (\u00b1 0.23) 83.76 (\u00b1 0.37) GloVe 6B 50d 91.09 (\u00b1 0.15) 86.25 (\u00b1 0.24) GloVe 6B 300d 90.71 (\u00b1 0.21) 86.26 (\u00b1 0.30) Google 100B 300d 90.60 (\u00b1 0.23) 85.34 (\u00b1 0.25) Collobert 50d 91.62 (\u00b1 0.33) 86.28 (\u00b1 0.26) Our GloVe 50d 91.41 (\u00b1 0.21) 86.24 (\u00b1 0.35) Our Skip-gram 50d 90.76 (\u00b1 0.23) 85.70 (\u00b1 0.29)",
                "metadata": {
                    "filename": "1511.08308.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 8: F1 score results with various dropout values. Models were trained using only the training set for each dataset. All other experiments use dropout = 0.68 for CoNLL-2003 and dropout = 0.63 for OntoNotes 5.0.\nDropout Dev CoNLL-2003 Test OntoNotes 5.0 Dev Test - 93.72 (\u00b1 0.10) 90.76 (\u00b1 0.22) 82.02 (\u00b1 0.49) 84.06 (\u00b1 0.50) 0.10 93.85 (\u00b1 0.18) 90.87 (\u00b1 0.31) 83.01 (\u00b1 0.39) 84.94 (\u00b1 0.25) 0.30 94.08 (\u00b1 0.17) 91.09 (\u00b1 0.18) 83.61 (\u00b1 0.32) 85.44 (\u00b1 0.33) 0.50 94.19 (\u00b1 0.18) 91.14 (\u00b1 0.35) 84.35 (\u00b1 0.23) 86.36 (\u00b1 0.28) 0.63 - - 84.47 (\u00b1 0.23) 86.29 (\u00b1 0.25) 0.68 94.31 (\u00b1 0.15) 91.23 (\u00b1 0.16) - - 0.70 94.31 (\u00b1 0.24) 91.17 (\u00b1 0.37) 84.56 (\u00b1 0.40) 86.17 (\u00b1 0.25) 0.90 94.17 (\u00b1 0.17) 90.67 (\u00b1 0.17) 81.38 (\u00b1 0.19) 82.16 (\u00b1 0.18)",
                "metadata": {
                    "filename": "1511.08308.pdf",
                    "page_number": 9,
                    "type": "Table"
                }
            },
            {
                "content": "Table 9: Comparison of lexicon and matching/encoding methods over the BLSTM-CNN model employing random embeddings and no other features. When using both lexicons, the best combination of matching and encoding is Exact-BIOES for SENNA and Partial-BIOES for DBpedia. Note that the SENNA lexicon already contains \u201cpartial entries\u201d so exact matching in that case is really just a more primitive form of partial matching.\nLexicon Matching Encoding CoNLL-2003 OntoNotes No lexicon - - 83.38 (\u00b1 0.20) 82.53 (\u00b1 0.40) SENNA Exact Exact YN BIOES 86.21 (\u00b1 0.39) 86.14 (\u00b1 0.48) 83.24 (\u00b1 0.33) 83.01 (\u00b1 0.52) Exact YN 84.93 (\u00b1 0.30) 83.15 (\u00b1 0.26) Exact BIOES 85.02 (\u00b1 0.23) 83.39 (\u00b1 0.39) DBpedia Partial YN 85.72 (\u00b1 0.45) 83.25 (\u00b1 0.33) Partial BIOES 86.18 (\u00b1 0.56) 83.97 (\u00b1 0.38) Collobert\u2019s method 85.01 (\u00b1 0.31) 83.24 (\u00b1 0.26) Both Best combination 87.77 (\u00b1 0.29) 83.82 (\u00b1 0.19)",
                "metadata": {
                    "filename": "1511.08308.pdf",
                    "page_number": 11,
                    "type": "Table"
                }
            },
            {
                "content": "Table 10: Per genre F1 scores on OntoNotes. BC = broadcast conversation, BN = broadcast news, MZ = magazine, NW = newswire, TC = telephone conversation, WB = blogs and newsgroups\nModel BC BN MZ NW TC WB Test set size (# tokens) 32,576 23,557 18,260 51,667 11,015 19,348 Test set size (# entities) 1,697 2,184 1,163 4,696 380 1,137 Finkel and Manning (2009) 78.66 87.29 82.45 85.50 67.27 72.56 Durrett and Klein (2014)38 78.88 87.39 82.46 87.60 72.68 76.17 BLSTM-CNN 81.26 86.87 79.94 85.27 67.82 72.11 BLSTM-CNN + emb 85.05 89.93 84.31 88.35 72.44 77.90 BLSTM-CNN + emb + lex 85.23 89.93 84.45 88.39 72.39 78.38",
                "metadata": {
                    "filename": "1511.08308.pdf",
                    "page_number": 12,
                    "type": "Table"
                }
            }
        ]
    },
    "D18-1523.pdf": {
        "normalized_output": [
            {
                "Task": "Word Sense Induction",
                "Dataset": "SemEval 2013 Task 13",
                "Metric": "Fuzzy normalized mutual information (FNMI)",
                "Result": "11.26 \u00b1 0.48"
            },
            {
                "Task": "Word Sense Induction",
                "Dataset": "SemEval 2013 Task 13",
                "Metric": "Fuzzy B-Cubed (FBC)",
                "Result": "57.49 \u00b1 0.23"
            },
            {
                "Task": "Word Sense Induction",
                "Dataset": "SemEval 2013 Task 13",
                "Metric": "AVG",
                "Result": "25.43 \u00b1 0.48"
            }
        ],
        "source_documents": [
            {
                "content": "References",
                "metadata": {
                    "start_index": 0,
                    "filename": "D18-1523.pdf",
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "Supplementary Material Statistics of the SemEval 2013 Task 13 Dataset SemEval 2013 Task 13 consists of 50 targets, each has a lemma and a part of speech (20 verbs, 20 nouns and 10 adjectives). We use the dataset only for evaluation. Most targets have around 100 la- beled instances (sentences containing a usage of the target in its designated part of speech together with one or more WordNet senses assigned by hu- man labeler). Exceptions are the targets of trace.n and book.v which have 37 and 22 labeled in- stances accordingly. Leaving out the two anoma- lous targets mentioned above we are left with 4605 instances from 48 targets: 19 verb, 19 noun and 10 adjective targets. We note that the small size of the dataset should make one cautious to draw quick conclusions, yet, our results seem to be consistent. Effect of the Choice of Number of Clusters An important statistic of the dataset is the num- ber of senses per target. The average number of senses per target in the dataset is 6.94 (stdev:2.71). Breaking down by part of speech, the means and standard deviations of target senses are: verbs: 5.90 (\u00b11.37), nouns: 7.32 (\u00b12.21), adjectives: 7.11 (\u00b13.54). In this work we follow this statis- tic and always look for 7 clusters. Figure 2 shows the accuracy as a function of the number of clus- ters. While 7 clusters indeed produces the highest scores, all numbers in the range 4 to 15 produce state-of-the-art results. We leave the selection of per-instance number of clusters to future work. Figure 2 also tells us our system is better at in- ducing senses for adjectives, at least according to task score. The Importance of Lemmatization The ablation results in the paper indicate that for verbs, using symmetric patterns without lemmati- zation yields poor results. We present the analysis the motivated our use of lemmatization. Consider the samples from the biLM with and without sym- metric patterns, for the instance It was when I was a high-school student that I became convinced",
                "metadata": {
                    "filename": "D18-1523.pdf",
                    "type": "Text",
                    "start_index": 0,
                    "page_number": 7
                }
            },
            {
                "content": "target word itself. Ta- ble 1 compares the context-only and symmetric- pattern substitutes for two senses of the word sound. 2.3 Representative Generation To perform fuzzy clustering, we follow AI-KU and associate each instance with k representatives, but deviate in the way the representatives are gen- erated. Specifically, each representative is a set of size 2\u20ac , containing \u00a2 samples from the forward distribution and @ samples from the backward dis- tribution. In the symmetric pattern case above, a plausible representative, assuming \u00a3 = 2, would be: {feel, sounds, sight, rhythm} where two words were predicted by each side LM. In this work, we use = 4 and k = 20. 2.4 Sense Clustering After obtaining k representatives for each of the n word instances, we cluster the nk representa- tives into distinct senses and translate this hard- clustering of representatives into a probabilistic clustering of the originating instances. Hard-clustering of representatives Let V be the vocabulary obtained from all the representa- tives. We associate each representative with a sparse |V | dimensional bag-of-features vector, and arrange the representatives into a nk \u00d7 |V | matrix M where each row corresponds to a representa- tive. We now cluster M\u2019s rows into senses. We found it is bene\ufb01cial to transform the matrix us- ing TF-IDF. Treating each representative as a doc- ument, TF-IDF reduces the weight of uninforma- tive words shared by many representatives. We use agglomerative clustering (cosine distance, av- erage linkage) and induce a \ufb01xed number of clus- ters.3 We use sklearn (Pedregosa et al., 2011) for both TF-IDF weighting and clustering. Inducing soft clustering over instances After clustering the representatives, we induce a soft- clustering over the instances by associating each instance j to sense i based on the proportion of representatives of j that are assigned to cluster i. 2.5 Additional Processing Lemmatization The WSI task is de\ufb01ned over lemmas, and some target",
                "metadata": {
                    "start_index": 1798,
                    "filename": "D18-1523.pdf",
                    "type": "Text",
                    "page_number": 3
                }
            },
            {
                "content": "numbers as well. Results Table 2 summarizes the results. Our system using symmetric patterns outperforms all other setups with an AVG score of 25.4, establish- ing a new state-of-the-art on the task. 3 Experiments and Results We evaluate our method on the SemEval 2013 Task 13 dataset (Jurgens and Klapaftis, 2013), containing 50 ambiguous words each with roughly 100 in-sentence instances, where each instance is soft-labeled with one or more WordNet senses. Experiment Protocol Due to the stochastic na- ture of the algorithm, we repeat each experiment 30 times and report the mean scores together with the standard deviation. Ablation and analysis We perform ablations to explore the contribution of the different compo- nents (Symmetric Patterns (SP), Lemmatization (LEM) and TF-IDF re-weighting). Figure (1) shows the results for the entire dataset (ALL, top), as well as broken-down by part-of-speech. All components are bene\ufb01cial and are needed for ob- taining the best performance in all cases. How- ever, their relative importance differs across parts- of-speech. Adjectives gain the most from the use of the dynamic symmetric patterns, while nouns gain the least. For verbs, the lemmatization is",
                "metadata": {
                    "filename": "D18-1523.pdf",
                    "page_number": 4,
                    "type": "Text",
                    "start_index": 1801
                }
            },
            {
                "content": "Table 1: Predicted substitutes for two senses of sound, for context-only and the symmetric-pattern approaches.\nContext Only Symmetric Pattern Forward dist. Backward dist. Forward dist. Backward dist. This is a sound idea, I like it. sad 0.02 bad 0.12 welcome 0.09 funny 0.10 great 0.02 good 0.09 practical 0.03 beautiful 0.05 huge 0.02 great 0.06 comprehensive 0.03 fun 0.04 very 0.02 wonderful 0.05 light 0.02 simple 0.04 lesson 0.02 nice 0.04 balanced 0.02 interesting 0.03 I liked the sound of the harpsichord idea 0.12 sounds 0.04 feel 0.15 sight 0.16 fact 0.07 version 0.03 felt 0.11 sounds 0.11 article 0.05 rhythm 0.03 thought 0.07 rhythm 0.04 guy 0.04 strings 0.03 smell 0.06 tone 0.03 concept 0.02 piece 0.03 sounds 0.05 noise 0.03",
                "metadata": {
                    "filename": "D18-1523.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Evaluation Results on the SemEval 2013 Task 13 Dataset. SW: Embeddings similarity based feature weighting. AAC: Extending instance sentences from their traced source. AUC: Adding similar sentences from the dataset originating corpus. We report our mean scores over 30 runs \u00b1 standard deviation\nModel FNMI FBC AVG Original task dataset Ours 11.26 \u00b1 0.48 57.49 \u00b1 0.23 25.43 \u00b1 0.48 MCC-S 7.62 55.6 20.58 Sense-Topic (SW) 7.14 55.4 19.89 Sense-Topic 6.96 53.5 19.30 AI-KU 6.5 39.0 15.92 unimelb 6.0 48.3 17.02 With data enrichment Sense-Topic (AAC) 9.39 59.1 23.56 Sense-Topic (AUC) 9.74 54.5 23.04",
                "metadata": {
                    "filename": "D18-1523.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "bw LM, no SP: seem, be, grow, be, be fw LM, with SP: went, got, started, wasn, loved bw LM, with SP: 1990s, decade, 1980s,",
                "metadata": {
                    "filename": "D18-1523.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "fw LM, no SP: be, be, remain, likely, be bw LM, no SP: becoming, grown becoming, much, becomes fw LM, with SP: remains, remain, which, continue, how bw LM, with SP: rising, overseas, booming, abroad, expanded",
                "metadata": {
                    "filename": "D18-1523.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Correlation between tense and sense. NMI is averaged on all verbs, using best matching sense. SP: Symmetric Patterns, LEM: Lemmatizing predictions, ALL: LEM, SP, TFIDF. The bold line show symmetric patterns without lemmatization excessively correlates tense and sense and provides additional validation to our hypothesis, suggesting its essential to lemmatizate when symmetric patterns are used.\nSettings NMI (mean \u00b1 STD) Gold labels 0.15 \u00b1 0.07 Final model 0.22 \u00b1 0.12 w/o SP 0.19 \u00b1 0.08 w/o TFIDF 0.18 \u00b1 0.07 w/o LEM 0.67\u00b1 0.12 w/o LEM and SP 0.26 \u00b1 0.09 w/o ALL 0.24 \u00b1 0.08",
                "metadata": {
                    "filename": "D18-1523.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            }
        ]
    },
    "1907.00390.pdf": {
        "normalized_output": [
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "ATIS",
                "Metric": "F-Score (F-S)",
                "Result": "95.80"
            },
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "ATIS",
                "Metric": "Accuracy",
                "Result": "97.76"
            },
            {
                "Task": "Word Sense Induction",
                "Dataset": "ATIS",
                "Metric": "Accuracy",
                "Result": "86.90"
            },
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "SNIPS",
                "Metric": "F1</s>",
                "Result": "92.23"
            },
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "SNIPS",
                "Metric": "Accuracy",
                "Result": "97.43"
            },
            {
                "Task": "Word Sense Induction",
                "Dataset": "SNIPS",
                "Metric": "Accuracy",
                "Result": "80.57"
            }
        ],
        "source_documents": [
            {
                "content": "Sen. (Acc) metrics in the experiments. For the slot \ufb01lling task, the F1-score is applied. For the intent de- tection task, the accuracy is utilized. Besides, the sentence-level semantic frame accuracy (sentence accuracy) is used to indicate the general perfor- mance of both tasks, which refers to proportion of the sentence whose slots and intent are both correctly-predicted in the whole corpus. Training Details: In our experiments, the layer size for the BLSTM networks is set to 64. Dur- ing training, the adam optimization (Kingma and Ba, 2014) is applied. Besides, the learning rate is updated by \u03b7t = \u03b70/(1 + pt) with a decay rate of p = 0.05 and an initial learning rate of \u03b70 = 0.01, and t denotes the number of completed steps. Model Performance: The performance of the models are given in Table 2, wherein it can be seen that our model outperforms the baselines in all three aspects: slot \ufb01lling (F1), intent detection (Acc) and sentence accuracy (Acc). Specially, on the sentence-level semantic frame results, the rel- ative improvement is around 3.79% and 5.42% for ATIS and Snips respectively, indicating that SF- ID network can bene\ufb01t the SLU performance sig- ni\ufb01cantly by introducing the bi-directional interre- lated mechanism between the slots and intent. Analysis of Seperate Subnets: We analyze the effect of seperate subnets, and the obtained results are given in Table 3. The experiments are con- ducted when the CRF layer is added. As we can see, both models including only the SF subnet or the ID subnet have acheived better results than the BLSTM model. Therefore, we believe that both SF subnet and ID subnet have signi\ufb01cance in per- formance improvement. Beside, we also analyse the condition with inde- pendent SF and ID subnet, in other words, when there is no interaction in SF and ID subnet. We can see it also obtains good results. However, the SF-ID network which allows the two subnets inter- act with each other achieve better results. This is because the",
                "metadata": {
                    "filename": "1907.00390.pdf",
                    "type": "Text",
                    "start_index": 0,
                    "page_number": 4
                }
            },
            {
                "content": "tion task. 2) We establish a novel iteration mecha- nism inside the SF-ID network in order to enhance the connections between the intent and slots. 3) The experiments on two benchmark datasets show the effectiveness and superiority of the proposed model. 2.2 SF-ID Network The SF-ID network consists of an SF subnet and an ID subnet. The order of the SF and ID subnets can be customized. Depending on the order of the two subnets, the model have two modes: SF-First and ID-First. The former subnet can produce ac- tive effects to the latter one by a medium vector. 2 Proposed Approaches This section \ufb01rst introduces how we acquire the integration of context of slots and intent by atten- tion mechanism. And then it presents an SF-ID network which establishes the direct connections between intent and slots. The model architecture based on bi-directional LSTM (BLSTM) is shown in Figure 2.1 2.2.1 SF-First Mode In the SF-First mode, the SF subnet is executed \ufb01rst. We apply the intent context vector cinte and slot context vector cslot in the SF subnet and gener- ate the slot reinforce vector rslot. Then, the newly- formed vector rslot is fed to the ID subnet to bring the slot information. 2.1 Integration of Context In SLU, word tags are determined not only by the corresponding terms, but also the context (Chen et al., 2016b). The intent label is also relevant with every element in the utterance. To capture such de- pendencies, attention mechanism is introduced. Slot \ufb01lling: The ith slot context vector ci slot is computed as the weighted sum of BLSTM\u2019s hid- den states (h1,...,ht): SF subnet: The SF subnet applies the intent and slot information (i.e. cinte and cslot) in the calcu- lation of a correlation factor f which can indicate the relationship of the intent and slots. This corre- lation factor f is de\ufb01ned by: In addition, we introduce a slot reinforce vector rslot de\ufb01ned by (3), and it is fed to the ID subnet to bring slot information. where the attention weight \u03b1 is acquired",
                "metadata": {
                    "type": "Text",
                    "page_number": 2,
                    "filename": "1907.00390.pdf",
                    "start_index": 0
                }
            },
            {
                "content": "by the hidden states and the context vectors of BLSTM. Thus, (4) (5) (6) can be replaced by: The intent reinforce vector rinte is still de\ufb01ned by (7), and it is fed to the SF subnet. SF subnet: The intent reinforce vector rinte is fed to the SF subnet and the relation factor f is calcu- lated the same way as (8). Other algorithm details are the same as in SF-First mode. Iteration Mechanism: Iteration mechanism in ID-First mode is almost the same as that in SF- First mode except for the order of the two subnets. 2.3 CRF layer Slot \ufb01lling is essentially a sequence labeling prob- lem. For the sequence labeling task, it is bene\ufb01cial to consider the correlations between the labels in neighborhoods. Therefore, we add the CRF layer above the SF subnet outputs to jointly decode the best chain of labels of the utterance. 3 Experiment Dataset: We conducted experiments using two public datasets, the widely-used ATIS dataset (Hemphill et al., 1990) and custom-intent-engine dataset called the Snips (Coucke et al., 2018), which is collected by Snips personal voice assis- tant. Compared with the ATIS dataset, the Snips dataset is more complex due to its large vocabu- lary and cross-domain intents. Evaluation Metrics: We use three evaluation",
                "metadata": {
                    "type": "Text",
                    "filename": "1907.00390.pdf",
                    "start_index": 1799,
                    "page_number": 3
                }
            },
            {
                "content": "in SF and ID subnet. We can see it also obtains good results. However, the SF-ID network which allows the two subnets inter- act with each other achieve better results. This is because the bi-directional interrelated mechanism help the two subnets promote each other mutually, which improves the performance in both tasks. Analysis of Model Mode: In Table 2, it can be seen that the ID-First mode achieves better perfor- mance in the slot \ufb01lling task. This is because the ID-First mode treats the slot \ufb01lling task as a more important task, because the SF subnet can utilize the intent information output from the ID subnet. Similarly, the SF-First mode performs better in the intent detection task. In general, the difference be- tween the two modes is minor. Iteration Mechanism: The effect of iteration mechanism is shown in Figure 3. The experiments are conducted in SF-First mode. Sentence accu- racy is applied as the performance measure be- cause it can re\ufb02ect the overall model performance. It increases gradually and reaches the maximum value when the iteration number is three on both ATIS and Snips dataset, indicating the effective-",
                "metadata": {
                    "start_index": 1803,
                    "page_number": 4,
                    "filename": "1907.00390.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "Sentence what Slots O \ufb02ights O leave O from O phoenix B-fromloc Intent atis \ufb02ight",
                "metadata": {
                    "filename": "1907.00390.pdf",
                    "page_number": 1,
                    "type": "Table"
                }
            },
            {
                "content": "Model Slot (F1) ATIS Dataset Intent (Acc) Sen. (Acc) Slot (F1) Snips Dataset Intent (Acc) Joint Seq (Hakkani-T\u00a8ur et al., 2016) 94.30 92.60 80.70 87.30 96.90 73.20 Atten.-Based (Liu and Lane, 2016) 94.20 91.10 78.90 87.80 96.70 74.10 Sloted-Gated (Goo et al., 2018) 95.42 95.41 83.73 89.27 96.86 76.43 SF-First (with CRF) 95.75 97.76 86.79 91.43 97.43 80.57 SF-ID SF-First (without CRF) 95.55 97.40 85.95 90.34 97.34 78.43 Network ID-First (with CRF) 95.80 97.09 86.90 92.23 97.29 80.43 ID-First (without CRF) 95.58 96.58 86.00 90.46 97.00 78.37",
                "metadata": {
                    "filename": "1907.00390.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Performance comparison on ATIS and Snips datasets. The improved cases are written in bold.\nModel Without SF-ID ID subnet Only SF subnet Only SF-ID (no interaction) SF-ID (SF-First) ATIS Slot Intent 95.05 95.34 95.43 95.74 95.14 95.75 95.56 95.75 95.75 97.76 Snips Slot Intent 88.9 96.23 89.57 97.42 90.7 96.71 90.97 97.01 91.43 97.43 SF-ID (ID-First) 95.80 97.09 92.23 97.29",
                "metadata": {
                    "filename": "1907.00390.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            }
        ]
    },
    "1709.04109.pdf": {
        "normalized_output": [
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2003 - English</s>",
                "Metric": "F1</s>",
                "Result": "91.71\u00b10.10"
            },
            {
                "Task": "Part-of-Speech (POS) Tagging",
                "Dataset": "Penn Treebank (PTB)",
                "Metric": "Accuracy",
                "Result": "97.53\u00b10.03"
            },
            {
                "Task": "Text Chunking",
                "Dataset": "CoNLL-2000</s>",
                "Metric": "F1</s>",
                "Result": "95.96\u00b10.08"
            }
        ],
        "source_documents": [
            {
                "content": "representation learning. In ACL. Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove: Global vectors for word representation. In EMNLP. Peters, M. E.; Ammar, W.; Bhagavatula, C.; and Power, R. 2017. Semi-supervised sequence tagging with bidirectional language models. arXiv:1705.00108. Ratinov, L., and Roth, D. 2009. Design challenges and mis- conceptions in named entity recognition. In CoNLL. Rei, M. 2017. Semi-supervised multitask learning for se- quence labeling. In ACL. Reimers, N., and Gurevych, I. 2017. Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging. arXiv preprint arXiv:1707.09861. Sha, F., and Pereira, F. 2003. Shallow parsing with condi- tional random \ufb01elds. In NAACL-HLT. Shang, J.; Liu, J.; Jiang, M.; Ren, X.; Voss, C. R.; and Han, J. 2017. Automated phrase mining from massive text corpora. arXiv:1702.04457. S\u00f8gaard, A., and Goldberg, Y. 2016. Deep multi-task learn- ing with low level tasks supervised at lower layers. In ACL. S\u00f8gaard, A. 2011. Semisupervised condensed nearest neigh- bor for part-of-speech tagging. In NAACL-HLT. Srivastava, R. K.; Greff, K.; and Schmidhuber, J. 2015. Highway networks. arXiv:1505.00387. Sun, X. 2014. Structure regularization for structured predic- tion. In NIPS. Tjong Kim Sang, E. F., and Buchholz, S. 2000. Introduc- tion to the conll-2000 shared task: Chunking. In Learning language in logic and CoNLL. Tjong Kim Sang, E. F., and De Meulder, F. 2003. Introduc- tion to the conll-2003 shared task: Language-independent named entity recognition. In Natural language learning at NAACL-HLT. Yang, Z.; Salakhutdinov, R.; and Cohen, W. W. 2017. Trans- fer learning for sequence tagging with hierarchical recurrent networks. arXiv:1703.06345.",
                "metadata": {
                    "start_index": 3605,
                    "filename": "1709.04109.pdf",
                    "type": "Text",
                    "page_number": 8
                }
            },
            {
                "content": "the limits of language modeling. arXiv:1602.02410. Jozefowicz, R.; Zaremba, W.; and Sutskever, I. 2015. An empirical exploration of recurrent network architectures. In ICML. Karpathy, A. 2015. The unreasonable effectiveness of re- current neural networks. http://karpathy.github. io/2015/05/21/rnn-effectiveness/. Accessed: 2017-08-22. Lafferty, J. D.; McCallum, A.; and Pereira, F. 2001. Con- ditional random \ufb01elds: Probabilistic models for segmenting and labeling sequence data. In ICML. Lample, G.; Ballesteros, M.; Kawakami, K.; Subramanian, S.; and Dyer, C. 2016. Neural architectures for named entity recognition. In NAACL-HLT. Liu, L.; Ren, X.; Zhu, Q.; Zhi, S.; Gui, H.; Ji, H.; and Han, J. 2017. Heterogeneous Supervision for Relation Extraction: A Representation Learning Approach. Proc. EMNLP. Luo, G.; Huang, X.; Lin, C.-Y.; and Nie, Z. 2015. Joint named entity recognition and disambiguation. In EMNLP. Ma, X., and Hovy, E. 2016. End-to-end sequence labeling via bi-directional lstm-cnns-crf. In ACL. Manning, C. D. 2011. Part-of-speech tagging from 97% to 100%: is it time for some linguistics? In International Con- ference on Intelligent Text Processing and Computational Linguistics. Springer. Marcus, M. P.; Marcinkiewicz, M. A.; and Santorini, B. 1993. Building a large annotated corpus of english: The penn treebank. Computational linguistics. McCallum, A., and Li, W. 2003. Early results for named entity recognition with conditional random \ufb01elds, feature in- duction and web-enhanced lexicons. In CoNLL. Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In NIPS. Peng, N., and Dredze, M. 2016. Improving named entity recognition for chinese social media with word segmenta- tion representation learning. In ACL. Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove: Global vectors for word representation. In EMNLP. Peters, M. E.; Ammar, W.; Bhagavatula, C.; and Power,",
                "metadata": {
                    "filename": "1709.04109.pdf",
                    "start_index": 1804,
                    "page_number": 8,
                    "type": "Text"
                }
            },
            {
                "content": "ments in the \ufb01nal version. Research was sponsored in part by the U.S. Army Research Lab. under Cooperative Agree- ment No. W911NF-09-2-0053 (NSCTA), National Science Foundation IIS 16-18481, IIS 17-04532, and IIS-17-41317, grant 1U54GM114838 awarded by NIGMS through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov), and Google PhD Fellowship. The views and conclusions contained in this document are those of the author(s) and should not be interpreted as repre- senting the of\ufb01cial policies of the U.S. Army Research Lab- oratory or the U.S. Government. The U.S. Government is au- thorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon. References Chieu, H. L., and Ng, H. T. 2002. Named entity recognition: A maximum entropy approach using global information. In COLING. Chiu, J. P. C., and Nichols, E. 2016. Named entity recogni- tion with bidirectional lstm-cnns. TACL. Collobert, R.; Weston, J.; Bottou, L.; Karlen, M.; Kavukcuoglu, K.; and Kuksa, P. P. 2011. Natural language processing (almost) from scratch. JMLR. Fernandez, J.; Yu, Z.; and Downey, D. 2017. Vecshare: A framework for sharing word representation vectors. Florian, R.; Ittycheriah, A.; Jing, H.; and Zhang, T. 2003. Named entity recognition through classi\ufb01er combination. In CoNLL. Glorot, X., and Bengio, Y. 2010. Understanding the dif- \ufb01culty of training deep feedforward neural networks. Proceedings of the Thirteenth International Conference on Arti\ufb01cial Intelligence and Statistics. In Hashimoto, K.; Xiong, C.; Tsuruoka, Y.; and Socher, R. 2016. A joint many-task model: Growing a neural network for multiple nlp tasks. arXiv:1611.01587. Jozefowicz, R.; Vinyals, O.; Schuster, M.; Shazeer, N.; and Wu, Y. 2016. Exploring the limits of language modeling. arXiv:1602.02410. Jozefowicz, R.; Zaremba, W.; and Sutskever, I. 2015. An empirical exploration of recurrent network architectures. In ICML. Karpathy, A. 2015. The",
                "metadata": {
                    "filename": "1709.04109.pdf",
                    "start_index": 0,
                    "type": "Text",
                    "page_number": 8
                }
            },
            {
                "content": "Table 5: Accuracy on the WSJ dataset. We mark models adopting pre-trained word embedding as \u2020, and record mod- els which leverage language models as \u2021. Table 6: Training statistics of TagLM (index 4 and 5) and LM-LSTM-CRF on the CoNLL03 NER dataset. 11) even performs worse than TagLM (index 10) , which reveals that directly applying co-training might hurt the se- quence labeling performance. We will also discuss this chal- lenge later in the Highway Layers & Co-training section. Besides, changing the forward language model from 4096-8192-1024 to LSTM-2048-512, TagLM (index 5) gets a lower F1 score of 91.62\u00b10.23. Comparing this score to ours (91.71\u00b10.10), one can verify that pre-trained language model usually extracts a large portion of unrelated knowledge. Relieving such redundancy by guiding the lan- guage model with task-speci\ufb01c information, our model is able to conduct both effective and ef\ufb01cient learning. POS Tagging Similar to the NER task, LM-LSTM-CRF outperforms all baselines on the WSJ portion of the PTB POS tagging task. Although the improvements over LSTM- CRF and CNN-LSTM-CRF are less obvious than those on the CoNLL03 NER dataset, considering the fact that the POS tagging task is believed to be easier than the NER task and current methods have achieved relatively high perfor- mance, this improvement could still be viewed as signi\ufb01cant. Moreover, it is worth noting that for both NER and POS tag- ging tasks, LM-LSTM-CRF achieves not only higher F1 scores, but also with smaller variances, which further veri- \ufb01es the superiority of our framework. Chunking In the chunking task, LM-LSTM-CRF also achieves relatively high F1 scores, but with slightly higher variances. Considering the fact that this corpus is much smaller than the other two (only about 1/5 of WSJ or 1/2 of CoNLL03 NER), we can expect more variance due to the Table 7: F1 score on the CoNLL00 chunking dataset. We mark models adopting pre-trained word embedding as \u2020, and record models which leverage",
                "metadata": {
                    "filename": "1709.04109.pdf",
                    "start_index": 0,
                    "page_number": 6,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Notation Table.\nx word-level input xi i-th word c character-level input ci,j j-th char in xi ci, space after xi c0, space before x1 y label sequence yi label of xi fi output of forward character-level LSTM at ci, ri output of backward character-level LSTM at ci, fL rL fN rN vi zi i i i i output of forward-to-LM highway unit output of backward-to-LM highway unit output of forward-to-SL highway unit output of backward-to-SL highway unit input of word-level bi-LSTM at xi output of word-level bi-LSTM at xi",
                "metadata": {
                    "filename": "1709.04109.pdf",
                    "page_number": 2,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Dataset summary.\nDataset # of Sentences Train Dev Test CoNLL03 NER 14,987 3,466 3,684 CoNLL00 chunking 7,936 1,000 2,012 WSJ 38,219 5,527 5,426",
                "metadata": {
                    "filename": "1709.04109.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Hyper-parameters of LM-LSTM-CRF.\nLayer Parameter POS NER chunking character-level embedding dimension 30 character-level depth 1 LSTM state size 300 Highway depth 1 word-level embedding dimension 100 word-level depth 1 bi-LSTM state size 300 Optimization \u03b70 0.015 0.01",
                "metadata": {
                    "filename": "1709.04109.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: F1 score on the CoNLL03 NER dataset. We mark models adopting pre-trained word embedding as \u2020, and record models which leverage language models as \u2021.\nExtra Resource Index & Model Type F1 score Value (\u00b1std) gazetteers 0) Collobert et al. 2011\u2020 1) Chiu et al. 2016\u2020 reported reported 89.59 91.62\u00b10.33 AIDA dataset 2) Luo et al. 2015 reported 91.20 CoNLL 2000 / PTB-POS dataset 3) Yang et al. 2017\u2020 reported 91.26 1B Word dataset & 4096-8192-1024 4) Peters et al. 2017\u2020\u2021 reported 91.93\u00b10.19 1B Word dataset 5) Peters et al. 2017\u2020\u2021 reported 91.62\u00b10.23 6) Collobert et al. 2011\u2020 reported 88.67 7) Luo et al. 2015 reported 89.90 8) Chiu et al. 2016\u2020 reported 90.91\u00b10.20 9) Yang et al. 2017\u2020 reported 91.20 10) Peters et al. 2017\u2020 reported 90.87\u00b10.13 11) Peters et al. 2017\u2020\u2021 reported 90.79\u00b10.15 None 12) Rei 2017 \u2020\u2021 mean max 87.38\u00b10.36 87.94 reported 86.26 mean 90.76\u00b10.08 13) Lample et al. 2016\u2020 max 91.14 reported 90.94 mean 91.37\u00b10.17 14) Ma et al. 2016\u2020 max 91.67 reported 91.21 15) LM-LSTM-CRF \u2020\u2021 mean max 91.71\u00b10.10 91.85",
                "metadata": {
                    "filename": "1709.04109.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5: Accuracy on the WSJ dataset. We mark models adopting pre-trained word embedding as \u2020, and record mod- els which leverage language models as \u2021.\nInd & Model Type Accuracy Value (\u00b1std) 0) Collobert et al. 2011\u2020 reported 97.29 16) Manning 2011 reported 97.28 17) S\u00f8gaard 2011 reported 97.50 18) Sun 2014 reported 97.36 mean 96.97\u00b10.22 12) Rei 2017\u2020\u2021 max 97.14 reported 97.43 13) Lample et al. 2016\u2020 mean\u00b1std 97.35\u00b10.09 maximum 97.51 mean\u00b1std 97.42\u00b10.04 14) Ma et al. 2016\u2020 maximum 97.46 reported 97.55 15) LM-LSTM-CRF \u2020\u2021 mean\u00b1std 97.53\u00b10.03 maximum 97.59",
                "metadata": {
                    "filename": "1709.04109.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 6: Training statistics of TagLM (index 4 and 5) and LM-LSTM-CRF on the CoNLL03 NER dataset.\nCoNLLO03 NER WSJ POS: CoNLLO00 Chunking Model h | FiScore || h | Accuracy | hb Fi Score LSTM-CRF 46 90.76 37 97.35 26 94.37 LSTM-CNN-CRF 7 91.22 21 97.42 6 95.80, LM-LSTM-CRF 6 9171 16 97.53 5 95.96 LSTM-CRF* 4 91.19 8 OT 44 2 \u201895.82 LSTM-CNN-CRF* 3 90.98 7 96.98 2 95.51",
                "metadata": {
                    "filename": "1709.04109.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 7: F1 score on the CoNLL00 chunking dataset. We mark models adopting pre-trained word embedding as \u2020, and record models which leverage language models as \u2021.\nExtra Resource Ind & Model Type F1 score Value (\u00b1std) PTB-POS 19) Hashimoto et al. 2016\u2020 20) S\u00f8gaard et al. 2016\u2020 reported reported 95.77 95.56 CoNLL 2000 / PTB-POS dataset 3)Yang et al. 2017\u2020 reported 95.41 1B Word dataset 4) Peters et al. 2017\u2020\u2021 reported 96.37\u00b10.05 21) Hashimoto et al. 2016\u2020 reported 95.02 22) S\u00f8gaard et al. 2016\u2020 reported 95.28 9) Yang et al. 2017\u2020 reported 94.66 mean 94.24\u00b10.11 None 12) Rei 2017\u2020\u2021 max reported 94.33 93.88 13) Lample et al. 2016\u2020 mean 94.37\u00b10.07 maximum 94.49 14) Ma et al. 2016\u2020 mean 95.80\u00b10.13 maximum 95.93 15) LM-LSTM-CRF \u2020\u2021 mean 95.96\u00b10.08 maximum 96.13",
                "metadata": {
                    "filename": "1709.04109.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 8: Training time and performance of LSTM-CRF, LSTM-CNN-CRF and LM-LSTM-CRF on three datasets. Our re-implementations are marked with *\nInd & Model F1score Module Time \u00b7 Device 15) LM-LSTM-CRF 91.71 total 6 h\u00b7GTX 1080 5) Peters et al. 2017 91.62 LSTM-2048-512 LSTM-2048-512 320 320 h\u00b7Telsa K40 h\u00b7Telsa K40 4) Peters et al. 2017 91.93 4096-8192-1024 LSTM-2048-512 14112 320 h\u00b7Telsa K40 h\u00b7Telsa K40",
                "metadata": {
                    "filename": "1709.04109.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 9: Effect of hidden state size of LSTM\n300 91.71\u00b10.10 92.14\u00b10.12 91.30\u00b10.13 LM-LSTM-CRF 200 91.63\u00b10.23 92.07\u00b10.22 91.19\u00b10.30 100 91.13\u00b10.32 91.60\u00b10.37 90.67\u00b10.32 300 90.76\u00b10.08 90.82\u00b10.08 90.69\u00b10.08 LSTM-CRF 200 90.41\u00b10.07 90.63\u00b10.07 90.20\u00b10.07 100 90.74\u00b10.22 91.08\u00b10.50 90.42\u00b10.17 300 91.22\u00b10.19 91.70\u00b10.16 90.74\u00b10.27 LSTM-CNN-CRF 200 91.37\u00b10.17 91.08\u00b10.53 90.58\u00b10.11 100 91.18\u00b10.10 91.56\u00b10.16 90.81\u00b10.15",
                "metadata": {
                    "filename": "1709.04109.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 10: Effect of language model and highway\nState Size Model F1score\u00b1std Recall\u00b1std Precision\u00b1std LM-LSTM-CRF 91.71\u00b10.10 92.14\u00b10.12 91.30\u00b10.13 300 LM-LSTM-CRF NL 91.43\u00b10.09 91.85\u00b10.18 91.01\u00b10.19 LM-LSTM-CRF NH 91.16\u00b10.22 91.67\u00b10.28 90.66\u00b10.23 LM-LSTM-CRF 91.63\u00b10.23 92.07\u00b10.22 91.19\u00b10.30 200 LM-LSTM-CRF NL 91.44\u00b10.10 91.95\u00b10.16 90.94\u00b10.16 LM-LSTM-CRF NH 91.34\u00b10.28 91.79\u00b10.18 90.89\u00b10.30 LM-LSTM-CRF 91.13\u00b10.32 91.60\u00b10.37 90.67\u00b10.32 100 LM-LSTM-CRF NL 91.17\u00b10.11 91.72\u00b10.14 90.61\u00b10.21 LM-LSTM-CRF NH 91.01\u00b10.19 91.50\u00b10.21 90.53\u00b10.30",
                "metadata": {
                    "filename": "1709.04109.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            }
        ]
    },
    "1811.05021.pdf": {
        "normalized_output": [
            {
                "Task": "Dialogue Act Classification",
                "Dataset": "Switchboard Dialog Act Corpus (SWDA)",
                "Metric": "Accuracy",
                "Result": "81.5"
            },
            {
                "Task": "Dialogue Act Classification",
                "Dataset": "MapTask",
                "Metric": "Accuracy",
                "Result": "68.5"
            }
        ],
        "source_documents": [
            {
                "content": "is labeled with one of the 13 tags. Unlike SwDA, the MapTask corpus emphasizes on directions and conductions. Table II presents different statistics for both datasets. For SwDA, training and testing sets are provided but not the validation set, so we use the standard practice of taking a part of training data set as validation set [15]. We also shows the top 10 utterance labels on different datasets in Figure 3. (a) SwDA Figure 3: The distribution of utterance labels on two datasets: a) SwDA, b) MapTask. B. Evaluation Metrics We employ the standard accuracy as the metric of evaluating our proposed ALDMN method. The accuracy is de\ufb01ned as: where \u02c6yi and yi are the predicted label and ground true label, respectively. 1[ ] is the indicator function. When the predicted \u00b7 label is the same as the true, label 1[ ] equals to 1 otherwise 0. C. Implementation Details To train our proposed model, we \ufb01rst randomize the training data and set the mini-batch to 128. For each batch, the utterance is padded with a special token <pad> to the maximum length. We regard the words whose occurrence is less than 2 as Out of Vocabulary (OOV) tokens and replace them with <unk>s. We remove all punctuation marks except interrogation and all characters are converted to lower-case. We set the memory updating iterations to 3. We adopt Adam [41] optimizer with the learning rate initialized to 0.01. We run the training data set for 45 epochs with early stopping when validation loss did not decline for \ufb01ve consecutive epochs. The random uniform initialization with range [ 0.1,0.1] is used for all matrix \u2212 variables, including word embedding and other weights. Both the embedding and hidden dimensions are set to d = 200. The",
                "metadata": {
                    "page_number": 6,
                    "filename": "1811.05021.pdf",
                    "start_index": 1809,
                    "type": "Text"
                }
            },
            {
                "content": "the input data correspond- ing to the question being asked to are encoded into a sequence of distributed vectors representations. We name the vectors as facts (evidences), denoted as E = [e1,e2, ], where ,e E \u00b7\u00b7\u00b7 | | is the total number of facts, usually is the number of E | | sentence in a document. So ei is the vector representation of the i-th sentence. The order of vectors in E cannot be changed at random because memory updates need to be based on the where rt and zt are the reset gate vector and update gate vector respectively, gt is the output vector, Ws and bs are weights matrices and biases, \u03c3 is the sigmoid activation function, an element-wise multiplication. \u25e6 is 2) Question Module: This module is similar to the input module, which also utilizes GRU as the encoder and encodes a sequence of question words into a distributed vector repre- sentation q. And then q is fed into episode memory module and answer module separately. 3) Episode Memory Module: Episode memory module is the main component of the DMN. It is comprised of an attention mechanism and a memory updating mechanism. This module iterates over the input evidences representations and extracts the information to answer the question q. When the questions are too complex to answer so that we need reasoning, the episode memory network may iterates over the input evidences representations multiple times. The episode memory may be updated after each iteration. We denote the memory after i- th iteration over the input evidences representations as mi. We initialize the foremost m0 as the question vector q. The attention mechanism is in charge of producing a contextual vector ci, a weighted sum of the input evidences representation, with relevance of the question q and the previous memory",
                "metadata": {
                    "start_index": 1806,
                    "page_number": 3,
                    "filename": "1811.05021.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "each utterance and RNNs are used to create a general view of the whole dialogue. Khanpour et al. [7] design a deep neural network model that bene\ufb01ts from pre-trained word embeddings combined with a variation of the RNN structure for the DA classi\ufb01cation task. Ji et al. [8] also investigate the performance of using standard RNN and CNN on DA classi\ufb01cation and get the cutting edge results on the MRDA corpus using CNN. Lee et al. [15] propose a model based on CNNs and RNNs that incorporates preceding short texts as context to classify current DAs. Unlike previous models, we cast the DA classi\ufb01cation task into a question and answering problem. B. Dynamic Memory Network One line of research related to dynamic memory network is attention and memory mechanism [16], [17], which have been successfully applied in many tasks such as text generation [18], [19] and question answering [20], [21]. In these works, memory is encoded as a continuous representation and operations on memory (e.g. reading and writing) are typically implemented with neural networks. Attention mechanism could be viewed as a compositional function, where lower level representations are regarded as the memory, and the function is to assign a weight to each lower position when computing an upper level representation. Such attention based approaches have achieved promising performances on a variety of NLP tasks [22]. Based on these works, [23] develops dynamic memory network which simultaneously contains memory updating mechanism and attention mechanism. [10] proposes an improved dynamic memory network with some modi\ufb01cations in memory and input module. The dynamic memory network has been successfully applied in many scenarios such as question answering and sentiment analysis. To the best of our knowledge, it\u2019s the \ufb01rst time that we apply dynamic memory network in DA classi\ufb01cation. C. Adversarial Learning Adversarial training [24] introduces an end-to-end and deterministic way of data perturbation by",
                "metadata": {
                    "start_index": 1797,
                    "page_number": 2,
                    "type": "Text",
                    "filename": "1811.05021.pdf"
                }
            },
            {
                "content": "1. The weights of evidences that contribute more to the mi \u2212 answer are larger. The episode module may focus on the important information via soft alignment. The memory updating mechanism is in charge of producing the mi based on the contextual vector ci and the previous memory mi 1. 4) Answer Module: The answer module generates an appropriate answer given the question representation q and the \ufb01nal episode memory. After multiple updates, mT contains all information that is required to answer the question. This module takes different execution modes according to different types of tasks. For single-token-answer case, it is treated as a simple classi\ufb01cation task. The token-generation layer is composed of a linear layer with a softmax activation classi\ufb01er to compute the probability distribution of the answer over the entire vocabulary table. For task that requires generating a sequence of tokens, another GRU is used to decode the concatenation of q and mT to a sequence of tokens. C. An Overview Figure 1 shows an overview of the network architecture of our proposed model. The framework can be divided into four parts: (a) Hierarchical pyramid utterance encoder module (cf. Sec. IV-A). In this module, we \ufb01rst represent the input utterance via a pyramidal BiGRU. We \ufb01rst embed each word token into a continuous distributed embedding using Glove [35], and then we add a perturbation for each word embedding for adversarial training. Then we fed the perturbed embedding to a pyramidal BiGRU. For dialogue history embedding, we apply a pyramidal BiGRU to embed each utterance and apply another utterance level GRU to represent the whole context. We call this encoder as hierarchical pyramidal utterance encoder. (b) General question module (cf. Sec. IV-B). In this module, we \ufb01x the question as \u201cWhat is the label of this utterance?\u201d. We adopt a vanilla BiGRU to embed the question. (c) Attentional episode memory updates module (cf. Sec. IV-C). This module is the core component of our",
                "metadata": {
                    "page_number": 4,
                    "filename": "1811.05021.pdf",
                    "type": "Text",
                    "start_index": 0
                }
            },
            {
                "content": "Table I: A snippet of a conversation sample. Each utterance has related dialogue act label [5].\nSpeaker A B A B Utterance Hi, long time no see. Hi, how are you? What are you doing these days? I\u2019m busying writing my paper. DA label Greeting Greeting Question Answer A I heard that the deadline is coming. Statement B Yeah. Backchannel A B A B You need to make a push. Sure, thats why I am so busy now. I can\u2019t bother you for too long, goodbye. See you later. Opinion Agreement Farewell Farewell",
                "metadata": {
                    "filename": "1811.05021.pdf",
                    "page_number": 1,
                    "type": "Table"
                }
            },
            {
                "content": "Table II: is the number of Dialogue Act classes, C | | the vocabulary size. MinL, MeanL and MaxL indicate the | V | is minimum, mean and maximum of utterance length, respectively.\nDataset |C| |V | MinL MeanL MaxL SwDA 42 19K 16 56 118 MapTask 13 15K 2 4 21",
                "metadata": {
                    "filename": "1811.05021.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table III and Table IV respectively show the experimental Accuracy results of the methods on the SwDA and MRDA datasets. The hyper-parameters and parameters which achieve the best performance on the validation set are chosen to conduct the testing evaluation. From these two tables, we can \ufb01nd that our proposed model ALDMN de\ufb01nitely outperforms other baselines in both datasets. On SwDA dataset, our model\nModel Accuracy(%) RCNN(Bulnsom et al. 2013) 73.9 BiLSTM-Softmax(Khanpour et al. 2016) 75.8 DRLM-Conditional(Ji et al. 2016) 77.0 PDI(Tran et al. 2017) 75.6 UCI(Liu et al. 2017) 79.9 DMN(Kumar et al., 2015) 75.2 ALDMN (Our Model) 81.5",
                "metadata": {
                    "filename": "1811.05021.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table III: Classi\ufb01cation accuracy on SwDA corpus, comparing our ALDMN model with other methods as described in literatures.\nModel Accuracy(%) ADC(Julia et al. 2010) 55.4 GAN(Tran et al. 2017b) 62.9 PDI(Tran et al. 2017a) 65.9 DMN(Kumar et al., 2015) 64.7 ALDMN (Our Model) 68.5",
                "metadata": {
                    "filename": "1811.05021.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Figure 7: The performance of our proposed model w.r.t. varying length of utterances on SwDA dataset.\nfo 59 04 00 68 00 00 00 00 00 qw 0.0 00 00 30 144 00 00 00 00 qyd 00 00 [8987 264 102 00 00 00 00 143 qy 00 03 07 00 00 00 oO 214 sd 00 00 04 00 ERM o2 of 01 00 40 ad 00 00 00 00 324 296 00 00 04 232 h 00 00 00 00 163 00 aa 00 00 00 00 52 00 b 00 00 00 08 00 00 sv 00 00 03 00 254 00 fo qw qyd qy sd_ ad",
                "metadata": {
                    "filename": "1811.05021.pdf",
                    "page_number": 9,
                    "type": "Table"
                }
            }
        ]
    },
    "2002.03184.pdf": {
        "normalized_output": [
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201914 EN-DE",
                "Metric": "BLEU",
                "Result": "29.6"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201914 EN-FR",
                "Metric": "BLEU",
                "Result": "43.2"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "IWSLT\u201914 EN-DE</s>",
                "Metric": "BLEU",
                "Result": "35.5"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-1</s>",
                "Result": "40.59"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-2</s>",
                "Result": "18.97"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-L",
                "Result": "36.81"
            },
            {
                "Task": "Language Modeling",
                "Dataset": "WikiText-103",
                "Metric": "Perplexity</s>",
                "Result": "23.3"
            }
        ],
        "source_documents": [
            {
                "content": "Tang, G., Mller, M., Rios, A., and Sennrich, R. Why self- attention? a targeted evaluation of neural machine trans- lation architectures. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018. Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., ukasz Kaiser, Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O., Cor- rado, G., Hughes, M., and Dean, J. Google\u2019s neu- ral machine translation system: Bridging the gap be- tween human and machine translation, 2016. URL https://arxiv.org/abs/1609.08144.",
                "metadata": {
                    "filename": "2002.03184.pdf",
                    "type": "Text",
                    "start_index": 0,
                    "page_number": 12
                }
            },
            {
                "content": "Table 2. Machine translation accuracy in terms of BLEU for WMT En-De and WMT En-Fr on newstest2014. Table 3. Machine translation accuracy in terms of BLEU on IWSLT De-En. eling. These three tasks are considered touchstone and challenging in the NLP \ufb01eld. Machine Translation On the machine translation task, we report results on three mainstream benchmark datasets: WMT English to German (En-De), WMT English to French (En-Fr) and IWSLT German to English (De-En). et al. (2017), we applied compound splitting for WMT En- De. We trained \ufb01ve random initializations of each model con\ufb01guration and report test accuracy of the seed which re- sulted in the highest validation BLEU score. For all datasets, we used beam search with beam width 5. Similar to Wu et al. (2019), we tuned a length penalty as well as the number of checkpoints to average on the validation set. Abstractive Summarization For the abstractive summa- rization task, we decided to experiment with the CNN- DailyMail (Hermann et al., 2015; Nallapati et al., 2016) dataset. The dataset is composed by approximately 280K news articles with associated multi-sentence summaries. We followed the same pre-processing steps as described by Wu et al. (2019). The BPE vocabulary consists of 30K subword tokens. We report results using the F1-Rouge (Rouge-1, Rouge-2 and Rouge-L) metric (Lin, 2004). For generating the summaries, similar to Wu et al. (2019) we tuned the maximum output length, disallowing repeating the same trigram, and we apply a stepwise length penalty. For all datasets, we replicated the pre-processing steps men- tioned in Wu et al. (2019). Speci\ufb01cally, for the WMT En-De we used the WMT\u201916 training data that consists of 4.5M sentence pairs. We validated on newstest2013 and tested on newstest2014. We employed byte-pair encoding (BPE) (Sennrich et al., 2016) to the sentences, with a 32K joint source and target vocabulary. For the WMT En-Fr, we used 36M training sentence pairs from WMT\u201914. We validated on",
                "metadata": {
                    "type": "Text",
                    "start_index": 0,
                    "filename": "2002.03184.pdf",
                    "page_number": 6
                }
            },
            {
                "content": "Acknowledgements This research was supported by the Canada Research Chairs program and the NSERC Discovery grant. We would like to express our gratitude to our anonymous reviewers for their valuable comments and feedback. A special thank you to Vasileia Karasavva for editing and proofreading the \ufb01nal manuscript. of the 2016 Conference on Empirical Methods in Natural Language Processing, 2016. References",
                "metadata": {
                    "filename": "2002.03184.pdf",
                    "start_index": 0,
                    "page_number": 10,
                    "type": "Text"
                }
            },
            {
                "content": "Table 6. Throughput and memory consumption decrease measured for different sequence lengths (n) on a batch of size 10 with each token being represented with d = 1024 and H = 16. Throughput is calculated across 100K iterations of a single input encoding execution for each method. Memory decrease is computed as how many times less memory we need to encoding the input embedding compared to Self-Attention. Larger numbers indicate better performance. of-the-art score. It is important to underline, however, that our method uses the least number of parameters compared to the other counterpart methods. Table 3 shows results for IWSLT De-En benchmark dataset. Following Wu et al. (2019), we employed a smaller model with less parameters to re\ufb02ect the size of the dataset. Specif- ically, we set d to 512, dff to 1024 and H to 4. Furthermore, we disabled the GLU unit that is described in Section 3.6 and made the input projection layer to size W \u2208 Rd\u00d7d. Our method was able to outperform all other methods setting a new state-of-the-art result. 4.4. Results on Abstractive Summarization We evaluated the proposed method on the task of abstractive summarization. We test the method\u2019s ability to process long documents on the CNN-DailyMail dataset. We encode an article of up to 400 sub-words and we generate a summariza- tion composed from multiple sentences. Table 4 shows the results of our experiments. Our Standard model is able to achieve better results on the Rouge-1 and Rouge-2 metrics than previous methods. In addition, the Standard model is using signi\ufb01cantly less parameters, approximately 30M pa- rameters less. The Deep model uses more layers to closely match the number of parameters and is able to outperform all previous models. This shows that our method is able to encode long sequences successfully without having the need to have access to all context. 4.5. Results on Language Modeling We evaluated our method on the task of language model- ing. We considered the WikiText-103",
                "metadata": {
                    "page_number": 8,
                    "type": "Text",
                    "start_index": 0,
                    "filename": "2002.03184.pdf"
                }
            },
            {
                "content": "Table 1. Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and nbuckets is the number of hash buckets.\nLayer Type Complexity per Layer Sequential Maximum Path Length Operations Recurrent (Sutskever et al., 2014) O(n \u00b7 d2) O(n) O(n) Convolutional (Kalchbrenner et al., 2016; Gehring et al., 2017) O(k \u00b7 n \u00b7 d2) O(1) O(logk(n)) or O(n/k) Self-Attention (Vaswani et al., 2017) O(n2 \u00b7 d) O(1) O(1) Dynamic Convolutions (Wu et al., 2019) O(k \u00b7 n \u00b7 d) O(1) O(n/k) Reformer (Kitaev et al., 2020) O(n \u00b7 log(n) \u00b7 d) O(log(n)) O(n/nbuckets) TaLK Convolutions (Ours) O(n \u00b7 d) O(log(n)) O(n/(lmax + rmax + 1))",
                "metadata": {
                    "filename": "2002.03184.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2. Machine translation accuracy in terms of BLEU for WMT En-De and WMT En-Fr on newstest2014.\nModel Param (En-De) WMT En-De WMT En-Fr Gehring et al. (2017) 216M 25.2 40.5 Vaswani et al. (2017) 213M 28.4 41.0 Ahmed et al. (2017) 213M 28.9 41.4 Chen et al. (2018) 379M 28.5 41.0 Shaw et al. (2018) - 29.2 41.5 Ott et al. (2018) 210M 29.3 43.2 Wu et al. (2019) 213M 29.7 43.2 Kitaev et al. (2020) 213M 29.1 \u2013 TaLK Convolution (Ours) 209M 29.6 43.2",
                "metadata": {
                    "filename": "2002.03184.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3. Machine translation accuracy in terms of BLEU on IWSLT De-En.\nModel Param IWSLT De-En Deng et al. (2018) - 33.1 Vaswani et al. (2017) 47M 34.4 Wu et al. (2019) 43M 35.2 TaLK Convolution (Ours) 42M 35.5",
                "metadata": {
                    "filename": "2002.03184.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4. Results on CNN-DailyMail abstractive summarization.\nModel Param Rouge-1 Rouge-2 Rouge-L LSTM (Paulus et al., 2018) - 38.30 14.81 35.49 CNN (Fan et al., 2018) - 39.06 15.38 35.77 Self-Attention Baseline (Wu et al., 2019) 90M 39.26 15.98 36.35 Lightweight Convolution (Wu et al., 2019) 86M 39.52 15.97 36.51 Dynamic Convolution (Wu et al., 2019) 87M 39.84 16.25 36.73 TaLK Convolution (Standard) 59M 40.03 18.45 36.13 TaLK Convolution (Deep) 83M 40.59 18.97 36.81",
                "metadata": {
                    "filename": "2002.03184.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5. Test perplexity on WikiText-103. We used adaptive inputs similar to Baevski & Auli (2019) and show that our method yields better perplexity than dynamic convolutions and comparative per- formance with self-attention.\nGrave et al. (2017) Dauphin et al. (2017) Param Test - 40.8 229M 37.2 Merity et al. (2018) 151M 33.0 Rae et al. (2018) - 29.2 Baevski & Auli (2019) 247M 20.5 Dynamic Convolutions TaLK Convolution (Ours) 255M 25.0 240M 23.3",
                "metadata": {
                    "filename": "2002.03184.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 6. Throughput and memory consumption decrease measured for different sequence lengths (n) on a batch of size 10 with each token being represented with d = 1024 and H = 16. Throughput is calculated across 100K iterations of a single input encoding execution for each method. Memory decrease is computed as how many times less memory we need to encoding the input embedding compared to Self-Attention. Larger numbers indicate better performance.\nMethod n = 10 iter/sec Mem. \u2193 n = 100 iter/sec Mem. \u2193 n = 1,000 iter/sec Mem. \u2193 n = 10,000 iter/sec Mem. \u2193 Self-Attention 4576 1x 3437 1x 102 1x OOM 1x DynamicConv (k = 3) 3739 1x 3308 0.99x 443 2.8x 45 25.4x DynamicConv (k = 31) 4535 0.97x 3860 1x 325 2.7x 29 20.2x TaLK Convolution 9686 1.1x 6126 1.1x 898 3.1x 92 26.4x",
                "metadata": {
                    "filename": "2002.03184.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 7. Ablation on IWSLT De-En validation set. (+) indicates that a result includes all preceding features.\nModel Param BLEU TaLK Convolution (al i,ar i=1x7, H=1) 42M diverges + Output Normalization 42M 35.70 \u00b1 0.1 + Increasing Max Offsets (al i,ar i=1,3,7,15x4) 42M 36.23 \u00b1 0.1 + Offsets Dropout (p=0.1) 42M 36.37 \u00b1 0.05 + Fully-headed Kernels (H=512) 47M 36.51 \u00b1 0.07 + Multi-headed Kernels (H=4) 42M 36.65 \u00b1 0.05 Replacing Swish with ReLU 42M 36.21 \u00b1 0.05",
                "metadata": {
                    "filename": "2002.03184.pdf",
                    "page_number": 9,
                    "type": "Table"
                }
            }
        ]
    },
    "D18-1279.pdf": {
        "normalized_output": [
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2002 - Spanish",
                "Metric": "F1",
                "Result": "86.68"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2002- Dutch",
                "Metric": "F1",
                "Result": "87.81"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2003 - English",
                "Metric": "F1</s>",
                "Result": "91.64"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2003 - German",
                "Metric": "F1</s>",
                "Result": "79.43"
            },
            {
                "Task": "Text Chunking",
                "Dataset": "CoNLL-2000",
                "Metric": "F1</s>",
                "Result": "95.29"
            },
            {
                "Task": "Part-of-Speech (POS) Tagging",
                "Dataset": "Penn Treebank (PTB)",
                "Metric": "F1",
                "Result": "97.58"
            }
        ],
        "source_documents": [
            {
                "content": "(such as transfer learning, joint training), hand- crafted features, or any character preprocessing, we do not replace any rare words into UNKNOWN. Named entity recognition. CoNLL-2002 and CoNLL2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) con- tain named entity labels for Spanish, Dutch, En- glish and German as separate datasets. These four datasets contain different types of named entities: locations, persons, organizations, and miscella- neous entities. Unlike some approaches, we do not combine the validation set with the training set. Although POS tags were made available for these datasets, we do not leverage those as addi- tional information which sets our approach apart from that of transfer learning. Part-of-speech tagging. The Wall Street Jour- nal (WSJ) portion of Penn Treebank (PTB) (Mar- cus et al., 1993) contains 25 sections and catego- rizes each word into one out of 45 POS tags. We adopt the standard split and use sections 0-18 as training data, sections 19-21 as development data, and sections 22-24 as test data. Syntactic chunking. The CoNLL 2000 chunk- ing task (Tjong Kim Sang and Buchholz, 2000) uses sections 15-18 from the Wall Street Journal corpus for training and section 20 for testing. It de\ufb01nes 11 syntactic chunk types (e.g., NP, VP, ADJP), we adopt the standard split and sample 1000 sentences from the training set as the devel- opment set. 4.2 Training Settings Initialization. The size of the dimensions of char- acter embeddings is 32 which are randomly ini- tialized using a uniform distribution. We adopt the same initialization method for randomly ini- tialized word embeddings that are updated during training. For IntNet, the \ufb01lter size of the initial convolution is 32 and that of other convolutions is 16. We have used \ufb01lters of size [3,4,5] for all the kernels. The number of convolutional layers are 5 and 9 for IntNet-5 and IntNet-9, respectively, and we have adopted the same weight initialization as that of",
                "metadata": {
                    "page_number": 6,
                    "type": "Text",
                    "filename": "D18-1279.pdf",
                    "start_index": 0
                }
            },
            {
                "content": "any ex- ternal resources, hand-crafted features, additional knowledge, joint training, or character-level pre- processing, and achieves new state-of-the-art per- formance for various sequence labeling tasks, in- cluding named entity recognition, part-of-speech tagging and syntactic chunking. In the future, we would like to explore using the IntNet model for other NLP tasks. References",
                "metadata": {
                    "type": "Text",
                    "page_number": 9,
                    "start_index": 1809,
                    "filename": "D18-1279.pdf"
                }
            },
            {
                "content": "character-to-word representa- tions in Section 5. Without down-sampling. Compared to other CNN models like ResNet and DenseNet, our model does not contain any halve down-sampling layer or average pooling to reduce resolution. We did not \ufb01nd these operations to be helpful and, in some cases, found them to be detrimental to per- formance. These operations are useful for sen- tences and images, but might break the internal structure of words, like the sequential patterns for pre\ufb01xes and suf\ufb01xes. Character-to-word representations. In the last layer, we use a max-over-time pooling oper- ation: which takes the maximum value corresponding to a particular \ufb01lter. The idea is to capture the most important feature with the highest value for each feature map. Finally, we concatenate all of salient features together as a representation for this word:",
                "metadata": {
                    "filename": "D18-1279.pdf",
                    "page_number": 4,
                    "start_index": 1798,
                    "type": "Text"
                }
            },
            {
                "content": "where u is the number of salient features which is equal to the total number of output feature maps in the last layer. If each function Fl produces p feature maps, we obtain (p0 + p \u00d7 L\u22121 2 ) represen- tations, where p0 is the number of output feature maps in the initial convolution layer. 3.2 Bi-directional RNN Given the character-to-word representations are computed by IntNet in Equation 6, we denote the input vector (z1, z2, . . . , zn) for a sentence. LSTM (Hochreiter and Schmidhuber, 1997) re- turns the sequence (h1, h2, . . . , hn) that repre- sents the sequential information at every step. We use the following implementation: i, = 0 (W2i2, + Writy\u20141 + Weitr\u20141 + bi) fi = o(Wepzt + Wryhi-1 + Weper-1 + be) CG = tanh(Weetz + Wachi-1 + be) ee =fOac1 tho 0, = 0(W2o2%1 + Wholti\u20141 + Weots + bo) hy = 0; \u00a9 tanh(e;), ee =fOac1 tho where o is the element-wise sigmoid function and \u00a9 is the element-wise product. z; is the input vec- tor at time t and i;, f;, 0;, c; are the input gate, forget gate, output gate, and cell vectors, all of which are the same size as the hidden vector h;. W.i, Wz, Wzo, Wee denote the weight matrices of different gates for input 2; Whi, War, Who; Whe are the weight matrices for hidden state h;, and b;, by, bo, b, denote the bias vectors. Forward LSTM and backward LSTM compute the repre- sentations of hy and h; for left and right context of the sentence, respectively. We concatenate two hidden states to form the output of bi-directional LSTM th;, hi] for capturing context information from both sides. 3.3 Scoring Function Instead of predicting each label independently, we consider the correlations between labels in neigh- borhoods and jointly decode the best chain of la- bels for a given input sentence by leveraging a conditional random \ufb01eld (Lafferty et al., 2001). Formally, the sequence of labels is de\ufb01ned as: To de\ufb01ne the scoring function f(h, y) for each position t, we multiply the hidden state hw t with a parameter vector wyt that is indexed",
                "metadata": {
                    "type": "Text",
                    "start_index": 0,
                    "page_number": 5,
                    "filename": "D18-1279.pdf"
                }
            },
            {
                "content": "Table 1: F1 score of different character-to-word models.\nModel Spanish NER Dutch NER English NER German NER Chunking PTB POS Baseline 70.73\u00b10.42 63.49\u00b10.42 77.51\u00b10.39 54.07\u00b10.42 91.97\u00b10.21 95.76\u00b10.13 + char-LSTM 79.93\u00b10.43 77.16\u00b10.47 83.98\u00b10.46 64.29\u00b10.47 93.31\u00b10.23 97.14\u00b10.11 + char-CNN 79.78\u00b10.41 76.43\u00b10.48 83.85\u00b10.38 63.53\u00b10.41 92.67\u00b10.24 97.02\u00b10.12 + char-CNN-5 79.63\u00b10.38 76.92\u00b10.42 83.60\u00b10.39 64.26\u00b10.42 93.11\u00b10.26 97.15\u00b10.12 + char-CNN-9 79.25\u00b10.56 74.82\u00b10.46 83.31\u00b10.47 63.97\u00b10.46 92.92\u00b10.27 97.13\u00b10.13 + char-ResNet-9 74.34\u00b10.45 76.54\u00b10.39 83.91\u00b10.42 66.15\u00b10.44 93.85\u00b10.24 96.99\u00b10.15 + char-DenseNet-9 78.25\u00b10.52 76.71\u00b10.53 84.16\u00b10.41 67.54\u00b10.46 93.82\u00b10.25 97.13\u00b10.11 + char-IntNet-9 78.53\u00b10.44 76.93\u00b10.47 83.83\u00b10.44 70.11\u00b10.41 93.94\u00b10.26 97.19\u00b10.12 + char-IntNet-5 80.44\u00b10.43 78.06\u00b10.45 85.34\u00b10.39 69.48\u00b10.42 94.27\u00b10.23 97.23\u00b10.11",
                "metadata": {
                    "filename": "D18-1279.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: F1 score of our proposed models in comparison with state-of-the-art results.\nModel Spanish Dutch English German Chunking POS Conv-CRF+Lexicon (Collobert et al., 2011) - - 89.59 - 94.32 97.29 LSTM-CRF+Lexicon (Huang et al., 2015) - - 90.10 - 94.46 97.43 LSTM-CRF+Lexicon+char-CNN (Chiu and Nichols, 2016) - - 90.77 - - - LSTM-Softmax+char-LSTM (Ling et al., 2015) - - - - - 97.55 LSTM-CRF+char-LSTM (Lample et al., 2016) 85.75 81.74 90.94 78.76 - - LSTM-CRF+char-CNN (Ma and Hovy, 2016) - - 91.21 - - 97.55 GRM-CRF+char-GRU (Yang et al., 2017) 84.69 85.00 91.20 - 94.66 97.55 LSTM-CRF 80.33\u00b10.37 79.87\u00b10.28 88.41\u00b10.22 73.42\u00b10.39 94.29\u00b10.11 96.63\u00b10.08 LSTM-CRF+char-LSTM 86.12\u00b10.34 87.13\u00b10.25 91.13\u00b10.15 78.31\u00b10.35 94.97\u00b10.09 97.49\u00b10.04 LSTM-CRF+char-CNN 85.91\u00b10.38 86.69\u00b10.22 91.11\u00b10.14 78.15\u00b10.31 94.91\u00b10.08 97.45\u00b10.03 LSTM-CRF+char-IntNet-9 85.71\u00b10.39 87.38\u00b10.27 91.39\u00b10.16 79.43\u00b10.33 95.08\u00b10.07 97.51\u00b10.04 LSTM-CRF+char-IntNet-5 86.68\u00b10.35 87.81\u00b10.24 91.64\u00b10.17 78.58\u00b10.32 95.29\u00b10.08 97.58\u00b10.02",
                "metadata": {
                    "filename": "D18-1279.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: F1 score of different models for IV, OOTV, OOEV and OOBV.\nModel IV English OOTV OOEV OOBV IV German OOTV OOEV OOBV IV Spanish OOTV OOEV OOBV IV Dutch OOTV OOEV OOBV char-LSTM 97.15 89.87 89.41 87.07 86.97 85.80 68.35 64.76 89.63 89.06 78.14 74.13 94.50 87.98 80.00 72.37 Dev char-CNN char-IntNet-9 97.10 96.86 90.04 90.52 95.45 91.95 88.02 90.16 87.45 87.92 86.13 85.29 57.14 76.07 63.28 67.98 88.93 88.43 88.85 88.58 72.90 74.53 71.96 72.09 94.54 93.68 87.27 87.49 74.55 89.09 68.77 75.58 char-IntNet-5 96.65 90.14 88.10 88.31 87.21 85.00 67.10 64.17 88.56 88.47 78.90 70.23 94.63 88.56 89.09 74.40 char-LSTM 93.68 92.48 100.00 82.64 86.97 83.95 69.67 62.74 87.19 87.79 95.29 76.01 95.13 83.00 78.26 72.34 Test char-CNN char-IntNet-9 93.85 93.79 92.65 94.94 100.00 100.00 84.09 82.31 64.72 87.56 83.67 83.85 69.67 74.33 58.19 65.75 87.81 87.08 88.46 87.98 87.96 95.29 73.68 77.16 94.25 94.42 82.50 83.85 73.27 85.02 73.37 75.46 char-IntNet-5 93.94 92.72 100.00 83.91 87.11 83.60 67.22 60.92 87.19 88.42 97.38 78.02 94.71 84.84 82.13 76.99",
                "metadata": {
                    "filename": "D18-1279.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: Nearest neighbours of different models for frequent words, rare words and OOV words.\nModel Frequent Words newspapers slipped world Commerce Rare Words youthful sessions OOV Words 11-month Thursdays undetermined enclosures stirred wolrd Committee luthier cessions 19-month Thousands undereducated nelsonville clipped worde Computer loughmoe sensible 10-month Tunbridge underpinned char-LSTM entrances snipped lowed Comments wrathful stepanos 12-month Standings undermined newpapers striped wowed Corrects slothful stefanos 14-month Torrance underlined necklaces sti\ufb02ed crowd Clippers ephorus constans 11-inch Phillies underprepared newspaper slipper worli Committee mouthful suppressions 31-month Thursday determined newspapermen slippy worle Community eeyou oppressions 51-month Wednesday overdetermined char-CNN newpapers stripped worse Commodities mouthfeel digressions 1-month Tuesday determinist nitri\ufb01cation shipped werle Communist motul confessions 21-month Ecuador determiners megaphones stopped wereld Comments yourself \ufb01ssions 41-month Windass determiner newpapers blipped eworld Commissioner mouthful recessions 55-month Thursday undermined wallpapers unclipped offworld Commodities mirthful accessions 51-month Saturday determined char-IntNet escapers tripped homeworld Clarence mouthfuls missions 22-month thursdays overdetermined carcases dripped linuxworld Commission youths conversions 25-month Tuesday unexamined spacers slopped westworld Commons slothful possessions 12-month tuesdays predetermined",
                "metadata": {
                    "filename": "D18-1279.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            }
        ]
    },
    "2008.07772.pdf": {
        "normalized_output": [
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201914 EN-FR",
                "Metric": "BLEU",
                "Result": "43.8"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201914 EN-FR",
                "Metric": "TER",
                "Result": "40.3"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201914 EN-FR</s>",
                "Metric": "METEOR",
                "Result": "62.4"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201914 EN-DE</s>",
                "Metric": "BLEU",
                "Result": "30.1"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201914 EN-DE</s>",
                "Metric": "TER",
                "Result": "51.8"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201914 EN-DE",
                "Metric": "METEOR",
                "Result": "48.3"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201914 EN-FR",
                "Metric": "BLEU</s>",
                "Result": "46.4"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201914 EN-FR</s>",
                "Metric": "BLEU",
                "Result": "44.4"
            }
        ],
        "source_documents": [
            {
                "content": "machine translation: Controlling for optimizer insta- bility. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu- man Language Technologies, pages 176\u2013181.",
                "metadata": {
                    "start_index": 0,
                    "type": "Text",
                    "filename": "2008.07772.pdf",
                    "page_number": 6
                }
            },
            {
                "content": "Table 3: BLEU comparison of different encoder and de- coder layers (using ADMIN initialization, on WMT\u201914 EN-FR). In the matrix, each element (i,j) indicates if the model in row i signi\ufb01cantly outperforms the model in column j (+), under-performs j (-), or has no statisti- cally signi\ufb01cant difference (=). ous work), and with sacrebleu.py (version: tok.13a+version.1.2.10). which allows for a safer token-agnostic evaluation (Post, 2018). Learning Curve: We would like to understand why 60L-12L ADMIN is doing better from the opti- mization perspective. Figure 2 (a) plots the learning curve comparing ADMIN to Default initialization. We see that Default has dif\ufb01culty decreasing the training perplexity; its gradients hit NaN, and the resulting model is not better than a random model. In Figure 2 (b), we see that larger models (60L-12L, 36L-36L) are able obtain lower dev perplexities than 6L-6L, implying that the increased capacity does lead to better generalization. Fine-grained error analysis: We are also inter- ested in understanding how BLEU improvements are re\ufb02ected in terms of more nuanced measures. For example, do the deeper models particularly im- prove translation of low frequency words? Do they work better for long sentences? The answer is that the deeper models appear to provide improvements generally across the board (Figure 3).12 Ablation Studies: We experimented with differ- ent number of encoder and decoder layers, given the constraint of a 16GB GPU. Table 3 shows the pairwise comparison of models. We observe that 60L-12L, 48L-12L, and 36L-36L are statistically tied for best BLEU performance. It appears that deeper encoders are more worthwhile than deeper decoders, when comparing 60L-12L to 12L-60L, despite the latter having more parameters.13 12Computed by compare-mt (Neubig et al., 2019). Figure 3: Fine-grained Error Analysis: note the deep model performs better across the board, indicating that it helps translation in general. We also experiment with wider",
                "metadata": {
                    "start_index": 0,
                    "type": "Text",
                    "page_number": 4,
                    "filename": "2008.07772.pdf"
                }
            },
            {
                "content": "layers and 12 decoder layers.9 For each architec- ture, we train with either default initialization (Glo- rot and Bengio, 2010) or ADMIN initialization. The results in terms of BLEU (Papineni et al., 2002), TER (Snover et al., 2006), and METEOR (Lavie and Agarwal, 2007) are reported in Table 1. Similar to previous work (Bapna et al., 2018), we observe that deep 60L-12L Default diverges during training. But the same deep model with ADMIN successfully trains and impressively achieves 2.5 BLEU improvement over the baseline 6L-6L De- fault in both datasets. The improvements are also seen in terms of other metrics: in EN-FR, 60L-12L ADMIN outperforms the 6L-6L models in TER (40.3 vs 42.2) and in METEOR (62.4 vs 60.5). All results are statistically signi\ufb01cant (p < 0.05) with a 1000-sample bootstrap test (Clark et al., 2011). These results indicate that it is feasible to 9We use \u201c(N)L-(M)L\" to denote that a model has N en- coder layers and M decoder layers. N & M are chosen based on GPU (16G) memory constraint. For reproducibility and simplicity, we focused on models that \ufb01t easily on a single GPU system. Taking FR as an example, it takes 2.5 days to train 60L-12L using one DGX-2 (16 V100\u2019s), 2 days to train a 6L-6L using 4 V100\u2019s. train standard (post-LN) Transformers that are very deep.10 These models achieve state-of- the-art results in both datasets. The top re- sults in the literature are compared in Ta- ble 2.11 We list BLEU scores computed with multi-bleu.perl on the tokenization of the downloaded data (commonly done in previ- 10Note: the pre-LN version does train successively on 60L- 12L and achieves 29.3 BLEU in DE & 43.2 in FR. It is better than 6L-6L but worse than 60L-12L ADMIN.",
                "metadata": {
                    "start_index": 0,
                    "page_number": 3,
                    "type": "Text",
                    "filename": "2008.07772.pdf"
                }
            },
            {
                "content": "big transformer architecture (6L-6L) which obtains states-of-the-art results (Edunov et al., 2018). We see that with back-translation, both ADMIN 60L- 12L + BT and ADMIN 36L-12L-768D still signi\ufb01- cantly outperforms its baseline ADMIN 60L-12L. Furthermore, ADMIN 36L-12L-768D achieves new state-of-the-art benchmark results on WMT\u201914 English-French (46.4 BLEU and 44.4 sacreBLEU 15). 5 Conclusion We show that it is feasible to train Transformers at a depth that was previously believed to be dif\ufb01cult. Using ADMIN initialization, we build Transformer- based models of 60 encoder layers and 12 decoder layers. On WMT\u201914 EN-FR and WMT\u201914 EN-EN, these deep models outperform the conventional 6- layer Transformers by up to 2.5 BLEU, and obtain state-of-the-art results. We believe that the ability to train very deep models may open up new avenues of research in NMT, including: (a) Training on extremely large but noisy data, e.g. back-translation (Edunov et al., 2018) and adversarial training (Cheng et al., 2019; Liu et al., 2020b), to see if it can be exploited by the larger model capacity. (b) Analyzing the inter- nal representations, to see if deeper networks can indeed extract higher-level features in syntax and semantics (Belinkov and Glass, 2019). (c) Com- pressing the very deep model via e.g. knowledge distillation (Kim and Rush, 2016), to study the trade-offs between size and translation quality. (d) Analyzing how deep models work (Allen-Zhu and Li, 2020) in theory. 15BLEU+case.mixed+lang.en- fr+numrefs.1+smooth.exp+test.wmt14+tok.13a+version.1.2.10 Acknowledgments We thank Hao Cheng, Akiko Eriguchi, Hany Has- san Awadalla and Zeyuan Allen-Zhu for valuable discussions. References Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A Smith. 2011. Better hypothesis testing for statistical",
                "metadata": {
                    "type": "Text",
                    "filename": "2008.07772.pdf",
                    "start_index": 0,
                    "page_number": 5
                }
            },
            {
                "content": "Table 1: Test results on WMT\u201914 benchmarks, in terms of TER (T\u2193), METEOR (M\u2191), and BLEU. \u2206 shows difference in BLEU score against baseline 6L-6L Default. Best results are boldfaced. 60L-12L ADMIN outper- forms 6L-6L in all metrics with statistical signi\ufb01cance (p < 0.05). Following convention, BLEU is computed by multi-bleu.perl via the standardized tokenization of the publicly-accessible dataset.\nWMT\u201914 English-French (FR) WMT\u201914 English-German (DE) Model #param T\u2193 M\u2191 BLEU\u2191 \u2206 #param T\u2193 M\u2191 BLEU\u2191 \u2206 6L-6L Default 67M 42.2 60.5 41.3 - 61M 54.4 46.6 27.6 - 6L-6L ADMIN 67M 41.8 60.7 41.5 0.2 61M 54.1 46.7 27.7 0.1 60L-12L Default 262M diverge 256M diverge 60L-12L ADMIN 262M 40.3 62.4 43.8 2.5 256M 51.8 48.3 30.1 2.5",
                "metadata": {
                    "filename": "2008.07772.pdf",
                    "page_number": 3,
                    "type": "Table"
                }
            },
            {
                "content": "BLEU via multi-bleu.perl 60L-12L ADMIN (Wu et al., 2019b) (Wang et al., 2019) (Wu et al., 2019a) (Ott et al., 2018) (Vaswani et al., 2017) (So et al., 2019) (Gehring et al., 2017) BLEU via sacreBLEU.py 60L-12L ADMIN FR 43.8 43.3 - 43.2 43.2 41.8 41.3 40.5 FR 41.8 DE 30.1 29.9 29.6 29.7 29.3 28.4 29.8 25.2 DE 29.5 (Ott et al., 2018) (So et al., 2019) 41.4 n/a 28.6 29.2",
                "metadata": {
                    "filename": "2008.07772.pdf",
                    "page_number": 3,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: BLEU comparison of different encoder and de- coder layers (using ADMIN initialization, on WMT\u201914 EN-FR). In the matrix, each element (i,j) indicates if the model in row i signi\ufb01cantly outperforms the model in column j (+), under-performs j (-), or has no statisti- cally signi\ufb01cant difference (=).\nModel BLEU a b c d e f a:6L-6L 41.5 - - - - - b:12L-12L 42.6 + - - - - c:24L-12L 43.3 + + = - = d:48L-12L 43.6 + + = = = e:60L-12L 43.8 + + + = = f:36L-36L 43.7 + + = = = g:12L-60L 43.1 + + = - - - g - - = + + +",
                "metadata": {
                    "filename": "2008.07772.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: Back-translation results on WMT\u201914 EN-FR.\nBLEU via multi-bleu.perl FR 36L-12L-768D ADMIN + BT 46.4 60L-12L ADMIN + BT 46.0 BT (Edunov et al., 2018) 45.6 60L-12L ADMIN 43.8 BLEU via sacreBLEU.py FR 36L-12L-768D ADMIN + BT 44.4 60L-12L ADMIN + BT 44.1 60L-12L ADMIN 41.8 BT (Edunov et al., 2018) -",
                "metadata": {
                    "filename": "2008.07772.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1811.09242.pdf": {
        "normalized_output": [
            {
                "Task": "Word Sense Induction",
                "Dataset": "SemEval 2010 Task 14",
                "Metric": "F-Score (F-S)</s>",
                "Result": "61.7"
            },
            {
                "Task": "Word Sense Induction",
                "Dataset": "SemEval 2010 Task 14",
                "Metric": "V-Measure (V-M)",
                "Result": "9.8"
            },
            {
                "Task": "Word Sense Induction",
                "Dataset": "SemEval 2010 Task 14",
                "Metric": "AVG",
                "Result": "24.59"
            },
            {
                "Task": "Word Sense Induction",
                "Dataset": "SemEval 2010 Task 14",
                "Metric": "Fuzzy B-Cubed (FBC)</s>",
                "Result": "0.33"
            },
            {
                "Task": "Word Sense Induction",
                "Dataset": "SemEval 2013 Task 13",
                "Metric": "Fuzzy B-Cubed (FBC)</s>",
                "Result": "61.7"
            },
            {
                "Task": "Word Sense Induction",
                "Dataset": "SemEval 2013 Task 13",
                "Metric": "Fuzzy normalized mutual information (FNMI)",
                "Result": "7.96"
            },
            {
                "Task": "Word Sense Induction",
                "Dataset": "SemEval 2013 Task 13",
                "Metric": "AVG",
                "Result": "22.16"
            },
            {
                "Task": "Word Sense Induction",
                "Dataset": "arXiv",
                "Metric": "AVG</s>",
                "Result": "46.5"
            }
        ],
        "source_documents": [
            {
                "content": "2004. Finding scienti\ufb01c topics. Proceedings of the Na- tional academy of Sciences 101(suppl 1):5228\u20135235. [Jurgens and Klapaftis 2013] Jurgens, D., and Klapaftis, I. 2013. Semeval-2013 task 13: Word sense induction for graded and non-graded senses. In *SEM, volume 2, 290\u2013 299. [Komninos and Manandhar 2016] Komninos, A., and Man- andhar, S. 2016. Structured generative models of continuous features for word sense induction. COLING 11. [Lau et al. 2012] Lau, J. H.; Cook, P.; McCarthy, D.; New- man, D.; and Baldwin, T. 2012. Word sense induction for novel sense detection. In EACL, 591\u2013601. Association for Computational Linguistics. [Lau, Cook, and Baldwin 2013] Lau, J. H.; Cook, P.; and Baldwin, T. 2013. unimelb: Topic modelling-based word sense induction for web snippet clustering. In SemEval, 217\u2013221. [Manandhar et al. 2010] Manandhar, S.; Klapaftis, I. P.; Dli- gach, D.; and Pradhan, S. S. 2010. Semeval-2010 task 14: Word sense induction & disambiguation. In Proceedings of the 5th international workshop on semantic evaluation, 63\u2013 68. Association for Computational Linguistics. [Manning et al. 2014] Manning, C.; Surdeanu, M.; Bauer, J.; Finkel, J.; Bethard, S.; and McClosky, D. 2014. The stanford corenlp natural language processing toolkit. In Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations, 55\u201360. [Mikolov et al. 2013] Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed represen- tations of words and phrases and their compositionality. In NIPS, 3111\u20133119. [Pelevina et al. 2016] Pelevina, M.; Are\ufb01ev, N.; Biemann, C.; and Panchenko, A. 2016. Making sense of word em- beddings. In Proceedings of the 1st Workshop on Represen- tation Learning for NLP, 174\u2013183. [Shu, Long, and Meng 2009] Shu, L.; Long, B.; and Meng, W. 2009. A latent topic model for complete entity resolu- tion. In ICDE, 880\u2013891. IEEE. [Song et al. 2007] Song, Y.; Huang, J.; Councill, I. G.; Li, J.; and Giles, C. L.",
                "metadata": {
                    "filename": "1811.09242.pdf",
                    "type": "Text",
                    "page_number": 8,
                    "start_index": 1800
                }
            },
            {
                "content": "above, i.e. (C1) and (C2), of the WSI task (Brody and Lapata 2009). How- ever, it is not \ufb02exible with regards to (C3), or the sense granularity problem, as it requires the users to specify the number of senses: Current systems (Wang et al. 2015; Chang, Pei, and Chen 2014) required to set the number of senses to a small number (set to 3 or 5 in the literature) to get a good accuracy, however many words may have a large number of senses, e.g. play in Figure 1.",
                "metadata": {
                    "start_index": 3598,
                    "filename": "1811.09242.pdf",
                    "page_number": 1,
                    "type": "Text"
                }
            },
            {
                "content": "Shu, L.; Long, B.; and Meng, W. 2009. A latent topic model for complete entity resolu- tion. In ICDE, 880\u2013891. IEEE. [Song et al. 2007] Song, Y.; Huang, J.; Councill, I. G.; Li, J.; and Giles, C. L. 2007. Ef\ufb01cient topic-based unsupervised name disambiguation. In JCDL, 342\u2013351. ACM. [Song 2016] Song, L. 2016. Word embeddings, sense em- beddings and their application to word sense induction. The University of Rochester, April. [Stevenson and Wilks 2003] Stevenson, M., and Wilks, Y. 2003. Word sense disambiguation. The Oxford Handbook of Comp. Linguistics 249\u2013265. [Tang et al. 2012] Tang, J.; Fong, A. C.; Wang, B.; and Zhang, J. 2012. A uni\ufb01ed probabilistic framework for name disambiguation in digital library. IEEE Transactions on Knowledge and Data Engineering 24(6):975\u2013987. [Teh et al. 2004] Teh, Y. W.; Jordan, M. I.; Beal, M. J.; and Blei, D. M. 2004. Sharing clusters among related groups: Hierarchical dirichlet processes. In NIPS, 1385\u20131392. [Tsvetkov et al. 2014] Tsvetkov, Y.; Schneider, N.; Hovy, D.; Bhatia, A.; Faruqui, M.; and Dyer, C. 2014. Augmenting english adjective senses with supersenses. In LREC. [Wang et al. 2015] Wang, J.; Bansal, M.; Gimpel, K.; Ziebart, B. D.; and Clement, T. Y. 2015. A sense-topic model for word sense induction with unsupervised data en- richment. Transactions of the Association for Computa- tional Linguistics 3:59\u201371. [Yao and Van Durme 2011] Yao, X., and Van Durme, B. 2011. Nonparametric bayesian word sense induction. In",
                "metadata": {
                    "type": "Text",
                    "filename": "1811.09242.pdf",
                    "start_index": 3601,
                    "page_number": 8
                }
            },
            {
                "content": "an arbitrary large number, i.e. 100. We set the number of iterations to 2000 and run the Gibbs sampler. Following the convention of previous works (Lau et al. 2012; Goyal and Hovy 2014; Wang et al. 2015), we assume convergence when the number of iterations is high. However, due to the randomized nature of Gibbs sampling, we report the average scores over 5 runs of Gibbs sampling. We then use the distribution \u03b8s|d as shown in Equation 1 as the solution of the WSI problem. 2https://www.cs.york.ac.uk/semeval2010_WSI 3https://www.cs.york.ac.uk/semeval-2013/ task13/",
                "metadata": {
                    "filename": "1811.09242.pdf",
                    "start_index": 3606,
                    "type": "Text",
                    "page_number": 4
                }
            },
            {
                "content": "(a) SemEval 2010 WSI dataset\nModel F-S V-M AVG \u03b4(#S) LDA 60.7 4.4 16.34 1.40 Spectral 61.5 4.5 16.64 1.98 LVMs HC HC+Zipf 44.4 35.1 11.5 15.2 22.62 23.10 1.15 3.81 BNP-HC 23.1 21.4 22.23 11.77 NBEs CRP-PPMI SE-WSI-\ufb01x 57.7 55.1 2.9 9.8 12.94 23.24 2.09 1.35 AutoSense\u2212wp 59.3 9.2 23.36 2.16 AutoSense\u2212sw 61.1 8.6 22.92 1.42 AutoSense 61.7 9.8 24.59 0.33 AutoSenses=5 62.9 10.1 25.20 0.32 AutoSenses=100 61.2 9.6 24.23 0.78",
                "metadata": {
                    "filename": "1811.09242.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "(b) SemEval 2013 WSI dataset\nModel F-BC F-NMI AVG Substitution AI-KU 39.0 6.50 15.92 LVMs Unimelb STM 48.3 53.5 6.00 6.96 17.02 19.30 NBEs WG DIVE 58.1 49.9 1.60 3.50 9.64 13.22 LVMs + NBEs STM+w2v MCC 55.4 55.6 7.14 7.62 19.89 20.58 AutoSense\u2212wp 55.7 7.69 20.69 AutoSense\u2212sw 61.4 7.36 21.26 AutoSense 61.7 7.96 22.16 AutoSenses=7 61.7 7.97 22.17 AutoSenses=100 61.0 7.25 21.03 with additional contexts STM +actual +ukWac 59.1 54.5 9.39 9.74 23.56 23.04 AutoSense +actual 62.2 9.55 24.37",
                "metadata": {
                    "filename": "1811.09242.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Six of the 15 senses of the target verb book using AutoSense with S = 15. The word lists shown are prepro- cessed to remove stopwords and the target word. The \ufb01rst three senses are senses which are assigned at least once to an instance document. The last three are garbage senses.\nSense Word distribution #Docs 1 hotel tour tourist summer \ufb02ight 22 2 month ticket available performance 3 3 guest of\ufb01ce stateroom class suite 3 * advance overseas line popular japan 0 * email day buy unable tour 0 * sort basic tour time 0",
                "metadata": {
                    "filename": "1811.09242.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: Statistics of the number of senses of target words/names in the datasets used in the paper.\nDataset Min Max Mean StdDev SemEval 2010 2 16 7.68 3.35 SemEval 2013 2 7 3.85 1.40 PubMed 1 28 10.41 7.68 Arnet 1 112 14.18 18.02",
                "metadata": {
                    "filename": "1811.09242.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Model S = 5 S = 25 S = 50 S = 100 LDA 31.5 13.4 9.8 8.2 HC 46.3 46.3 44.4 41.7 STM 52.8 55.0 55.5 55.0",
                "metadata": {
                    "filename": "1811.09242.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "(a) Arnet Dataset\nModel S = 5 S = 25 S = 50 S = 100 LDA 41.4 13.3 8.9 9.0 HC 42.5 44.1 41.6 41.3 STM 44.9 44.4 44.9 41.9 AutoSense 44.4 45.5 46.6 46.5",
                "metadata": {
                    "filename": "1811.09242.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            }
        ]
    },
    "1702.02098.pdf": {
        "normalized_output": [
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2003 - English",
                "Metric": "F1",
                "Result": "90.54"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "Ontonotes v5 - English",
                "Metric": "F1",
                "Result": "86.99"
            }
        ],
        "source_documents": [
            {
                "content": "set using these parameters. Note that we do not in the final stage include the development set as training data as has been done in some previous work, and so do not directly compare to results from other papers which do so. We evaluate test-time speed using our top- performing trained models. All timing experi- ments were run on a nVidia Titan X GPU with a 2.4GHz Intel Xeon CPU. We do not include data",
                "metadata": {
                    "start_index": 3607,
                    "filename": "1702.02098.pdf",
                    "type": "Text",
                    "page_number": 12
                }
            },
            {
                "content": "A Appendix A.1 Optimization and data pre-processing Our models are trained end-to-end using back- propagation and mini-batched Adam (Kingma and Ba, 2015) SGD. We use dropout regularization (Srivastava et al., 2014) on the input embeddings and \ufb01nal dilation layer of each block, along with the dropout regularizer described in Ma et al. (2017) using a single Monte Carlo sample for each training example. We also found word dropout (Dai and Le, 2015; Lample et al., 2016) crucial for learning a high-quality representation for out- of-vocabulary words. We used the modi\ufb01ed ver- sion of identity initialization (Le et al., 2015) re- ported by Yu and Koltun (2016) to initialize our dilated layers, which we found to perform the best in initial experiments compared to orthogonal and Xavier initialization (Glorot and Bengio, 2010). Since our models use the same number of \ufb01lters in each dilated layer, this initialization simpli\ufb01es to setting the parameters corresponding to the cen- tral token to the identity matrix, and all other pa- rameters (corresponding to left and right context) to zero. All other layers (embeddings, projections) were initialized using normally distributed Xavier initialization. As in previous work, we found that initializ- ing the word embedding lookup table with pre- trained embeddings was vital to achieve good per- formance. In initial experiments, we found the 100-dimensional skip-n-gram (Ling et al., 2013) embeddings of Lample et al. (2016) to outperform the 50-dimensional word embeddings of Collobert et al. (2011), and so we use these 100-dimensional embeddings in all experiments. We concate- nate a 5-dimensional word shape vector based on whether the token was all capitalized, not capital- ized, \ufb01rst-letter capitalized or contained a capital letter. We preprocessed the data by replacing all digits with 0, but did not lowercase thus our em- beddings are case-sensitive. We use the parameters of the trained sentence models to initialize the parameters of",
                "metadata": {
                    "page_number": 12,
                    "type": "Text",
                    "start_index": 0,
                    "filename": "1702.02098.pdf"
                }
            },
            {
                "content": "with bt (1) = B (it): We apply a simple af\ufb01ne transformation Wo to this \ufb01nal representation to obtain per-class scores for each token xt: 4.2 Training Our main focus is to apply the ID-CNN an en- coder to produce per-token logits for the \ufb01rst con- ditional model described in Sec. 2.1, where tags are conditionally independent given deep features, since this will enable prediction that is paralleliz- able across the length of the input sequence. Here, maximum likelihood training is straightforward because the likelihood decouples into the sum of the likelihoods of independent logistic regression problems for every tag, with natural parameters given by Eqn. (9): We can also use the ID-CNN as logits for the CRF model (Eqn. (2)), where the partition function and its gradient are computed using the forward-backward algorithm. We next present an alternative training method that helps bridge the gap between these two tech- niques. Sec. 2.1 identi\ufb01es that the CRF has prefer- able sample complexity and accuracy since pre- diction directly reasons in the space of structured outputs. In response, we compile some of this rea- soning in output space into ID-CNN feature ex- traction. Instead of explicit reasoning over output labels during inference, we train the network such that each block is predictive of output labels. Sub- sequent blocks learn to correct dependency viola- tions of their predecessors, re\ufb01ning the \ufb01nal se- quence prediction. To do so, we \ufb01rst de\ufb01ne predictions of the model after each of the Lb applications of the block. Let ht (k) be the result of applying the ma- trix Wo from (9) to bt (k), the output of block k. We minimize the average of the losses for each application of the block: By rewarding accurate predictions after each application of the block, we learn a model where later blocks are used to re\ufb01ne initial predictions.",
                "metadata": {
                    "start_index": 1801,
                    "type": "Text",
                    "page_number": 4,
                    "filename": "1702.02098.pdf"
                }
            },
            {
                "content": "the data by replacing all digits with 0, but did not lowercase thus our em- beddings are case-sensitive. We use the parameters of the trained sentence models to initialize the parameters of the docu- ment models in order to signi\ufb01cantly speed up the rate of convergence of the document models. A.2 Data details Entities in the CoNLL-2003 corpus are la- beled with one of four types: PER, ORG, LOC or MISC, with a fairly even distribu- tion over the four entity types. OntoNotes contains a larger and more diverse set of 19 different entity types, adding: ORDINAL, PRODUCT, NORP, WORK OF ART, LAN- GUAGE, MONEY, PERCENT, CARDINAL, GPE, TIME, DATE, FAC, LAW, EVENT and QUANTITY. The OntoNotes corpus also covers a wider range of text genres, including telephone conversations, web text, broadcast news and trans- lated documents, whereas the CoNLL-2003 text covers only newswire. The combined entity types and boundary encodings result in 17 possible out- put labels in the CoNLL-2003 corpus and 74 la- bels in the OntoNotes corpus. The sizes of the two corpora in terms of documents, sentences, tokens and entities are given in Table 8. A.3 Evaluation To select hyperparameters, we iteratively perform grid search over increasingly fine-grained settings of dropout, learning rate, Adam {2 and \u20ac param- eters, gradient clipping threshold, number of di- lated layers, number of repeated blocks, regular- izer penalty and batch size. Since we found the variance in score between runs to vary signifi- cantly, in the last iteration of grid search, we ran each setting of parameters three times and aver- aged their scores on the validation set. Of these, we ran the top ten settings ten times, and took the parameters which averaged the highest F1 on the development set, and report scores on the test set using these parameters. Note that we do not in the final stage include the development set as training data as has been done in some previous work, and so do not directly compare to results from",
                "metadata": {
                    "page_number": 12,
                    "type": "Text",
                    "filename": "1702.02098.pdf",
                    "start_index": 1810
                }
            },
            {
                "content": "Table 1 lists F1 scores of models predicting with sentence-level context on CoNLL-2003. For mod- els that we trained, we report F1 and standard deviation obtained by averaging over 10 random restarts. The Viterbi-decoding Bi-LSTM-CRF and ID-CNN-CRF and greedy ID-CNN obtain the highest average scores, with the ID-CNN- CRF outperforming the Bi-LSTM-CRF by 0.11 points of F1 on average, and the Bi-LSTM-CRF out-performing the greedy ID-CNN by 0.11 as well. Our greedy ID-CNN outperforms the Bi- LSTM and the 4-layer CNN, which uses the same number of parameters as the ID-CNN, and per- forms similarly to the 5-layer CNN which uses more parameters but covers the same effective in- put width. All CNN models out-perform the Bi-\nModel F1 Ratinov and Roth (2009) 86.82 Collobert et al. (2011) 86.96 Lample et al. (2016) 90.33 Bi-LSTM 89.34 \u00b1 0.28 4-layer CNN 89.97 \u00b1 0.20 5-layer CNN 90.23 \u00b1 0.16 ID-CNN 90.32 \u00b1 0.26 Collobert et al. (2011) 88.67 Passos et al. (2014) 90.05 Lample et al. (2016) 90.20 Bi-LSTM-CRF (re-impl) 90.43 \u00b1 0.12 ID-CNN-CRF 90.54 \u00b1 0.18",
                "metadata": {
                    "filename": "1702.02098.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Model Speed Bi-LSTM-CRF 1\u00d7 Bi-LSTM 9.92\u00d7 ID-CNN-CRF 1.28\u00d7 5-layer CNN 12.38\u00d7",
                "metadata": {
                    "filename": "1702.02098.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Relative test-time speed of sentence mod- els, using the fastest batch size for each model.5\nModel w/o DR w/ DR Bi-LSTM 88.89 \u00b1 0.30 89.34 \u00b1 0.28 4-layer CNN 89.74 \u00b1 0.23 89.97 \u00b1 0.20 5-layer CNN 89.93 \u00b1 0.32 90.23 \u00b1 0.16 Bi-LSTM-CRF 90.01 \u00b1 0.23 90.43 \u00b1 0.12 4-layer ID-CNN 89.65 \u00b1 0.30 90.32 \u00b1 0.26",
                "metadata": {
                    "filename": "1702.02098.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: F1 score of models trained to predict document-at-a-time. Our greedy ID-CNN model performs as well as the Bi-LSTM-CRF.\nModel F1 4-layer ID-CNN (sent) 90.32 \u00b1 0.26 Bi-LSTM-CRF (sent) 90.43 \u00b1 0.12 4-layer CNN \u00d7 3 90.32 \u00b1 0.32 5-layer CNN \u00d7 3 90.45 \u00b1 0.21 Bi-LSTM 89.09 \u00b1 0.19 Bi-LSTM-CRF 90.60 \u00b1 0.19 ID-CNN 90.65 \u00b1 0.15",
                "metadata": {
                    "filename": "1702.02098.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5: Comparing ID-CNNs with 1) back- propagating loss only from the \ufb01nal layer (1-loss) and 2) untied parameters across blocks (noshare)\nModel F1 ID-CNN noshare 89.81 \u00b1 0.19 ID-CNN 1-loss 90.06 \u00b1 0.19 ID-CNN 90.65 \u00b1 0.15",
                "metadata": {
                    "filename": "1702.02098.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 6: Relative test-time speed of document models (fastest batch size for each model).\nModel Speed Bi-LSTM-CRF 1\u00d7 Bi-LSTM 4.60\u00d7 ID-CNN 7.96\u00d7",
                "metadata": {
                    "filename": "1702.02098.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 7: F1 score of sentence and document mod- els on OntoNotes.\nModel F1 Speed Ratinov and Roth (2009)6 83.45 Durrett and Klein (2014) 84.04 Chiu and Nichols (2016) 86.19 \u00b1 0.25 Bi-LSTM-CRF 86.99 \u00b1 0.22 1\u00d7 Bi-LSTM-CRF-Doc 86.81 \u00b1 0.18 1.32\u00d7 Bi-LSTM 83.76 \u00b1 0.10 24.44\u00d7 ID-CNN-CRF (1 block) 86.84 \u00b1 0.19 1.83\u00d7 ID-CNN-Doc (3 blocks) 85.76 \u00b1 0.13 21.19\u00d7 ID-CNN (3 blocks) 85.27 \u00b1 0.24 13.21\u00d7 ID-CNN (1 block) 84.28 \u00b1 0.10 26.01\u00d7",
                "metadata": {
                    "filename": "1702.02098.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Data Train Dev Test CoNLL-2003 Tok 204,567 51,578 46,666 Sent 14,041 3,250 3,453 Doc 945 215 230 Ent 23,499 5,942 5,648 OntoNotes 5.0 Tok 1,088,503 147,724 Sent 59,924 8,528 8,262 Doc 2,483 319 322 Ent 81,828 11,066 11,257",
                "metadata": {
                    "filename": "1702.02098.pdf",
                    "page_number": 12,
                    "type": "Table"
                }
            }
        ]
    },
    "1906.05012.pdf": {
        "normalized_output": [
            {
                "Task": "Summarization",
                "Dataset": "Gigaword</s>",
                "Metric": "ROGUE-1",
                "Result": "39.11"
            },
            {
                "Task": "Summarization",
                "Dataset": "Gigaword</s>",
                "Metric": "ROGUE-2",
                "Result": "19.78"
            },
            {
                "Task": "Summarization",
                "Dataset": "Gigaword</s>",
                "Metric": "ROGUE-L",
                "Result": "36.87"
            }
        ],
        "source_documents": [
            {
                "content": "weights are shared between the article and template encoders. The k of k-max pooling is set to 10. L2 weight de- cay with \u03bb = 3\u00d710\u22126 is performed over all train- able variables. The initial learning rate is 0.001 and multiplied by 0.1 every 10K steps. Dropout between layers is applied. BiSET. A two-layer BiLSTM is used as the en- coder, and another two-layer LSTM as the de- coder. The sizes of word embeddings and LSTM hidden states are both set to 500. We only apply dropout in the LSTM stack with a rate of 0.3. The learning rate is set to 0.001 for the \ufb01rst 50K steps and halved every 10K steps. Beam search with size 5 is applied to search for optimal answers. 3.2 Evaluation Metrics Following previous work (Nallapati et al., 2016; Zhou et al., 2017; Cao et al., 2018a), we use the standard F1 scores of ROUGE-1, ROUGE- 2 and ROUGE-L (Lin and Hovy, 2003) to eval- uate the selected templates and generated sum- maries, where the of\ufb01cial ROUGE script5 is ap- plied. We employ the normalized discounted cu- mulative gain (NDCG) (J\u00a8arvelin and Kek\u00a8al\u00a8ainen, 2002) from information retrieval to evaluate the Fast Rerank module. 4 Results and Analysis In this section, we report our experimental results with thorough analysis and discussions. 4.1 Performance of Retrieve The Retrieve module is intended to narrow down the search range for a best template. We evaluated this module by considering three types of tem- plates: (a) Random means a randomly selected summary from the training corpus; (b) Retrieve- top is the highest-ranked summary by Retrieve; (c) N-Optimal means among the N top search results, the template is speci\ufb01ed as the summary with largest ROUGE score with gold summary. As the results show in Table 1, randomly se- lected templates are totally irrelevant and unhelp- ful. When they are replaced by the Retrieve-top 5The ROUGE evaluation option: -m -n 2 -w 1.2",
                "metadata": {
                    "page_number": 5,
                    "type": "Text",
                    "start_index": 1802,
                    "filename": "1906.05012.pdf"
                }
            },
            {
                "content": "the summaries of which will be treated as candidate templates. 2.2 Fast Rerank The above retrieval process is essentially based on super\ufb01cial word matching and cannot measure the deep semantic relationship between two articles. Therefore, the Fast Rerank module is developed to identify a best template from the candidates based on their deep semantic relevance with the source article. We regard the candidate with highest rel- evance as the template. As illustrated in Figure 1, this module consists of a Convolution Encoder Block, a Similarity Matrix and a Pooling Layer. Convolution Encoder Block. This block maps the input article and its candidate templates into high-level representations. The popular ways to this are either by using recurrent neural network (RNN) or a stack of convolutional neural network (CNN), while none of them are suitable for our problem. This is because a source article is usu- ally much longer than a template, and both RNN and CNN may lead to semantic irrelevance after encodings. Instead, we implement a new convo- lution encoder block which includes a word em- bedding layer, a 1-D convolution followed by a non-linearity function, and residual connections (Gehring et al., 2017). Formally, given word embeddings {ei}E i=1 \u2208 Rd of an article, we use a 1-D convolution with kernel k \u2208 R2d\u00d7kd and bias bh \u2208 R2d to extract the n-gram features: where hi \u2208 R2d. We pad both sides of an arti- cle/template with zeros to keep \ufb01xed length. Af- ter that, we employ the gated linear unit (GLU) (Dauphin et al., 2017) as our activation function to control the proportion of information to pass through. GLU takes half the dimension of hi as input and reduces the input dimension to d. Let hi = [h1 i;h2 i], where h1 i \u2208 Rd, we have: i,h2 where ri \u2208 Rd, \u03c3 is the sigmoid function, and \u2297 means element-wise multiplication. To retain the original information, we add residual connections",
                "metadata": {
                    "start_index": 1801,
                    "page_number": 2,
                    "filename": "1906.05012.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "templates, the results improve apparently, demon- strating the relatedness of top-ranked summaries to gold summaries. Furthermore, when the N- Optimal templates are used, additional improve- ments can be observed as N grows. This trend is also con\ufb01rmed by Figure 3, in which the ROUGE scores increase before 30 and stabilize afterwards. These results suggest that the ranges given by Re- trieve indeed help to \ufb01nd quality templates. 4.2 Fast Rerank As mentioned before, the role of Fast Rerank is to re-rank the initial search results and return a best template for summarization. To examine the effect of this module, we studied its ranking quality un- der different ranges as in Section 4.1. The original rankings by Retrieve are presented for comparison with the NDCG metric. We regard the ROUGE- 2 score of each candidate template with the ref- erence summary as the ground truth. As shown in Figure 4, Fast Rerank consistently provides en- hanced rankings over the original. Interaction Approaches In Section 2.3, we also explored three alternative approaches to integrating an article with its tem- plate. The results are shown in Table 2, from which we can note that none of these approaches help yield satisfactory performance. Even though DCN Attention works impressively in machine reading comprehension, it performs even worse in this task than the simple concatenation. We conjecture the reason is that the DCN Attention attempts to fuse the template information into an article as in ma- chine reading comprehension, rather than selects key information from the two to form an enhanced article representation. ROUGE-1 ROUGE-2 ROUGE-L Table 2: Results of different interaction approaches. 4.4 BiSET The overall performance of all the studied mod- els is shown in Table 3. The results show that our model signi\ufb01cantly outperforms all the baseline models and sets a new state of the art for abstrac- tive sentence summarization. To evaluate the im- pact of templates on our model, we also",
                "metadata": {
                    "type": "Text",
                    "filename": "1906.05012.pdf",
                    "start_index": 0,
                    "page_number": 6
                }
            },
            {
                "content": "vocabulary size and fed through a softmax layer to output the target word distribution: p(wt|w1,...,wt\u22121) = softmax(Wpha 2.5 Training The Retrieve module involves an unsupervised process with traditional indexing and retrieval techniques. For Fast Rerank, since there is no ground truth available, we use ROUGE-13 (Lin and Hovy, 2003) to evaluate the saliency of a can- didate template with respect to the gold summary of current source article. Therefore, the loss func- tion is de\ufb01ned as: where s is a score predicted by Equation 9, and N is the product of the training set size, D, and the number of retrieved templates for each article. For the BiSET module, the loss function is chosen as the negative log-likelihood between the generated summary, w, and the true summary, w\u2217: where L is the length of the true summary, \u03b8 con- tains all the trainable variables, and x and y denote the source article and the template, respectively. 3 Experiments In this section, we introduce our evaluations on a standard dataset. 3.1 Dataset and Implementation The dataset used for evaluation is Annotated En- glish Gigaword (Napoles et al., 2012), a parallel corpus formed by pairing the \ufb01rst sentence of an article with its headline. For a fair comparison, we use the version preprocessed by Rush et al. (2015)4 as previous work. During training, both the Fast Rerank and BiSET modules have a batch size of 64 with the Adam optimizer (Kingma and Ba, 2015). We also apply grad clipping (Pascanu et al., 2013) with a 3We also tried ROUGE-2 and ROUGE-L, but ROUGE-1 shows to be more suitable. range of [-5,5]. The differences of the two mod- ules in settings are listed below. Fast Rerank. We set the size of word embeddings to 300, the convolution encoder block number to 1, and the kernel size of CNN to 3. The weights are shared between the article and template encoders. The k of k-max pooling is set to 10. L2 weight de- cay with \u03bb = 3\u00d710\u22126 is performed over all train- able variables. The initial learning",
                "metadata": {
                    "type": "Text",
                    "filename": "1906.05012.pdf",
                    "start_index": 0,
                    "page_number": 5
                }
            },
            {
                "content": "Table 1: Performance of different types of templates.\nType ROUGE-1 ROUGE-2 ROUGE-L Random 2.58 0.00 2.48 Retrieve-top 23.46 7.67 20.94 5-Optimal 32.69 11.74 28.71 10-Optimal 35.90 13.32 31.42 15-Optimal 37.82 16.79 34.08 20-Optimal 38.92 17.72 34.94 30-Optimal 40.49 19.01 36.10",
                "metadata": {
                    "filename": "1906.05012.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Interaction method Concatenation 32.26 15.30 30.19 Concate+multi self-att 33.15 15.93 31.21 DCN Attention 31.53 13.77 27.96 Bi-selective layer 39.11 19.78 36.87",
                "metadata": {
                    "filename": "1906.05012.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Model ROUGE-1 ROUGE-2 ABS\u2021 (Rush et al., 2015) 29.55 11.32 26.42 ABS+\u2021 (Rush et al., 2015) 29.78 11.89 26.97 RAS-Elman\u2021 (Chopra et al., 33.78 15.97 31.15 2016) Featseq2seq\u2021 (Nallapati 32.67 15.59 30.64 et al., 2016) Open-NMT\u2021 (Klein et al., 34.07 16.35 31.78 2017) SEASS\u2021 (Zhou et al., 2017) 36.15 17.54 33.63 S2S+CGU\u2021 (Lin et al., 2018) 36.30 18.00 33.80 FTSum\u2021 (Cao et al., 2018b) 37.27 17.65 34.24 R3Sum\u2021 (Cao et al., 2018a) 37.04 19.03 34.46 BiSET 39.11 19.78 36.87",
                "metadata": {
                    "filename": "1906.05012.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Performance of all the models, where results marked with \u2021 are taken from the corresponding papers.\nTemplate Type ROUGE-1 ROUGE-2 ROUGE-L Random 33.85 15.83 31.14 5-rerank 37.69 18.62 34.38 10-rerank 38.34 19.35 34.97 20-rerank 38.89 19.64 36.67 30-rerank 39.11 19.78 36.87",
                "metadata": {
                    "filename": "1906.05012.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Model Concatenation 32.26 15.30 30.19 BiSET without T2A 34.51 16.55 31.17 BiSET without A2T 39.02 19.21 36.02 BiSET(full) 39.11 19.78 36.87",
                "metadata": {
                    "filename": "1906.05012.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 6: Results of human evaluation.\nModel Info Concise Read R3Sum 3.30 3.83 3.90 Open-NMT 3.26 3.69 3.86 BiSET(random template) 3.09 3.69 3.71 BiSET(without A2T) 3.24 3.75 3.72 BiSET(best template) 3.35 3.98 3.93 Reference 3.55 3.91 3.89",
                "metadata": {
                    "filename": "1906.05012.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Source factory orders for manufactured goods rose #.# percent in September, the commerce depart- ment said here Thursday. Ref September factory orders up #.# percent. Temp January factory orders in US up #.# percent. BiSET factory orders up #.# percent in September.",
                "metadata": {
                    "filename": "1906.05012.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 7: Examples of the generated templates and sum- maries by our model. \u2018#\u2019 refers to masked numbers.\nSource some #.# billion people worldwide are expected to watch Germany face Costa Rica on television at the opening match of football\u2019s World Cup, German public broadcaster zdf said Thursday. Ref #.# billion tv viewers expected for opening World Cup match. Temp billions around world watch the Olympic Games opening ceremony. BiSET #.# billions around world expected to watch World Cup.",
                "metadata": {
                    "filename": "1906.05012.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            }
        ]
    },
    "K19-1036.pdf": {
        "normalized_output": [
            {
                "Task": "Dialogue Act Classification",
                "Dataset": "Switchboard Dialog Act Corpus (SWDA)",
                "Metric": "Accuracy",
                "Result": "82.3"
            },
            {
                "Task": "Dialogue Act Classification",
                "Dataset": "ICSI Meeting Recorder Dialog Act Corpus (MRDA)",
                "Metric": "Accuracy",
                "Result": "92.2"
            },
            {
                "Task": "Dialogue Act Classification",
                "Dataset": "Dailydialog (DyDA)",
                "Metric": "Accuracy",
                "Result": "88.1"
            }
        ],
        "source_documents": [
            {
                "content": "the test, the optimal DA or topic sequence is calcu- lated using the Viterbi algorithm (Viterbi, 1967). 3.4 Automatically Acquiring Topic Labels To avoid expensive human annotation and to im- prove the generalisability of our model, we pro- pose to label the topic of each utterance of the datasets using LDA (Blei et al., 2003). While per- plexity has been widely used for model selection for LDA (Lin, 2011; He et al., 2012), we employ a topic coherence measure proposed by (R\u00a8oder et al., 2015) to determine the optimal topic number for each dataset, which combines the indirect co- sine measure with the normalised pointwise mu- tual information (Bouma, 2009, NPMI) and the Boolean sliding window. Empirically, we found the latter yields much better topic clusters than perplexity for supporting DA classi\ufb01cation. We treat each conversation as a document and train topic models using Gensim with topic num- ber settings ranging from 10 to 100 (using an in- crement step of 10). Gibbs sampling is used to es- timate the model posterior and for each model we run 1,000 iterations. For each trained model, we calculate the averaged coherence score of the ex- tracted topics using Gensim1, an implementation following (R\u00a8oder et al., 2015). Figure 2 shows the topic coherence score for each topic number setting for all datasets, from which we determine that the optimal topic number setting for SWDA, DyDA, and MRDA are 60, 30, and 30, respec- tively. Based on the optimal models (i.e., a trained LDA model using the optimal topic number set- ting), we assign topic labels to the datasets with two different strategies, i.e., conversation-level la- belling (conv) and utterance-level labelling (utt). 1https://radimrehurek.com/gensim/models/ coherencemodel.html For conversation-level labelling, we assign the topic label with the highest marginal probabil- ity to the conversation based on the correspond- ing per-document topic proportion estimated by LDA. Every utterance of the conversation then",
                "metadata": {
                    "page_number": 5,
                    "filename": "K19-1036.pdf",
                    "start_index": 0,
                    "type": "Text"
                }
            },
            {
                "content": "rep- resentations (i.e., lt and vt). Finally, the target la- bels (i.e., yt and zt) are predicted in the CRF layer. 3.1 Shared Utterance Encoder In our model, we adopt a shared utterance encoder to encode the input utterances. Such a design is based on the rationale that the shared encoder can transfer parameters between two tasks and re- duce the risk of over\ufb01tting (Ruder, 2017). Speci\ufb01- cally, the shared utterance encoder is implemented using the bidirectional gated recurrent unit (Cho et al., 2014, BiGRU), which encodes each utter- ance u; = (wi), of a conversation C,, as a se- ries of hidden states (hi). Here, 7 indicates the timestamp of a sequence, and we define h; as fol- lows where \u2295 is an operation for concatenating two \u2212\u2192 \u2190\u2212 h i h i vectors, and t and t are the i-th hidden state of the forward gated recurrent unit (Cho et al., 2014, GRU) and backward GRU for wi tively. Formally, the forward GRU t, respec- \u2212\u2192 h i t is com- puted as follows where e? is the concatenation of the word embed- ding and the character embedding of word wi. Fi- nally, the backward GRU encodes u; from the re- verse direction (ie. wj/* \u2014> w}) and generates = (hi), following the same formulation as the for- ward GRU. 3.2 Task-speci\ufb01c Attention Recall that one of the key challenges of our model is to capture for each utterance, information about both DAs and topics, as well as information about the interactions between them. We address this challenge by incorporating into our model a novel task-speci\ufb01c dual-attention mechanism, which ac- counts for both DA and topic information ex- tracted from utterances. In addition, DAs and top- ics are semantically relevant to different words in an utterance. With the proposed attention mecha- nism, our model can also assign different weights to the words of an utterance by learning the degree of importance of the words to the DA or topic la- belling task, i.e., promoting the words which are important to the task and reducing the noise intro- duced",
                "metadata": {
                    "type": "Text",
                    "start_index": 1797,
                    "filename": "K19-1036.pdf",
                    "page_number": 3
                }
            },
            {
                "content": "tated with 5 DAs, i.e., Statement (S), Question (Q), Floorgrabber (F), Backchannel (B), and Dis- ruption (D). The average number of utterances per conversation is 1,496. There are no manually an- notated topic labels available for this dataset. 4.2 Implementation Details For all experimental datasets, the top 85% high- est frequency words were indexed. For SWDA and MRDA, we split training/validation/testing datasets following (Stolcke et al., 2000; Lee and Dernoncourt, 2016). For DyDA, we used the standard split from the original dataset (Li et al., 2017). The statistics of the experimen- tal datasets are summarised in Table 1. We rep- resented input data with 300-dimensional Glove word embeddings (Pennington et al., 2014) and 50-dimensional character embeddings (Ma and Hovy, 2016). We set the dimension of the hid- den layers (i.e., hi t, gt and st) to 256 and applied a dropout layer to both the shared encoder and the sequence tagger at a rate of 0.2. The Adam opti- miser (Kingma and Ba, 2015) was used for train- ing with an initial learning rate of 0.001 and a weight decay of 0.0001. Each utterance in a mini- batch was padded to the maximum length for that batch, and the maximum batch-size allowed was 4.3 Baselines We compare the proposed DAH-CRF model in- corporating utterance-level topic labels extracted by LDA (denoted as DAH-CRF+LDAutt) against \ufb01ve strong baselines and two variants of our own models: JAS5: A generative joint, additive, sequential model of topics and speech acts in patient-doctor communication (Wallace et al., 2013); DRLM-Cond6: A latent variable recurrent neural network for DA classi\ufb01cation (Ji et al., 2016); Bi-LSTM-CRF7: A hierarchical Bi-LSTM with a CRF to classify DAs (Kumar et al., 2018); CRF-ASN: An attentive structured network with a CRF for DA classi\ufb01cation (Chen et al., 2018); SelfAtt-CRF: A hierarchical Bi-GRU with self- attention and CRF (Raheja and Tetreault, 2019); DAH-CRF+MANUALconv: Use the manually annotated conversation-level",
                "metadata": {
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 0,
                    "filename": "K19-1036.pdf"
                }
            },
            {
                "content": "DA classi\ufb01cation (Chen et al., 2018); SelfAtt-CRF: A hierarchical Bi-GRU with self- attention and CRF (Raheja and Tetreault, 2019); DAH-CRF+MANUALconv: Use the manually annotated conversation-level topic labels (i.e., each utterance of the conversation shares the same topic) for DAH-CRF model training rather than the topic labels automatically acquired from LDA; DAH-CRF+LDAconv: Use conversation-level topic labels automatically acquired from LDA for DAH-CRF model training. Note that only JAS (a non-deep-learning model) has attempted to model both DAs and topics, whereas all the deep learning baselines do not model topic information as a source of context for DA classi\ufb01cation. All the baselines mentioned above use the same test dataset as our models for all experimental datasets. 5 Experimental Results 5.1 Dialogue Acts Classi\ufb01cation Table 2 shows the DA classi\ufb01cation accuracy of our models and the baselines on three experi- mental datasets. We \ufb01ne-tuned the model pa- rameters for JAS, DRLM-Cond and Bi-LSTM- CRF in order to make the comparison as fair as possible. The implementation of CRF-ASN and SelfAtt-CRF are not available so we can only re- port their results for SWDA and MRDA based on the original papers (Chen et al., 2018; Raheja and Tetreault, 2019). It can be observed that by jointly modelling DA and topics, DAH-CRF+LDAutt outperforms the two best baseline models SelfAtt-CRF and CRF-ASN around 1% on the MRDA dataset. Our model also gives similar performance to SelfAtt- CRF, the baseline which achieved the state-of- the-art performance on the SWDA dataset (i.e., 82.3% vs. 82.9%). While both manually an- notated and automatically acquired topic labels are effective, we see that DAH-CRF+LDAutt outperforms both DAH-CRF+MANUALconv and DAH-CRF+LDAconv, i.e., with over 1.6% gain on DyDA and over 1.4% on SWDA (signi\ufb01- cant; paired t-test p < .01). It is also ob-",
                "metadata": {
                    "page_number": 6,
                    "filename": "K19-1036.pdf",
                    "type": "Text",
                    "start_index": 1801
                }
            },
            {
                "content": "Table 1: |C| is the number of DA classes, |T| is the number of manually labelled conversation-level topic classes, |V | is the vocabulary size. Training, Vali- dation and Testing indicate the number of conversa- tions/utterances in the respective splits.\nDataset |C| |T| |V | Training Validation Testing SWDA 42 66 20K 1003/193K 112/23K 19/5K DyDA 4 10 22K 11K/92.7K 1K/8.5K 1K/8.2K MRDA 5 - 15K 51/77.9K 11/15.8K 11/15.5K",
                "metadata": {
                    "filename": "K19-1036.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: DA classi\ufb01cation accuracy. \u2020 indicates the re- sults which are reported from the prior publications.\nModel SWDA MRDA DyDA s JAS e DRLM-Cond n i l e Bi-LSTM-CRF s a B CRF-ASN 71.2 77.0\u2020 79.2\u2020 80.8\u2020 81.3 88.4 90.9\u2020 91.4\u2020 75.9 81.1 83.6 - SelfAtt-CRF 82.9\u2020 91.1\u2020 - DAH-CRF + MANUALconv 80.9 - 86.5 s r u O DAH-CRF + LDAconv DAH-CRF + LDAutt 80.7 82.3 91.2 92.2 86.4 88.1 Human Agreement 84.0 - -",
                "metadata": {
                    "filename": "K19-1036.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Ablation studies of DA classi\ufb01cation.\nModel SWDA MRDA DyDA SAH 76.2 88.5 82.5 SAH-CRF 78.4 89.6 84.1 DAH + LDAutt 79.5 91.1 86.0 DAH-CRF + LDAutt 81.0 91.3 86.3 (without Dual-Att) DAH-CRF + LDAutt 82.3 92.2 88.1",
                "metadata": {
                    "filename": "K19-1036.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            }
        ]
    },
    "1812.09471.pdf": {
        "normalized_output": [
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "SNIPS",
                "Metric": "F1",
                "Result": "0.918"
            },
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "SNIPS",
                "Metric": "Accuracy",
                "Result": "0.973"
            },
            {
                "Task": "Linguistic Acceptability",
                "Dataset": "SNIPS</s>",
                "Metric": "Accuracy",
                "Result": "0.809"
            },
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "ATIS",
                "Metric": "F1",
                "Result": "0.952"
            },
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "ATIS",
                "Metric": "Accuracy",
                "Result": "0.950"
            },
            {
                "Task": "Linguistic Acceptability",
                "Dataset": "ATIS",
                "Metric": "Accuracy",
                "Result": "0.834"
            }
        ],
        "source_documents": [
            {
                "content": "Algorithm 1 Dynamic routing-by-agreement 1: procedure DYNAMIC ROUTING(pk|t, iter) is calculated by aggregating all the prediction vec- tors for that slot type {pk|t|t\u2208T}, weighted by the agreement values ckt obtained from bkt (Line 5-6): where a squashing function squash(\u00b7) is applied on the weighted sum sk to get vk for each slot type. Once we updated the slot representation vk in the current iteration, the logit bkt becomes larger when the dot product pk|t \u00b7 vk is large. That is, when a prediction vector pk|t is more similar to a slot representation vk, the dot product is larger, in- dicating that it is more likely to route this word to the k-th slot type (Line 7). An updated, larger bkt will lead to a larger agreement value ckt between the t-th word and the k-th slot in the next itera- tion. On the other hand, it assigns low ckt when there is inconsistency between pk|t and vk. The agreement values learned via the unsupervised, it- erative algorithm ensures the outputs of the Word- Caps get sent to appropriate subsequent SlotCaps after iterslot iterations. Cross Entropy Loss for Slot Filling For the t-th word in an utterance, its slot type is determined as follows: The slot \ufb01lling loss is de\ufb01ned over the utterance as the following cross-entropy function: where yk indicates the ground truth slot type for t the t-th word. yk t = 1 when the t-th word belongs to the k-th slot type. 2.3 IntentCaps The IntentCaps take the output vk for each slot k \u2208 {1,2,...,K} in SlotCaps as the input, and determine the utterance-level intent of the whole utterance. The IntentCaps also convert each slot representation in SlotCaps with respect to the in- tent type: where l \u2208 {1,2,...,L} and L is the number of intents. Wl \u2208 RDL\u00d7DP and bl \u2208 RDL\u00d71 are the weight and bias matrix for the l-th capsule in IntentCaps. IntentCaps adopt the same dynamic routing-by- agreement algorithm, where: ul = DYNAMIC ROUTING(ql|k,iterintent). (8) Max-margin Loss for Intent Detection Based on the capsule",
                "metadata": {
                    "filename": "1812.09471.pdf",
                    "type": "Text",
                    "start_index": 0,
                    "page_number": 4
                }
            },
            {
                "content": "lexical features. 2) Joint Seq. (Hakkani-T\u00a8ur et al., 2016) adopts a Recurrent Neural Network (RNN) for slot \ufb01lling and the last hidden state of the RNN is used to predict the ut- terance intent. 3) Attention BiRNN (Liu and Lane, 2016) further introduces a RNN based encoder- decoder model for joint slot \ufb01lling and intent de- tection. An attention weighted sum of all encoded hidden states is used to predict the utterance intent. 4) Slot-gated Full Atten. (Goo et al., 2018) utilizes a slot-gated mechanism as a special gate function in Long Short-term Memory Network (LSTM) to improve slot \ufb01lling by the learned intent context vector. The intent context vector is used for intent detection. 5) DR-AGG (Gong et al., 2018) aggre- gates word-level information for text classi\ufb01cation via dynamic routing. The high-level capsules af- ter routing are concatenated, followed by a multi- layer perceptron layer that predicts the utterance label. We used this capsule-based text classi\ufb01ca- tion model for intent detection only. 6) IntentCap- sNet (Xia et al., 2018) adopts a multi-head self- attention to extract intermediate semantic features from the utterances, and uses dynamic routing to aggregate semantic features into intent represen- tations for intent detection. We use this capsule- based model for intent detection only. We also compare our proposed model CAPSULE-NLU with existing commercial natural language understanding services, includ- ing api.ai (Now called DialogFlow)2, Waston Assistant3, Luis4, wit.ai5, snips.ai6, recast.ai7, and Amazon Lex8. Implementation Details The hyperparameters used for experiments are shown in Table 2. Table 2: Hyperparameter settings. 2https://dialogflow.com/ 3https://www.ibm.com/cloud/watson- assistant/ 4https://www.luis.ai/ 5https://wit.ai/ 6https://snips.ai/ 7https://recast.ai/ 8https://aws.amazon.com/lex/",
                "metadata": {
                    "filename": "1812.09471.pdf",
                    "start_index": 1798,
                    "page_number": 5,
                    "type": "Text"
                }
            },
            {
                "content": "2 Approach We propose to model the hierarchical relation- ship among each word, the slot it belongs to, and the intent label of the whole utterance by a hier- archical capsule neural network structure called CAPSULE-NLU. The proposed architecture con- sists of three types of capsules: 1) WordCaps that learn context-aware word representations, 2) SlotCaps that categorize words by their slot types via dynamic routing, and construct a representa- tion for each type of slot by aggregating words that belong to the slot, 3) IntentCaps determine the intent label of the utterance based on the slot representation as well as the utterance contexts. Once the intent label has been determined by In- tentCaps, the inferred utterance-level intent helps re-recognizing slots from the utterance by a re- routing schema. 2.1 WordCaps Given an input utterance x = (w1,w2,...,wT) of T words, where each word is initially represented by a vector of dimension DW. Here we simply trained word represenations from scratch. Vari- ous neural network structures can be used to learn context-aware word representations. For example, a recurrent neural network such as a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) can be applied to learn representations of each word in the utterance: For each word w;, we concatenate each for- ward hidden state hy obtained from the forward LSTM fw with a backward hidden state hy from LSTM,, to obtain a hidden state h;. The whole hidden state matrix can be defined as H = (hy, hg,...,h) \u20ac R?*?24, where Dy is the number of hidden units in each LSTM. In this work, the parameters of WordCaps are trained with the whole model, while sophisticated pre- trained models such as ELMo (Peters et al., 2018) or BERT (Devlin et al., 2018) may also be inte- grated. 2.2 SlotCaps Traditionally, the learned hidden state ht for each word wt is used as the logit to predict its slot tag. When H for all words in the utterance is learned, sequential tagging methods like the",
                "metadata": {
                    "page_number": 3,
                    "start_index": 0,
                    "filename": "1812.09471.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "5 Related Works Intent Detection With recent developments in deep neural networks, user intent detection mod- els (Hu et al., 2009; Xu and Sarikaya, 2013; Zhang et al., 2016; Liu and Lane, 2016; Zhang et al., 2017; Chen et al., 2016; Xia et al., 2018) are pro- posed to classify user intents given their diversely expressed utterances in the natural language. As a text classi\ufb01cation task, the decent performance on utterance-level intent detection usually relies on hidden representations that are learned in the intermediate layers via multiple non-linear trans- formations. Recently, various capsule based text classi\ufb01- cation models are proposed that aggregate word- level features for utterance-level classi\ufb01cation via dynamic routing-by-agreement (Gong et al., 2018; Zhao et al., 2018; Xia et al., 2018). Among them, Xia et al. (2018) adopts self-attention to extract in- termediate semantic features and uses a capsule- based neural network for intent detection. How- ever, existing works do not study word-level su- pervisions for the slot \ufb01lling task. In this work, we explicitly model the hierarchical relationship be- tween words and slots on the word-level, as well as intents on the utterance-level via dynamic routing- by-agreement. Slot Filling Slot \ufb01lling annotates the utterance with \ufb01ner granularity: it associates certain parts of the utterance, usually named entities, with pre- de\ufb01ned slot tags. Currently, the slot \ufb01lling is usu- ally treated as a sequential labeling task. A re- current neural network such as Gated Recurrent Unit (GRU) or Long Short-term Memory Network (LSTM) is used to learn context-aware word repre- sentations, and Conditional Random Fields (CRF) are used to annotate each word based on its slot type. Recently, Shen et al. (2017); Tan et al. (2017) introduce the self-attention mechanism for CRF- free sequential labeling. Joint Modeling via Sequence Labeling To over- come the error propagation in the word-level slot \ufb01lling task and the utterance-level",
                "metadata": {
                    "page_number": 8,
                    "start_index": 0,
                    "type": "Text",
                    "filename": "1812.09471.pdf"
                }
            },
            {
                "content": "Table 1: Dataset statistics.\nDataset SNIPS-NLU ATIS Vocab Size 11,241 722 Average Sentence Length 9.05 11.28 #Intents 7 21 #Slots 72 120 #Training Samples 13,084 4,478 #Validation Samples 700 500 #Test Samples 700 893",
                "metadata": {
                    "filename": "1812.09471.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Hyperparameter settings.\nDataset DW DH DP DL iterslot iterintent SNIPS-NLU 1024 512 512 128 2 2 ATIS 1024 512 512 256 3 3",
                "metadata": {
                    "filename": "1812.09471.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Slot \ufb01lling and intention detection results using CAPSULE-NLU on two datasets.\nModel Slot (F1) SNIPS-NLU Intent (Acc) Overall (Acc) Slot (F1) ATIS Intent (Acc) Overall (Acc) CNN TriCRF (Xu and Sarikaya, 2013) - - - 0.944 - - Joint Seq. (Hakkani-T\u00a8ur et al., 2016) 0.873 0.969 0.732 0.942 0.926 0.807 Attention BiRNN (Liu and Lane, 2016) 0.878 0.967 0.741 0.942 0.911 0.789 Slot-Gated Full Atten. (Goo et al., 2018) 0.888 0.970 0.755 0.948 0.936 0.822 DR-AGG (Gong et al., 2018) - 0.966 - - 0.914 - IntentCapsNet (Xia et al., 2018) - 0.974 - - 0.948 - CAPSULE-NLU 0.918 0.973 0.809 0.952 0.950 0.834 CAPSULE-NLU w/o Intent Detection 0.902 - - 0.948 - - CAPSULE-NLU w/o Joint Training 0.902 0.977 0.804 0.948 0.847 0.743",
                "metadata": {
                    "filename": "1812.09471.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            }
        ]
    },
    "2203.16804.pdf": {
        "normalized_output": [
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-1</s>",
                "Result": "47.78"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-2</s>",
                "Result": "23.55"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-L</s>",
                "Result": "44.57"
            },
            {
                "Task": "Summarization",
                "Dataset": "XSum",
                "Metric": "ROGUE-1</s>",
                "Result": "49.07"
            },
            {
                "Task": "Summarization</s>",
                "Dataset": "XSum",
                "Metric": "ROGUE-2</s>",
                "Result": "25.59"
            },
            {
                "Task": "Summarization",
                "Dataset": "XSum",
                "Metric": "ROGUE-L</s>",
                "Result": "40.40"
            },
            {
                "Task": "Summarization",
                "Dataset": "New York Times (NYT)",
                "Metric": "ROGUE-1</s>",
                "Result": "57.75"
            },
            {
                "Task": "Summarization",
                "Dataset": "New York Times (NYT)",
                "Metric": "ROGUE-2",
                "Result": "38.64"
            },
            {
                "Task": "Summarization",
                "Dataset": "New York Times (NYT)",
                "Metric": "ROGUE-L",
                "Result": "54.54"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "BERTScore",
                "Result": "32.11"
            }
        ],
        "source_documents": [
            {
                "content": "A Datasets Statistics Table 12: Datasets Statistics. B Implementation Details We use diverse beam search (Vijayakumar et al., 2018) to generate 16 candidates for each data sam- ple. On CNNDM and XSum, we use the pre-trained BART12 and PEGASUS13 models from the Trans- formers (Wolf et al., 2020) library as the base ab- stractive models for candidate summary generation and model \ufb01netuning respectively. On NYT, we \ufb01rst \ufb01ne-tuned a BART model14 with MLE train- ing as the base abstractive model, since our data pre-processing is sightly different from the previ- ous work and there are no available pre-trained checkpoints. We use 4 NVIDIA RTX 3090 GPUs for the model training, and the average running time for one epoch is around 20 hours. We use the Adam optimizer (Kingma and Ba, 2015) with learning rate scheduling for the model training: where warmup denotes the warmup steps, which is set to 10000, step is the number of updating steps, lr is the learning rate. We set the length penalty factor \u03b1 in the scoring function (Eq. 9) to the same value as used in the original beam search. We search the value of the margin \u03bb in the contrastive loss (Eq. 8) within the range [1 \u00d7 10\u22125,1], and decide the value based on the model performance on the validation set. We also performed extensive search for the coef\ufb01cient \u03b3 in Eq. 10. The speci\ufb01c hyper-parameter setting is reported in Tab. 13. We use the standard ROUGE (Lin, 2004) Perl package15 for evaluation. The command line pa- rameters are \u2018-c 95 -r 1000 -n 2 -m\u2019. Before the 12The checkpoint is \u201cfacebook/bart-large-cnn\u201d, containing around 400M parameters. 13The checkpoint is \u201cgoogle/pegasus-xsum\"\" containing around 568M parameters. 14The checkpoint is \u201cfacebook/bart-large\u201d. 15https://github.com/summanlp/evaluation/tree/master/ ROUGE-RELEASE-1.5.5 ROUGE evaluation, the reference summaries and system outputs are lower-cased and tokenized.16 C Details of Few-shot Fine-tuning On CNNDM, we randomly select 100 examples from the training set",
                "metadata": {
                    "start_index": 0,
                    "page_number": 14,
                    "type": "Text",
                    "filename": "2203.16804.pdf"
                }
            },
            {
                "content": "have such scores as \u201ccoordinated\u201d for conciseness. 1We have made our code, results, and trained models pub- licly available at https://github.com/yixinL7/BRIO. We introduce a training paradigm which requires the abstractive model to be able to be accurate with respect to predicting the tokens in the refer- ence summaries and coordinated with respect to",
                "metadata": {
                    "start_index": 3600,
                    "type": "Text",
                    "page_number": 1,
                    "filename": "2203.16804.pdf"
                }
            },
            {
                "content": "we use a model-based semantic similarity metric, BERTScore (Zhang* et al., 2020),7 as the evalua- tion metric M in Eq.7 to compare the performance of different candidate summaries. Then, we trained another version of BRIO-Mul based on the order of candidate summaries calculated by BERTScore. The results in Tab. 6 show that (1) Our model can signi\ufb01cantly improve the model performance when either ROUGE or BERTScore is used as the target evaluation metric for ordering candidate sum- maries. This suggests that it is possible to use our method to optimize any speci\ufb01c target met- ric, making our method an alternative to reinforce- ment learning or minimum risk training. (2) Our model that is trained on one evaluation metric (e.g. BERTScore) also achieves improvement on another metric (e.g. ROUGE) compared with the baseline model, which indicates that the improvement made by our model is not from exploiting the potential weaknesses of individual metrics. Besides, this re- sult also demonstrates a non-trivial degree of agree- ment between ROUGE and BERTScore. Novel n-grams We compare the ratio of novel n-grams in reference, BRIO-Mul\u2019s, and BART\u2019s summaries. As Tab. 7 shows, our model is more \u201cabstractive\u201d compared to BART, although refer- ence summaries still contain more novel n-grams. This is likely due to the fact that our model is op- timized at the sequence-level, allowing more free- dom for paraphrasing and compression. We further investigate the relation of the \u201cab- stractiveness\" and model performance by com- 7https://github.com/Tiiiger/bert_score. We use its default version for English texts. paring our model (BRIO-Mul) with the baseline model (BART) on different buckets of test exam- ples grouped by the \u201cnovelty\" of the reference sum- maries,8 i.e., where D and S\u2217 are the source document and ref- erence summary respectively, GD and GS\u2217 are the sets of bigrams in D and S\u2217, 1 is the indicator func- tion. The results in Fig. 3 show that when novelty is higher, (1)",
                "metadata": {
                    "type": "Text",
                    "start_index": 0,
                    "page_number": 7,
                    "filename": "2203.16804.pdf"
                }
            },
            {
                "content": "and comprehend. In Proceedings of the 28th Inter- national Conference on Neural Information Process- ing Systems - Volume 1, NIPS\u201915, page 1693\u20131701, Cambridge, MA, USA. MIT Press.",
                "metadata": {
                    "page_number": 11,
                    "filename": "2203.16804.pdf",
                    "type": "Text",
                    "start_index": 0
                }
            },
            {
                "content": "Table 1: Accuracy of different abstractive summarization systems w.r.t ranking the quality of candidate summaries on CNNDM dataset. Acc. stands for the frequency of the model assigning higher probabilities to better candidate summaries. The candidate summaries are generated by a pre-trained model (BART), and we select the best and the worst candidates (w.r.t. ROUGE scores) for each of the samples. High and Low repre- sent the average performance of the best and worst candidates respectively. R-1/2/L are the ROUGE-1/2/L scores. The origi- nal BART only achieves 54.80% accuracy.\nSystem R-1 R-2 R-L Acc.(%) High Low 53.99 33.48 29.85 10.85 51.12 30.45 100.00 0.00 BART Ours 44.88 50.10 21.68 26.29 41.92 47.19 54.80 79.63",
                "metadata": {
                    "filename": "2203.16804.pdf",
                    "page_number": 1,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Results on CNNDM, XSum and NYT. On NYT we only reported our own results due to different data pre-processing. \u2020: signi\ufb01cantly better than the baseline model (p < 0.01). *: results reported in the original papers. \u2021: results from our own evaluation script. R-1/2/L are the ROUGE-1/2/L F1 scores.\nSystem R-1 R-2 R-L CNNDM BART* 44.16 21.28 40.90 PEGASUS* 44.17 21.47 41.11 GSum* 45.94 22.32 42.48 ConSum* 44.53 21.54 41.57 SeqCo* 45.02 21.80 41.75 GOLD-p* 45.40 22.01 42.25 GOLD-s* 44.82 22.09 41.81 SimCLS* 46.67 22.15 43.54 BART\u2021 44.29 21.17 41.09 BRIO-Ctr 47.28\u2020 22.93\u2020 44.15\u2020 BRIO-Mul 47.78\u2020 23.55\u2020 44.57\u2020 XSum BART* 45.14 22.27 37.25 PEGASUS* 47.21 24.56 39.25 GSum* 45.40 21.89 36.67 ConSum* 47.34 24.67 39.40 SeqCo* 45.65 22.41 37.04 GOLD-p* 45.75 22.26 37.30 GOLD-s* 45.85 22.58 37.65 SimCLS* 47.61 24.57 39.44 PEGASUS\u2021 47.46 24.69 39.53 BRIO-Ctr 48.13\u2020 25.13\u2020 39.84\u2020 BRIO-Mul 49.07\u2020 25.59\u2020 40.40\u2020 NYT BART\u2021 55.78 36.61 52.60 BRIO-Ctr 55.98 36.54 52.51 BRIO-Mul 57.75\u2020 38.64\u2020 54.54\u2020",
                "metadata": {
                    "filename": "2203.16804.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Model performance with different \u03b3 coef\ufb01cients weighting the contrastive loss (Eq. 10) on CNNDM. BRIO- Ctr is trained with the contrastive loss only, which no longer preserves its generation ability. We report its performance when it is used as an evaluation model to select from candidate summaries. R-1/2/L are the ROUGE-1/2/L F1 scores.\nCoef\ufb01cient (\u03b3) R-1 R-2 R-L 0 (BART) 44.29 21.17 41.09 0.1 45.08 21.63 41.71 1 46.01 22.22 42.68 2 46.36 22.79 43.07 5 46.91 23.03 43.63 10 47.22 23.31 43.94 100 47.78 23.55 44.57 1000 46.83 22.17 43.68 +\u221e (BRIO-Ctr) 47.28 22.93 44.15",
                "metadata": {
                    "filename": "2203.16804.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Figure 2: Loop of candidate generation and model \ufb01netuning.\nSystem R-1 R-2 R-L BART 44.29 21.17 41.09 BRIO-Mul 47.78 23.55 44.57 BRIO-Loop 48.01\u2020 23.80\u2020 44.67\u2020",
                "metadata": {
                    "filename": "2203.16804.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5: Results on CNNDM with different beam widths (the number of beams) used in beam search. The default beam width is 4. R-1/2 are the ROUGE-1/2 F1 scores.\nBeams BART BRIO-Mul R-1 R-2 R-1 R-2 4 44.29 21.17 47.78 23.55 10 43.83 20.76 47.98 23.81 20 43.53 20.49 48.07 23.92 50 43.06 20.05 48.18 24.01 100 42.79 19.76 48.23 24.09",
                "metadata": {
                    "filename": "2203.16804.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 6: Results on CNNDM using different evaluation metrics as M in Eq.7. BRIO-Mul (R) is trained with candidate sum- maries ordered by ROUGE scores, while BRIO-Mul (B) is trained with candidate summaries ordered by BERTScore. R- 1/2/L are ROUGE-1/2/L F1 scores. BS denotes BERTScore.\nSystem R-1 R-2 R-L BS BART 44.29 21.17 41.09 27.38 BRIO-Mul (R) 47.78 23.55 44.57 32.11 BRIO-Mul (B) 47.53 23.22 44.37 32.59",
                "metadata": {
                    "filename": "2203.16804.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 7: Ratio of novel n-grams of different models on CNNDM. Novel n-grams are those that appear in the summaries but not in the source documents.\nSystem Unigram Bigram Reference .1110 .4865 BART .0101 .0924 BRIO-Mul .0262 .2381",
                "metadata": {
                    "filename": "2203.16804.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Figure 3: Performance comparison (BART v.s. BRIO-Mul) w.r.t. reference summary novelty. The x-axis represents differ- ent buckets of test examples grouped by reference summary novelty (Eq. 11). Larger x-coordinates correspond to exam- ples of which the reference summaries have higher novelty. The left \ufb01gure shows the performance improvement of our model compared with the baseline model, while the right one shows model performance.\nOwn PEGASUS BART .0470 .1205 BRIO-Mul .1839\u2020 .2768\u2020",
                "metadata": {
                    "filename": "2203.16804.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 9: Expected Calibration Error (ECE), accuracy (Acc) and con\ufb01dence (Conf) on the test set of CNNDM and XSum.\nDataset System ECE Acc Conf CNNDM BART BRIO-Mul .4097 .2719 .3711 .4271 .7365 .6652 XSum PEGASUS BRIO-Mul .2369 .1423 .4688 .4744 .6990 .5881",
                "metadata": {
                    "filename": "2203.16804.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 10: Case Study on CNNDM. BRIO-Mul learns to ignore the noise pattern (\u201cclick here\") while BART cannot.\nSystem Summary Reference chelsea forward tammy abraham nets \ufb01rst-half double for chelsea. dominic solanke adds a third late on as chelsea look set to win trophy. manchester city struggle without injured star thierry ambrose. read: mourinho warns his young chelsea players he can not play them all. click here to read our match report from man city \u2019s academy stadium. BART tammy abraham scored twice in the \ufb01rst half to give chelsea the lead. isaac buckley-ricketts levelled the game for manchester city. dominic solanke scored late on to put a gloss on the scoreline. click here to read sportsmail\u2019s player ratings from the youth cup \ufb01nal. BRIO-Mul chelsea beat manchester city 3-1 in the youth cup \ufb01nal at the etihad stadium. tammy abraham scored twice in the \ufb01rst half to give chelsea the lead. dominic solanke scored late on to seal the win for the home side. Reference alejandro valverde won ahead of julian alaphilippe and michael albasini. chris froome \ufb01nished 123rd after a crash during the \ufb01nal 12 kilometres. team sky\u2019s sports director gabriel rasch praised froome for \ufb01nishing. rasch said froome was \u2018banged up\u2019 but expects to ride tour de romandie. BART movistar rider alejandro valverde won \ufb02eche wallonne on wednesday. team sky\u2019s chris froome fell in the \ufb01nal 12km but \ufb01nished the race. philippe gilbert pulled out of the race after a bad crash 50km from the end. click here for more cycling news. BRIO-Mul alejandro valverde defended his \ufb02eche wallonne title in belgium on wednesday. movistar rider \ufb01nished ahead of julian alaphilippe and michael albasini. team sky\u2019s chris froome fell in the \ufb01nal 12km of the race but \ufb01nished in 123rd. froome was involved in a crash but \ufb01nished the race despite being \u2018banged up\u2019 Reference manuel pellegrini won the premier league and capital one cup last season. city currently sit fourth in the league table - 12 points behind chelsea. pellegrini\u2019s contract expires at the end of the 2015-16 season. city players have been impressed with vieira\u2019s work with the youth team. pep guardiola is city\u2019s \ufb01rst-choice to succeed pellegrini at the etihad. BART manuel pellegrini\u2019s future at manchester city is under scrutiny. patrick vieira is highly-respected among the city players. city\u2019s \ufb01rst-choice managerial option is bayern munich boss pep guardiola. click here for all the latest manchester city news. click here for more premier league news. BRIO-Mul manchester city players have backed patrick vieira to replace manuel pellegrini as manager of the club. the frenchman is highly-respected among the players at the etihad stadium. pellegrini\u2019s future at the club is under scrutiny after a disappointing season. city\u2019s \ufb01rst-choice manager is current bayern munich boss pep guardiola.",
                "metadata": {
                    "filename": "2203.16804.pdf",
                    "page_number": 9,
                    "type": "Table"
                }
            },
            {
                "content": "Table 11: Few-shot Fine-tuning. BRIO-Few is trained on only 100/1000 training examples on CNNDM and XSum respec- tively. R-1/2/L are ROUGE-1/2/L F1 scores.\nDataset System R-1 R-2 R-L CNNDM BART 44.29 BRIO-Few 45.81 21.17 21.91 41.09 42.61 XSum PEGASUS 47.46 BRIO-Few 47.95 24.69 24.89 39.53 39.71",
                "metadata": {
                    "filename": "2203.16804.pdf",
                    "page_number": 9,
                    "type": "Table"
                }
            },
            {
                "content": "Table 12: Datasets Statistics.\nDatasets # Examples Avg. Words Train Valid Test Doc. Sum. CNNDM 287K 13K 11K 791.6 55.6 XSum 203K 11K 11K 429.2 23.3 NYT 44K 5.5K 6.4K 1320.2 123.4",
                "metadata": {
                    "filename": "2203.16804.pdf",
                    "page_number": 14,
                    "type": "Table"
                }
            },
            {
                "content": "Table 13: Hyper-parameter Setting.\nDatasets \u03bb (Eq. 8) \u03b1 (Eq. 9) \u03b3 (Eq. 10) CNNDM 0.001 2.0 100 XSum 0.1 0.6 100 NYT 0.001 2.0 100",
                "metadata": {
                    "filename": "2203.16804.pdf",
                    "page_number": 14,
                    "type": "Table"
                }
            }
        ]
    },
    "2020.findings-emnlp.378.pdf": {
        "normalized_output": [
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "Ontonotes v5 - English",
                "Metric": "F1",
                "Result": "90.32"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "WNUT-16 - English",
                "Metric": "F1",
                "Result": "55.14"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "WNUT-17 - English",
                "Metric": "F1",
                "Result": "50.68"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "Ontonotes v4 - Chinese",
                "Metric": "F1",
                "Result": "81.18"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "ReCoRD",
                "Metric": "F1",
                "Result": "96.62"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "WNUT-17 - English",
                "Metric": "F1",
                "Result": "69.78"
            }
        ],
        "source_documents": [
            {
                "content": "coder in this work. So that the encoding of the input text can be formalized as where H = [h1,h2,\u00b7\u00b7\u00b7 ,hi,\u00b7\u00b7\u00b7 ,hn] and E = [e1,e2,\u00b7\u00b7\u00b7 ,ei,\u00b7\u00b7\u00b7 ,en] are lists of hidden vectors and embeddings of X, respectively. Note that, since pre-trained embeddings contain context infor- mation learned from large-scale corpora, and differ- ent types of them may carry heterogeneous context information learned from different algorithms and corpora, we incorporate multiple pre-trained em- beddings by direct concatenating them in the input: 2015).5 These datasets come from a wide range of sources so that we are able to comprehensively eval- uate our approach with them. In detail, WN16 and WN17 are constructed from social media; ON5e consists of mixed sources, such as telephone con- versation, newswire, etc.; ON4c is from news do- main; RE and WE are extracted from Chinese on- line resources. For all datasets, we use their origi- nal splits and the statistics of them with respect to the number of entity types (# T.), sentences (# S.) and total entities (# E.) in the train/dev/test sets are reported in Table 1. 3.2 Implementation where ei is the \ufb01nal word representation to feed the context encoder; ez i represents the word embedding of xi in embedding type z and Z the set of all embedding types. For the output, upon the receiving of oi, a train- able matrix Wo is used to align its dimension to the output space by ui = Wo \u00b7 oi. Finally, we apply a conditional random \ufb01eld (CRF) decoder to predict the labels \u02c6yi \u2208 T (where T is the set with all NE labels) in the output sequence \u02c6Y by To label NEs, we use the BIOES tagging scheme instead of the standard BIO scheme for the reason that previous studies have shown optimistic im- provement with this scheme (Lample et al., 2016; Yan et al., 2019). For the text input, we use three types of embeddings for each language by de- fault. Speci\ufb01cally, for English, we use Glove (100- dimension)6 (Pennington et al., 2014), ELMo (Pe- ters et al., 2018), and",
                "metadata": {
                    "type": "Text",
                    "page_number": 5,
                    "start_index": 0,
                    "filename": "2020.findings-emnlp.378.pdf"
                }
            },
            {
                "content": "2.1 Syntactic Information Extraction A good representation of the input text is the key to obtain good model performance for many NLP tasks (Song et al., 2017; Sileo et al., 2019). Nor- mally, a straightforward way to improve model performance is to enhance text representation by embeddings of extra features, which is demon- strated to be useful across tasks (Marcheggiani and Titov, 2017; Song et al., 2018a; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020c), in- cluding NER (Zhang and Yang, 2018; Seyler et al., 2018; Sui et al., 2019; Gui et al., 2019b,a; Liu et al., 2019b). Among different types of extra features, the syntactic one has been proved to be helpful in previous studies for NER, where the effectiveness of POS labels, syntactic constituents, and depen- dency relations, are demonstrated by McCallum (2003), Li et al. (2017), and Cetoli et al. (2018), respectively. In this paper, we also focus on these three types of syntactic information. In doing so, we obtain the POS labels, the syntax tree and the dependency parsing results from an off-the-shelf NLP toolkit (e.g., Stanford Parser) for each input sequence X. Then, for each token xi in X, we extract its context features and related syntactic information according to the following procedures. the tree to \ufb01nd the \ufb01rst acceptable syntactic node2, and select all tokens under that node as the con- text features and the combination of tokens and their syntactic nodes as the constituent information. For example, in Figure 2(b), we start from \u201cSalt\u201d and extract its \ufb01rst accepted node \u201cNP\u201d, then col- lect the tokens under \u201cNP\u201d as the context features (i.e., \u201cSalt\u201d, \u201cLake\u201d, and \u201cCity\u201d) and combine them with \u201cNP\u201d to get the constituent information (i.e., \u201cSalt NP\u201d, \u201cLake NP\u201d, and \u201cCity NP\u201d). For dependency relations, we \ufb01nd all context features for each xi by collecting all its dependents and governor from X\u2019s dependency parse, and then regard the combination of the context features and their in-bound",
                "metadata": {
                    "start_index": 0,
                    "page_number": 3,
                    "filename": "2020.findings-emnlp.378.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "Table 9: Values tested for different hyper-parameters and the best one used in our experiments. are obtained by tuning our model with the given hyper-parameter values on the development set of each dataset. Appendix D: The Results of Our Models on the Development Set Table 10: F1 scores of our models under different con\ufb01gurations on the development set of all datasets. \u201cGM\u201d is the gate mechanism; \u201cP.\u201d, \u201cC.\u201d and \u201cD.\u201d refer to POS labels, syntactic constituents and dependency relations, respectively. In Table 10, we report the experimental results (F1) of our models (i.e., with all types of embed- dings and KVMN) under different con\ufb01gurations (using syntax attention on different combinations of syntactic information and whether to use the gate mechanism) on development set of all datasets.",
                "metadata": {
                    "start_index": 0,
                    "page_number": 15,
                    "type": "Text",
                    "filename": "2020.findings-emnlp.378.pdf"
                }
            },
            {
                "content": "(e.g., inaccurate POS tagging results) may hurt model performance. As a result, it is still a challenge to \ufb01nd an appropriate way to incorpo- rate external information into neural models for NER. Moreover, in most cases, one would like to incorporate more than one types of extra features. Consequently, it is essential to design an effective mechanism to combine and weight those features \u2020Corresponding author. 1The code and the best performing models are available at https://github.com/cuhksz-nlp/AESINER so as to restrict the in\ufb02uence of noisy information. 4231 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4231-4245 November 16 - 20, 2020. \u00a92020 Association for Computational Linguistics",
                "metadata": {
                    "type": "Text",
                    "filename": "2020.findings-emnlp.378.pdf",
                    "start_index": 3602,
                    "page_number": 1
                }
            },
            {
                "content": "Table 1: Statistics of all datasets with respect to the number of NE types (T.), sentences (S.), and total NEs (E.).\nENGLISH CHINESE TYPE ON5E WN16 WN17 ON4C RE WE # T. = 18 # T. = 10 # T. = 6 # T. = 4 # T. = 8 # T. = 4 # S. # E. # S. # E. # S. # E. # S. # E. # S. # E. # S. # E. TRAIN 59.9K 81.8K 2.4K 1.5K 3.4K 2.0K 15.7K 13.4K 3.8K 13.4K 1.4K 1.9K DEV 8.5K 11.1K 1.0K 0.7K 1.0K 0.8K 4.3K 7.0K 0.5K 1.5K 0.3K 0.4K TEST 8.3K 11.3K 3.9K 3.5K 1.3K 1.1K 4.3K 7.7K 0.5K 1.6K 0.3K 0.4K",
                "metadata": {
                    "filename": "2020.findings-emnlp.378.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: F1 scores of the baseline model and ours enhanced with different types of syntactic information (\u201cPOS.\u201d, \u201cCON.\u201d and \u201cDEP.\u201d refer to POS labels, syntactic constituents and dependency relations, respectively).\nSYNTACTIC INFORMATION ON5E WN16 WN17 ON4C RE WE POS. CON. DEP. \u221a \u221a \u221a 89.32 89.51 89.64 89.58 53.81 53.94 54.59 54.37 48.96 49.68 49.82 49.47 79.04 79.53 79.76 80.03 95.84 96.09 96.11 96.02 67.79 68.76 68.11 68.64",
                "metadata": {
                    "filename": "2020.findings-emnlp.378.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: F1 scores of our models with different combinations of syntactic information. \u201cTYPE\u201d indicates how they are combined, where \u201cDC\u201d and \u201cSA\u201d refer to direct concatenation and syntax attention, respectively.\nTYPE SYNTACTIC INFORMATION ON5E WN16 WN17 ON4C RE WE POS. CON. DEP. DC \u221a \u221a \u221a \u221a \u221a \u221a \u221a \u221a \u221a 89.61 89.56 89.60 89.62 54.11 54.03 54.26 54.41 49.61 49.74 49.58 49.63 79.61 79.83 79.89 79.81 95.72 96.11 96.08 95.31 68.27 68.51 68.36 68.49 SA \u221a \u221a \u221a \u221a \u221a \u221a \u221a \u221a \u221a 89.68 89.76 89.78 89.86 54.68 54.61 54.56 54.79 49.81 49.89 49.96 50.21 79.92 80.29 80.41 80.65 96.19 96.23 96.31 96.43 68.94 69.01 68.76 69.37",
                "metadata": {
                    "filename": "2020.findings-emnlp.378.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: F1 scores of our models with and without applying the gate mechanism (\u201cGM\u201d) when different syntactic information are combined by syntactic attention.\nGM SYNTACTIC INFORMATION ON5E WN16 WN17 ON4C RE WE POS. CON. DEP. \u221a \u221a \u221a \u221a \u221a 89.68 90.09 54.68 54.92 49.81 50.28 79.92 80.31 96.19 96.51 68.94 69.31 \u221a \u221a \u221a \u221a \u221a 89.76 90.08 54.61 54.78 49.89 50.16 80.29 80.64 96.23 96.47 69.01 69.47 \u221a \u221a \u221a \u221a \u221a 89.78 90.11 54.56 54.96 49.96 50.36 80.41 80.87 96.31 96.51 68.76 69.24 \u221a \u221a \u221a \u221a \u221a \u221a \u221a 89.86 90.32 54.79 55.14 50.21 50.68 80.65 81.18 96.43 96.62 69.37 69.78",
                "metadata": {
                    "filename": "2020.findings-emnlp.378.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5: Comparison of F1 scores of our best performing model (i.e. the full model with attentive ensemble of all syntactic information) with that reported in previous studies on all English and Chinese benchmark datasets. \u201c*\u201d indicates the studies using BERT as the text encoder; \u201c\u2020\u201d means the results are our runs of their models.\nENGLISH CHINESE MODEL ON5E WN16 WN17 MODEL ON4C RE WE CHIU AND NICHOLS (2016) 86.12 - - ZHANG AND YANG (2018) 73.88 - 58.79 \u2020LUO ET AL. (2018) 88.79 51.26 48.63 YAN ET AL. (2019) 72.43 95.00 58.17 \u2020DANG ET AL. (2018) 88.91 51.84 48.12 GUI ET AL. (2019B) 74.89 95.37 60.21 AKBIK ET AL. (2018) 89.30 - - ZHU AND WANG (2019) 73.64 94.94 59.31 JIE AND LU (2019) 89.88 - - GUI ET AL. (2019A) 74.45 95.11 59.92 YAN ET AL. (2019) 89.78 54.06 48.98 LIU ET AL. (2019C) 74.43 95.21 59.84 \u2217DEVLIN ET AL. (2019) 89.16 54.36 49.52 SUI ET AL. (2019) 74.79 - 63.09 ZHOU ET AL. (2019) - 53.43 42.83 DING ET AL. (2019) 76.00 - 59.50 AKBIK ET AL. (2019) - - 49.59 \u2217MENG ET AL. (2019) 80.62 96.54 67.60 DAI ET AL. (2019) 89.83 - - XU ET AL. (2019A) - - 68.93 LIU ET AL. (2019B) 89.94 - - MA ET AL. (2020) 75.54 95.59 61.24 \u2217LUO ET AL. 90.30 - - \u2217HU AND WEI (2020) 80.20 95.80 64.00 OURS 90.32 55.14 50.68 OURS 81.18 96.62 69.78",
                "metadata": {
                    "filename": "2020.findings-emnlp.378.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 6: Experimental results (F1 scores) of our best performing model (i.e., the full model with attentive ensemble of all syntactic information) using different pre-trained embeddings and their combinations as input.\nENGLISH CHINESE EMBEDDINGS ON5E WN16 WN17 EMBEDDINGS ON4C RE WE GLOVE ELMO BERT GIGA TENCENT ZEN \u221a \u221a \u221a \u221a \u221a \u221a \u221a \u221a \u221a \u221a \u221a \u221a 89.37 89.71 89.53 89.91 89.82 90.13 90.32 47.92 53.96 53.74 54.36 54.16 54.92 55.14 43.24 47.92 48.74 48.21 49.61 50.12 50.68 \u221a \u221a \u221a \u221a \u221a \u221a \u221a \u221a \u221a \u221a \u221a \u221a 72.11 73.54 80.06 74.86 80.49 80.81 81.18 94.99 95.21 95.98 95.46 96.24 96.41 96.62 61.94 63.06 68.84 63.96 68.94 69.42 69.78",
                "metadata": {
                    "filename": "2020.findings-emnlp.378.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Figure 3: An illustration of how our model encodes syntactic information through KVMN, weights them by syntax attention (SA) and learns from the gate mechanism (GM), where the weights for different features and information types are visualized. The example sentence is shown at the top with the gold NE tags for each word marked below. The weights assigned to different syntactic information for \u201cBill\u201d in KVMN, SA, and GM are highlighted with colors, where the darker colors referring to higher values.\nMason was one of the drafters of the Rights S-PER O oO oO oO oO oO B-LAW E-LAW re KVMN SA i GM iP. | C. 1D. nmod uP || Context 1 NP case 4 1 uN ee ee met. OB): Syntax the (Bi of | the (Bill) of Rights)| drafters of the | Bill) Rights \\\\p. (1;",
                "metadata": {
                    "filename": "2020.findings-emnlp.378.pdf",
                    "page_number": 9,
                    "type": "Table"
                }
            },
            {
                "content": "Table 7: Experimental results (F1 scores) of our mod- els (i.e. the full model with attentive ensemble of all syntactic information), where BERT or ZEN is used as one of the three types of embeddings (the others are Giga and Tencent Embedding) in the embedding layer.\nEMBEDDINGS ON4C RE WE BERT + GIGA + TENCENT 80.91 96.56 69.61 ZEN + GIGA + TENCENT 81.18 96.62 69.78",
                "metadata": {
                    "filename": "2020.findings-emnlp.378.pdf",
                    "page_number": 14,
                    "type": "Table"
                }
            },
            {
                "content": "Context Encoder Syntactic Information ON5e WN16 WN17 Bi-LSTM \u221a 88.56 89.64 51.16 53.39 48.11 49.56 Transformer \u221a 88.97 89.92 52.31 54.56 48.69 50.21 Adapted- Transformer \u221a 89.32 90.32 53.81 55.14 48.96 50.68",
                "metadata": {
                    "filename": "2020.findings-emnlp.378.pdf",
                    "page_number": 14,
                    "type": "Table"
                }
            },
            {
                "content": "(b) Performance on all Chinese datasets.\nContext Encoder Syntactic Information ON4c RE WE Bi-LSTM \u221a 77.32 80.03 94.81 96.08 65.72 68.11 Transformer \u221a 78.18 80.46 95.26 96.31 67.16 69.24 Adapted- Transformer \u221a 79.04 81.18 95.84 96.62 67.79 69.78",
                "metadata": {
                    "filename": "2020.findings-emnlp.378.pdf",
                    "page_number": 14,
                    "type": "Table"
                }
            },
            {
                "content": "Table 9: Values tested for different hyper-parameters and the best one used in our experiments.\nVALUES BEST DROPOUT RATE 0, 0.1, 0.2, 0.3 0.2 LEARNING RATE e\u22125, e\u22124, e\u22123 e\u22124 BATCH SIZE 8, 16, 32 32 NUMBER OF LAYERS 1, 2, 4 2 NUMBER OF HEAD 4, 8, 12 12 HIDDEN UNITS 64, 128, 256 128",
                "metadata": {
                    "filename": "2020.findings-emnlp.378.pdf",
                    "page_number": 15,
                    "type": "Table"
                }
            },
            {
                "content": "Table 10: F1 scores of our models under different con\ufb01gurations on the development set of all datasets. \u201cGM\u201d is the gate mechanism; \u201cP.\u201d, \u201cC.\u201d and \u201cD.\u201d refer to POS labels, syntactic constituents and dependency relations, respectively.\nGM SYN. ON5E WN16 WN17 ON4C RE WE P. C. D. \u221a \u221a \u221a \u221a \u221a 86.21 86.78 55.54 56.26 49.48 49.76 77.06 77.65 96.04 67.86 96.34 68.35 \u221a \u221a \u221a \u221a \u221a 86.41 86.84 56.31 56.84 49.69 50.02 77.31 77.56 96.12 67.63 96.31 67.86 \u221a \u221a \u221a \u221a \u221a 86.58 86.92 56.56 57.26 49.75 50.18 77.42 77.74 96.12 67.52 96.39 68.14 \u221a \u221a \u221a \u221a \u221a \u221a \u221a 86.71 56.41 87.03 57.38 50.04 50.51 77.61 96.41 68.16 78.05 96.46 68.92",
                "metadata": {
                    "filename": "2020.findings-emnlp.378.pdf",
                    "page_number": 15,
                    "type": "Table"
                }
            }
        ]
    },
    "D19-1367.pdf": {
        "normalized_output": [
            {
                "Task": "Language Modeling",
                "Dataset": "Penn Treebank (PTB)",
                "Metric": "Perplexity",
                "Result": "56.0"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2003 - English",
                "Metric": "F1",
                "Result": "93.47"
            }
        ],
        "source_documents": [
            {
                "content": "perplexity than DARTS during architecture search. This may indicate better ar- chitectures found by I-DARTS because the search model is optimized with respect to validation per- plexity. Then, we test the learned architecture in a named entity recognition system on the English data from CoNLL-2003 shared task (Sang and Meulder, 2003). Following previous work (Akbik et al., 2018; Peters et al., 2017), we report the av- eraged F1 score over 5 runs on the test set. For modeling, we choose the single-layer RNN-CRF",
                "metadata": {
                    "page_number": 3,
                    "type": "Text",
                    "start_index": 3601,
                    "filename": "D19-1367.pdf"
                }
            },
            {
                "content": "ence lies in that we select top-n edges with respect to \u03b1i,j k . Here n is a hyper-parameter that control- s the density of the network. E.g., n = 1 means a sparse net, and n = \u221e means a very dense net involving all those edges. 3 Experiments We test our method on language modeling and named entity recognition tasks. Our experiments consist of two parts: recurrent neural architecture search and architecture evaluation. In architecture search, we search for good RNN cell architectures. Then, we train and evaluate the learned architec- ture. 3.1 Architecture Search For language modeling, we run neural search on the PTB corpus. We use the standard pre- processed version of the dataset (Pham et al., 2018). To make it comparable with previous work, we copy the setup used in (Pham et al., 2018; Liu et al., 2019). The recurrent cell consist of 8 nodes. The candidate operation set of every edge contain 5 activation functions, including zeroize, tanh, re- lu, sigmoid, and identity. To learn architectures, we run the search system for 40 training epochs with a batch size of 256. We optimize models pa- rameters {Wi} using SGD with a learning rate of 20 and a weight decay rate of 5e-7, and optimized softmax relaxation parameters {wi,j k } by Adam with a learning rate of 3e-3 and a weight decay rate of 1e-3. For RNN models, we use a single- layer recurrent network with embedding and hid- den sizes = 300. It takes us 4 hours to learn the architecture on a single GPU of NVIDIA 1080Ti. For named entity recognition, we choose the CONLL-2003 English dataset. We follow the same setup as in language modeling but with a d- ifferent learning rate (0.1) and a different hidden layer size (256). It takes us 4 hours to learn the architecture on the same GPU. 3.2 Architecture Evaluation Firstly, the discovered architecture is evaluated on the language modeling task. Before that, we train it on the same data used in architecture search. The size of hidden layers is set to 850. We use av-",
                "metadata": {
                    "type": "Text",
                    "page_number": 3,
                    "start_index": 0,
                    "filename": "D19-1367.pdf"
                }
            },
            {
                "content": "interesting phenomenon comes up that the best architecture on language modeling is different from that on name entity recognition. This might result from the fact that different tasks have different inductive bias. Al- so, this suggests the possibility of architecture se- lection from the top-k search results on the target task. 4 Related Work Neural architecture search has been proposed to automatically search for better architectures, showing competitive results on several tasks, e.g., image recognition and language modeling. A s- y Table 2: F1 scores on the CoNLL-2003 English NER test set. trand of NAS research focuses on reinforcemen- t learning (Zoph and Le, 2016) and evolutionary algorithm-based (Xie and Yuille, 2017) method- s. They are powerful but inef\ufb01cient. Recent ap- proaches speed up the search process by weight sharing (Pham et al., 2018) and differentiable ar- chitecture search (Liu et al., 2019). But there is no discussion on the softmax-local problem in previ- ous work. Moreover, previous methods are often tested on language modeling. It is rare to see stud- ies on these methods for other NLP tasks. Figure 3: Cells discovered by I-DARTS for language modeling (top) and NER (bottom).",
                "metadata": {
                    "type": "Text",
                    "filename": "D19-1367.pdf",
                    "start_index": 1798,
                    "page_number": 4
                }
            },
            {
                "content": "the discovered architecture is evaluated on the language modeling task. Before that, we train it on the same data used in architecture search. The size of hidden layers is set to 850. We use av- eraged SGD to train the model for 3,000 epochs, with a learning rate of 20 and a weight decay rate of 8e-7. For a fair comparison, we do not \ufb01ne-tune the model at the end of the training. Table 1: Perplexities on PTB (lower is better). V-RHN (Zilly et al., 2016) indicates Variational RHN. LSTM + SC (Yang et al., 2018) indicates LSTM with skip con- nection. LSTM + SE (Merity et al., 2018) indicates LSTM with mixture of softmax. Random RNNs indi- cates that the network generated by random initialized. Table 1 shows the perplexities of different RN- N models on PTB. We also report the results of previous systems. The model discovered by I- DARTS achieves a validation perplexity of 58.0 and a test perplexity of 56.0 when n = 1. It is on par with the state-of-the-art models that are de- signed either manually or automatically. Howev- er, we \ufb01nd that the model failed to optimize when n = 2. It might result from the complex interac- tion between operations. We leave this issue for future study. Since architecture search is initialization- sensitive (Pham et al., 2018; Liu et al., 2019), we search the architectures for 4 times with different random seeds. We evaluate the architecture every 10 search epochs by retraining it on PTB for 500 epochs. We compare DARTS with our I-DARTS method with the same random seed. See Figure 2(b) for averaged validation perplexities over 4 d- ifferent runs at different search epochs. We see that I-DARTS is easier to converge than DARTS (4 hours). It is 1.4X faster than that of DARTS. An- other interesting \ufb01nding is that I-DARTS achieves a lower validation perplexity than DARTS during architecture search. This may indicate better ar- chitectures found by I-DARTS because the search model is optimized with respect to validation per- plexity. Then, we",
                "metadata": {
                    "filename": "D19-1367.pdf",
                    "type": "Text",
                    "start_index": 1800,
                    "page_number": 3
                }
            },
            {
                "content": "Table 1: Perplexities on PTB (lower is better). V-RHN (Zilly et al., 2016) indicates Variational RHN. LSTM + SC (Yang et al., 2018) indicates LSTM with skip con- nection. LSTM + SE (Merity et al., 2018) indicates LSTM with mixture of softmax. Random RNNs indi- cates that the network generated by random initialized.\nArchitecture Perplexity val test Search Cost (GPU days) V-RHN 67.9 65.4 - LSTM 60.7 58.8 - LSTM + SC 60.9 58.3 - LSTM + SE 58.1 56.0 - ENAS 60.8 58.6 0.50 DARTS 58.3 56.1 0.25 Random RNNs 63.7 61.2 - I-DARTS (n = 1) 58.0 56.0 0.17 I-DARTS (n = 2) - - -",
                "metadata": {
                    "filename": "D19-1367.pdf",
                    "page_number": 3,
                    "type": "Table"
                }
            },
            {
                "content": "Figure 2: Perplexity vs. search epoch number.\nModel F1 best published BiLSTM-CRF (Lample et al., 2016) 90.94 BiLSTM-CRF+ELMo (Peters et al., 2018) 92.22 BERT Base (Devlin et al., 2018) 92.40 BERT Large (Devlin et al., 2018) 92.80 BiLSTM-CRF+PCE (Akbik et al., 2019) 93.18 Random RNNs w/o pre-trained LM 90.64 DARTS w/o pre-trained LM 91.05 I-DARTS (n = 2) w/o pre-trained LM 90.96 I-DARTS (n = 1) w/o pre-trained LM 91.23 Random RNNs 92.89 DARTS 93.13 I-DARTS (n = 2) 93.14 I-DARTS (n = 1) 93.47",
                "metadata": {
                    "filename": "D19-1367.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            }
        ]
    },
    "1711.05568.pdf": {
        "normalized_output": [
            {
                "Task": "Dialogue Act Classification",
                "Dataset": "Switchboard Dialog Act Corpus (SWDA)",
                "Metric": "Accuracy",
                "Result": "81.3"
            },
            {
                "Task": "Dialogue Act Classification",
                "Dataset": "ICSI Meeting Recorder Dialog Act Corpus (MRDA)",
                "Metric": "Accuracy",
                "Result": "91.7"
            }
        ],
        "source_documents": [
            {
                "content": "ABANDONED/UNINTERPRETABLE proportion Dataset Training Validation Testing Table 4: |C| is the number of Dialogue Act classes, |V | is the vocabulary size. Training, Validation and Testing indicate the number of conversations (number of utterances) in the respective splits. who had been charged with talking informally about one of several, self-selected general interest topics. For each ut- terance, together with a variety of automatic and semiau- tomatic tools, the tag set distinguishes 42 mutually exclu- sive utterance types via DAMSL taxonomy. The top five frequent DA types include STATEMENT, BACKCHANNEL / ACKNOWLEDGE, OPINION, ABANDONED / UNINTER- PRETABLE, AGREEMENT / ACCEPT. We list the top five percentages of utterance type in the overall corpus in table2. acts in table 3. From the table 2 and table 3, we can see the datasets are highly imbalanced in terms of label distributions. The dialogue act type STATEMENT occupies the largest proportion in both two datasets. Following the second place is the BACKCHANNEL act type which somewhat reflect the speaker\u2019s speech style. We present the detailed data preparation procedure for obtaining the clear dataset. For two datasets, we performed pre-processing steps in order to filter out the noise and some informal nature of utterances. We first strip the exclamations and commas, and then we convert the characters into lower-case. Notice that for SwDA, we only get the training and testing datasets. In order to smooth the training step and tune the parameters, we depart the original training dataset into two parts, one for training and the other small part used to be the validation set. We list the detailed statistics of the two datasets in table 4. 3.2 Evaluation Criteria We mainly evaluate the performance of our proposed CRF-ASN method based on the widely-used evaluation criteria for dialogue act recognition, Accuracy. The Accuracy is the normalized criteria of accessing the quality of the predicted dialogue acts based on",
                "metadata": {
                    "type": "Text",
                    "filename": "1711.05568.pdf",
                    "page_number": 6,
                    "start_index": 0
                }
            },
            {
                "content": "utterances were grouped together into mini-batches, and each ut- terance in a mini-batch was padded to the maximum length for that batch. The maximum batch-size allowed was 48. During training, we set the moving averages of all weights as the exponential decay rate of 0.999 [30]. The whole training process takes approximately 14 hours on a single 1080Ti GPU. All the hyper-parameters were selected by tuning one hyper-parameter at a time while keeping the others fixed. 3.4 Performance Comparisons We compare our propose method with other several state-of-the-art methods for the problem of dialogue act recognition as follows: Among them, The former five approaches eg. Bi-LSTM-CRF, DRLM- Conditional, LSTM-Softmax, RCNN, CNN all adopt the deep neural network model in order to better capture the utterances semantic representations. The latter three methods (HMM, CRF, SVM) just employ the simple feature selection on the text processing. About half of the baselines including Bi-LSTM-CRF, DRLM-Conditional, HMM, CRF consider the graphical structured prediction while the others eg. RCNN, CNN, LSTM-Softmax, SVM just adopt the tradi- tional multi-classification algorithms. Table 5 and Table 6 respectively show the experimental Accuracy results of the methods on the SwDA and MRDA datasets. The hyper- parameters and parameters which achieve the best performance on the validation set are chosen to conduct the testing evaluation. The experiments reveal some interesting points: SwDA and MRDA datasets. Numerically, Our model im- proves the DAR accuracy over Bi-LSTM-CRF by 2.1% and 0.8% on SwDA and MRDA respectively. It is remarkable that our CRF-ASN method is nearly close to the human annota- tors\u2019 performance on SwDA, which is very convincing to prove the superiority of our model.",
                "metadata": {
                    "type": "Text",
                    "page_number": 7,
                    "start_index": 0,
                    "filename": "1711.05568.pdf"
                }
            },
            {
                "content": "4 RELATED WORK In this section, we briefly review some related work on dialogue act recognition and attention network. 4.1 Dialogue Act Recognition The main task of dialogue act recognition is to assign an act label to each utterance in a conversation, which can be defined as a supervised problem due to the properties that each utterance has a corresponding act label. Most of the existing work for the problem of dialogue act recognition can be categorized as following two groups. Regarding the DAR as a multi-classification problem. Rei- thinger et al. [35] present deal with the dialogue act classification using a statistically based language model. Webb et al. [43] apply diverse intra-utterance features involving word n-gram cue phrases to understand the utterance and do the classification. Geertzen et al. [11] propose a multidimensional approach to distinguish and annotate units in dialogue act segmentation and classification. Grau et al. [12] focus on the dialogue act classification using a Bayesian approach. Serafin et al. [37] employ Latent Semantic Analysis (LSA) proper and augmented method to work for dialogue act classifi- cation. Chen et al. [6] had an empirical investigation of sparse log-linear models for improved dialogue act classification. Mila- jevs et al. [33] investigate a series of compositional distributional semantic models to dialogue act classification. Regarding the DAR as a sequence labeling problem. Stol- cke et al. [39] treat the discourse structure of a conversation as a hidden Markov model and the individual dialogue acts as obser- vations emanating from the model states. Tavafi et al. [41] study the effectiveness of supervised learning algorithms SVM-HMM for DA modeling across a comprehensive set of conversations. Similar to the SVM-HMM, Surendran et al. [40] also use a combination of linear support vector machines and hidden markov models for dialog act tagging in the HCRC MapTask corpus. Lendvai et al. [28] explore two sequence learners",
                "metadata": {
                    "start_index": 0,
                    "page_number": 9,
                    "filename": "1711.05568.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "method based on the widely-used evaluation criteria for dialogue act recognition, Accuracy. The Accuracy is the normalized criteria of accessing the quality of the predicted dialogue acts based on the testing utterance set ut. Given the testing conversation C = [u1,u2,...,un]withitsground-truthdialogueactsY = [y1,y2,...,yn], we denote the predicted dialogue acts from our CRF-ASN method by a. We now introduce the evaluation criteria below. 3.3 Implemental Details We preprocess each utterance using the library of nltk [29] and exploit the popular pretrained word embedding Glove with 100 dimensional vectors [34]. The size of char-level embedding is also set as 100-dimensional and is obtained by CNN filters under the instruction of Kim [23]. The Gated Recurrent Unit [7] which is variant from LSTM [15] is employed throughout our model. We adopt the AdaDelta [46] optimizer for training with an initial learn- ing rate of 0.005. We also apply dropout [38]between layers with a dropout rate of 0.2. For the memory network enhanced reasoning, we set the number of hops as 1 to preliminary learn the contex- tual dependencies among utterances. We do not set too many hops as increasing the number of GRU layers reduced the accuracy of the model. Early stopping is also used on the validation set with a patience of 5 epochs. Conversations with the same number of",
                "metadata": {
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 1802,
                    "filename": "1711.05568.pdf"
                }
            },
            {
                "content": "Speaker Utterance DA Label A Hi, long time no see. Greeting B A Hi, how are you? What are you doing these days? Greeting Question B A I\u2019m busying writing my paper. I heard that the deadline is coming. Answer Statement B A Yeah. You need to make a push. Opinion B Sure, that\u2019s why I am so busy now. Agreement A I can\u2019t bother you for too long, goodbye. Farewell B See you later. Farewell",
                "metadata": {
                    "filename": "1711.05568.pdf",
                    "page_number": 1,
                    "type": "Table"
                }
            },
            {
                "content": "Tag Example STATEMENT \"I am working on my projects trying to graduate.\" 36% BACKCHANNEL/ACKNOWLEDGE \"Uh-huh.\" \"Yeah.\" \"All right.\" \"Ok...\" \"Well...\" 19% OPINION \"I think it\u2019s great.\" / \"I don\u2019t believe it can work.\" 13% \"So, -\" \"Are yo-\" \"Maybe-\" 6% AGREEMENT/ACCEPT \"That\u2019s exactly it.\" \"I can\u2019t agree more.\" 5%",
                "metadata": {
                    "filename": "1711.05568.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Top five percentages of utterance type in the SWDA corpus\nTag Example proportion Disruption \"yeah | he == .\" \"yeah | it\u2019s uh == \" 14.73% BackChannel FloorGrabber Question Statement \"okay,\" \"right,\" \"oh,\" \"yes,\" \"yeah,\" \"let\u2019s see,\" \"well,\" \"I mean,\" \"but..\" Y/N, WH, Or \"Beijing is the capital of China\" 10.20% 12.40% 7.20% 55.46%",
                "metadata": {
                    "filename": "1711.05568.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: |C| is the number of Dialogue Act classes, |V | is the vocabulary size. Training, Validation and Testing indicate the number of conversations (number of utterances) in the respective splits.\nSwDA |C | 42 |V | 19K 1003(173K) 112(22K) 19(4K) MRDA 5 10K 51(76K) 11(15K) 11(15K)",
                "metadata": {
                    "filename": "1711.05568.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5: Comparing Accuracy of our method (CRF-ASN) with other methods in the literature on SwDA dataset.\nModel Accuracy(%) Human annotator 84.0 Ours (CRF-ASN) 81.3 Bi-LSTM-CRF 79.2 DRLM-Conditional 77.0 LSTM-Softmax 75.8 RCNN 73.9 CNN 73.1 CRF 71.7 HMM 71.0 SVM 70.6",
                "metadata": {
                    "filename": "1711.05568.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 6: Comparing Accuracy of our method (CRF-ASN) with other methods in the literature on the MRDA dataset.\nModel Accuracy(%) Ours (CRF-ASN) 91.7 Bi-LSTM-CRF 90.9 LSTM-Softmax 86.8 CNN 84.6 CRF 83.9 SVM 82.0",
                "metadata": {
                    "filename": "1711.05568.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 7: Component ablations on SwDA dataset\nModel Accuracy(%) Full CRF-ASN 81.3 Simple CRF 79.5 Simple SVM 77.8 Simple Word Embedding 78.7 Simple Context state 79.1 Simple Memory Network 78.3 Simple Utterance Embedding 79.0",
                "metadata": {
                    "filename": "1711.05568.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            }
        ]
    },
    "2105.03654.pdf": {
        "normalized_output": [
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "WNUT-16 - English",
                "Metric": "F1</s>",
                "Result": "58.14"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "WNUT-17 - English",
                "Metric": "F1</s>",
                "Result": "60.45"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2003 - English",
                "Metric": "F1</s>",
                "Result": "93.56"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL++",
                "Metric": "F1</s>",
                "Result": "94.81"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "BC5CDR",
                "Metric": "F1</s>",
                "Result": "90.99"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "NCBI",
                "Metric": "F1</s>",
                "Result": "89.24"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "E-commerce",
                "Metric": "F1",
                "Result": "84.10"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "New York Times (NYT)",
                "Metric": "Accuracy",
                "Result": "76.38"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "WNUT-17 - English",
                "Metric": "F1</s>",
                "Result": "84.10"
            }
        ],
        "source_documents": [
            {
                "content": "of entities based on short sentences8. For our approaches, with CL, the accuracy can be improved on both input views comparing with W/O CONTEXT and W/ CONTEXT, which shows adding constraints between the two views during training helps the model better utilize the original text information. For the two constraints in CL, we \ufb01nd that CL-KL is relatively stronger than CL-L2 in a majority of the cases. 3.3 Cross-Domain Transfer For cross-domain transfer, we train the models on the CoNLL-03 datasets, evaluate the accuracy on the CBS SciTech News dataset, and compare the results with those in Jia et al. (2019). We evalu- ate our approaches with each input view and the results are shown in Table 3. Our approaches can improve the accuracy in cross-domain evaluation. The external contexts during evaluation can help to improve the accuracy of W/ CONTEXT. However, the gap between the two input views for the CL approaches is diminished. The observation shows that CL is able to improve the accuracy in cross- domain transfer for both views and eliminate the gap between the two views. 3.4 Semi-supervised Cooperative Learning Cooperative learning can take advantage of large amounts of unlabeled text for further improvement. We jointly train on the labeled data and unlabeled data in training to form a semi-supervised train- ing manner. During training, we alternate between minimizing the loss (Eq. 9) for labeled data and the CL loss for unlabeled data (Eq. 4). We conduct the experiment on the E-commerce dataset as an exam- ple. Results in Table 4 show that the accuracy of 7For the result of Bio-BERT (Lee et al., 2020) on NCBI- disease dataset, we report the results reported in of\ufb01cial code (https://github.com/dmis-lab/biobert). The results (89.71 in NCBI-disease) reported in the paper used token-level F1 score instead of entity-level F1 score. 8We have con\ufb01rmed with the authors of LUKE (Yamada et al., 2020) that the accuracy on the CoNLL-03 dataset is consistent with their",
                "metadata": {
                    "type": "Text",
                    "start_index": 1802,
                    "page_number": 6,
                    "filename": "2105.03654.pdf"
                }
            },
            {
                "content": "3 Experiments 3.1 Settings Datasets To show the effectiveness of our ap- proach, we experiment on 8 NER datasets across 5 domains: Annotations of the E-commerce dataset We manually labeled the user queries through crowd- sourcing from www.aliexpress.com, which is a real-world E-commerce website. For each query, we asked one annotator to label the entities and ask another annotator to check the quality. After that, we randomly select 10% of the dataset and ask the third annotator to check the accuracy. As a result, the overall averaged query-level accuracy2 is 95%. The dataset will not be released due to user privacy. Retrieving and Ranking We use an internal E-commerce search engine for the E-commerce dataset. For the other datasets, we use Google Search as the search engine. Google Search is an off-the-shelf search engine and can simulate the of\ufb02ine search over various domains. We use sum- marized descriptions from the search results as the retrieved texts3. As Google Search limits the max- imal length of searching queries to 32 words, we chunk a sentence into multiple sub-sentences based on punctuation if the sentence is longer than 30, feed each sub-sentence to the search engine, and retrieve up to 20 results. We \ufb01lter the retrieved texts that contain any part of the datasets. Our re- ranking module selects top 6 relevant texts4 as the external contexts of the input sentence and chunk the external contexts if the total sub-token lengths of the input sentence and external contexts exceeds 510. Model Con\ufb01gurations For the re-ranking mod- ule, we use Roberta-Large (Liu et al., 2019) for token representations which is the default con\ufb01g- uration in the code5 of BERTScore (Zhang et al., 2020). For token representations in the NER model, 2the accuracy of a query counts 1.0 if all the entities in the query are correctly recognized and 0.0 otherwise. 3If the descriptions are not available, we use the titles of the results instead. 4We determined that 6 is a reasonable",
                "metadata": {
                    "page_number": 5,
                    "start_index": 0,
                    "type": "Text",
                    "filename": "2105.03654.pdf"
                }
            },
            {
                "content": "if all the entities in the query are correctly recognized and 0.0 otherwise. 3If the descriptions are not available, we use the titles of the results instead. 4We determined that 6 is a reasonable number based on preliminary experiments. We show the statistics of the datasets in Table 1. 5https://github.com/Tiiiger/bert_score",
                "metadata": {
                    "type": "Text",
                    "start_index": 1800,
                    "page_number": 5,
                    "filename": "2105.03654.pdf"
                }
            },
            {
                "content": "Table 8: A comparison of retrieved contexts and document-level contexts. \u2020: These approaches are trained on training and development sets. A Retrieved Contexts Versus Document-level contexts on CoNLL-03 We conduct a comparison between our retrieved contexts and the document-level contexts on CoNLL-03 datasets. In Table 8, we report the best model on development set following Yamada et al. (2020). Comparing with previous state-of-the-art approaches with encoding document-level contexts, our approaches are competitive and even stronger than some of the previous approaches utilizing max- imal document-level contexts. Comparing with our model trained on document-level contexts (W/ DOC CONTEXT), we \ufb01nd that there is still a gap between the document-level contexts and retrieved contexts but our CL approaches can reduce the gap between these two contexts.",
                "metadata": {
                    "page_number": 13,
                    "filename": "2105.03654.pdf",
                    "start_index": 0,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Statistics of the dateset split, number of entity types and the average lengths with and without external contexts.\n# Train # Dev # Test # Entity Labels Avg. Length Avg. Length w/ Context WNUT-16 2,394 1,000 3,849 10 19.41 138.58 WNUT-17 3,394 1,009 1,287 6 18.48 139.49 CONLL-03 14,987 3,466 3,684 4 13.64 116.23 CONLL++ 14,987 3,466 3,466 4 13.64 116.23 BC5CDR 4,560 4,581 4,797 2 25.91 144.13 NCBI 5,424 923 940 1 25.01 135.76 E-COMMERCE 38,959 5,000 5,000 26 2.54 124.61",
                "metadata": {
                    "filename": "2105.03654.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: A comparison among recent state-of-the-art models, the baseline and our approaches. \u2020 represents the model is signi\ufb01cantly stronger than the baseline model (W/O CONTEXT) with p < 0.05 on Student\u2019s T test.\nSocial Media News Biomedical WNUT-16 WNUT-17 CoNLL-03 CoNLL++ BC5CDR NCBI E-commerce Zhou et al. (2019) 55.43 42.83 - - - - - Nguyen et al. (2020) 52.10 56.50 - - - - - Nie et al. (2020) 55.01 50.36 - - - - - Baevski et al. (2019) - - 93.50 - - - - Wang et al. (2019) - - 93.43 94.28 - - - Li et al. (2020) - - 93.33 - - - - Nooralahzadeh et al. (2019) - - - - 89.93 - - Bio-Flair (2019) - - - - 89.42 88.85 - Bio-BERT (2020) - - - - - 87.70 - Evaluation: W/O CONTEXT LUKE (2020) 54.04 55.22 92.42 93.99 89.18 87.62 77.64 W/O CONTEXT 56.04 57.86 93.03 94.20 90.52 88.65 81.47 CL-L2 57.35\u2020 58.68\u2020 93.08 94.38\u2020 90.70\u2020 89.20\u2020 82.43\u2020 CL-KL 58.14\u2020 59.33\u2020 93.21\u2020 94.55\u2020 90.73\u2020 89.24\u2020 82.31\u2020 Evaluation: W/ CONTEXT W/ CONTEXT 57.43\u2020 60.20\u2020 93.27\u2020 94.56\u2020 90.76\u2020 89.01\u2020 83.15\u2020 CL-L2 58.61\u2020 60.26\u2020 93.47\u2020 94.62\u2020 90.99\u2020 89.22\u2020 83.87\u2020 CL-KL 58.98\u2020 60.45\u2020 93.56\u2020 94.81\u2020 90.93\u2020 88.96\u2020 83.99\u2020",
                "metadata": {
                    "filename": "2105.03654.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Evaluation Science and Technology Approach W/O CONTEXT W/ CONTEXT Jia et al. (2019) 73.59 - W/O CONTEXT 75.87 75.74 W/ CONTEXT CL-L2 CL-KL 75.72 76.16 76.37 75.94 76.10 76.38",
                "metadata": {
                    "filename": "2105.03654.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5: A comparison of different re-ranking ap- proaches by the F1 scores on WNUT-17. SE: Search engine. FM: Fuzzy match score. BS: BERTScore.\nSE FM BS BS+tf-idf AVG. 59.95 59.54 60.20 59.71 BEST 61.79 60.89 62.29 60.96",
                "metadata": {
                    "filename": "2105.03654.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: A comparison between of CL approaches with and without semi-supervised learning. SEMI rep- resents the approaches with semi-supervised learning. \u2020 represents the approach is signi\ufb01cantly (p < 0.05) stronger than the approach without semi-supervised learning with the same input view.\nEvaluation Approach W/O CONTEXT W/ CONTEXT CL-L2 CL-KL CL\u2013L2+SEMI 82.43 82.31 82.88\u2020 83.87 83.99 83.92 CL-KL+SEMI 82.58\u2020 84.10",
                "metadata": {
                    "filename": "2105.03654.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 6: A comparison among different contexts types.\nWNUT-17 w/ Context (Ours) 60.20 w/o Context 57.86 w/ Context (Dataset) 57.21 w/ Context (Generated) 57.71 w/ Context (Random Retrieved) 57.53 w/ Context (Random Data) 47.69",
                "metadata": {
                    "filename": "2105.03654.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 7: An ablation study of the training and predic- tion of models.\nEvaluation Approach W/O CONTEXT W/ CONTEXT W/O CONTEXT 57.86 59.40 W/ CONTEXT 57.46 60.20 W/O CL 58.14 59.64 CL-L2 + CL-KL 58.69 60.16 CL-L2 58.68 60.26 CL-KL 59.33 60.45",
                "metadata": {
                    "filename": "2105.03654.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 8: A comparison of retrieved contexts and document-level contexts. \u2020: These approaches are trained on training and development sets.\nApproach CoNLL-03 Yu et al. (2020)\u2020 93.50 Yamada et al. (2020) 94.30 Luoma and Pyysalo (2020)\u2020 93.74 Wang et al. (2021a) 94.60 W/ DOC CONTEXT 94.12 W/O CONTEXT 93.30 W/ CONTEXT 93.55 CL-L2 93.68 CL-KL 93.85",
                "metadata": {
                    "filename": "2105.03654.pdf",
                    "page_number": 13,
                    "type": "Table"
                }
            }
        ]
    },
    "1908.08345.pdf": {
        "normalized_output": [
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-1",
                "Result": "41.72"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-2",
                "Result": "19.39"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-L",
                "Result": "38.76"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-1",
                "Result": "43.85"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-2</s>",
                "Result": "20.34"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-L",
                "Result": "39.90"
            },
            {
                "Task": "Summarization",
                "Dataset": "New York Times (NYT)",
                "Metric": "ROGUE-1</s>",
                "Result": "48.92"
            },
            {
                "Task": "Summarization",
                "Dataset": "New York Times (NYT)",
                "Metric": "ROGUE-2",
                "Result": "30.84"
            },
            {
                "Task": "Summarization",
                "Dataset": "New York Times (NYT)",
                "Metric": "ROGUE-L",
                "Result": "45.41"
            },
            {
                "Task": "Summarization",
                "Dataset": "New York Times (NYT)",
                "Metric": "ROGUE-1",
                "Result": "46.66"
            },
            {
                "Task": "Summarization",
                "Dataset": "New York Times (NYT)",
                "Metric": "ROGUE-2",
                "Result": "26.35"
            },
            {
                "Task": "Summarization",
                "Dataset": "New York Times (NYT)",
                "Metric": "ROGUE-L",
                "Result": "42.62"
            },
            {
                "Task": "Summarization",
                "Dataset": "XSum",
                "Metric": "ROGUE-1",
                "Result": "38.81"
            },
            {
                "Task": "Summarization",
                "Dataset": "XSum",
                "Metric": "ROGUE-2",
                "Result": "16.50"
            },
            {
                "Task": "Summarization",
                "Dataset": "XSum",
                "Metric": "ROGUE-L</s>",
                "Result": "31.27"
            }
        ],
        "source_documents": [
            {
                "content": "saved and evaluated on the validation set ev- ery 2,500 steps. We selected the top-3 checkpoints based on their evaluation loss on the validation set, and report the averaged results on the test set. During decoding we used beam search (size 5), and tuned the \u03b1 for the length penalty (Wu et al., 2016) between 0.6 and 1 on the validation set; we decode until an end-of-sequence token is emitted and repeated trigrams are blocked (Paulus et al., 2018). It is worth noting that our decoder ap- plies neither a copy nor a coverage mechanism (See et al., 2017), despite their popularity in ab- stractive summarization. This is mainly because Table 2: ROUGE F1 results on CNN/DailyMail test set (R1 and R2 are shorthands for unigram and bigram overlap; RL is the longest common subsequence). Re- sults for comparison systems are taken from the au- thors\u2019 respective papers or obtained on our data by run- ning publicly released software. we focus on building a minimum-requirements model and these mechanisms may introduce ad- ditional hyper-parameters to tune. Thanks to the subwords tokenizer, we also rarely observe is- sues with out-of-vocabulary words in the out- put; moreover, trigram-blocking produces diverse summaries managing to reduce repetitions. 5 Results 5.1 Automatic Evaluation We evaluated summarization quality automati- cally using ROUGE (Lin, 2004). We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a means of assessing informa- tiveness and the longest common subsequence (ROUGE-L) as a means of assessing \ufb02uency. Table 2 summarizes our results on the CNN/DailyMail dataset. The \ufb01rst block in the ta- ble includes the results of an extractive ORACLE system as an upper bound. We also present the LEAD-3 baseline (which simply selects the \ufb01rst three sentences in a document). The second block in the table includes various extractive models trained on the CNN/DailyMail dataset (see Section 2.2 for an overview). For",
                "metadata": {
                    "type": "Text",
                    "filename": "1908.08345.pdf",
                    "page_number": 6,
                    "start_index": 1800
                }
            },
            {
                "content": "training/test exam- ples, based on the date of publication (the test set contains all articles published from January 1, 2007 onward). We used 4,000 examples from the training as validation set. We also followed their \ufb01ltering procedure, documents with summaries less than 50 words were removed from the dataset. The \ufb01ltered test set (NYT50) includes 3,452 ex- amples. Sentences were split with the Stanford CoreNLP toolkit (Manning et al., 2014) and pre- processed following Durrett et al. (2016). Input documents were truncated to 800 tokens. XSum contains 226,711 news articles accompa- nied with a one-sentence summary, answering the question \u201cWhat is this article about?\u201d. We used the splits of Narayan et al. (2018a) for training, valida- tion, and testing (204,045/11,332/11,334) and fol- lowed the pre-processing introduced in their work. Input documents were truncated to 512 tokens. Aside from various statistics on the three datasets, Table 1 also reports the proportion of novel bi-grams in gold summaries as a measure of their abstractiveness. We would expect mod- els with extractive biases to perform better on datasets with (mostly) extractive summaries, and abstractive models to perform more rewrite op- erations on datasets with abstractive summaries. CNN/DailyMail and NYT are somewhat extrac- tive, while XSum is highly abstractive. CNN/DailyMail contains news articles and as- sociated highlights, i.e., a few bullet points giving a brief overview of the article. We used the stan- dard splits of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 CNN documents and 196,961/12,148/10,397 DailyMail documents). We did not anonymize entities. We Implementation Details For both extractive and abstractive settings, we used PyTorch, OpenNMT (Klein et al., 2017) and the \u2018bert-base-uncased\u20192 version of BERT to im- plement BERTSUM. Both source and target texts 2https://git.io/fhbJQ",
                "metadata": {
                    "type": "Text",
                    "filename": "1908.08345.pdf",
                    "start_index": 1806,
                    "page_number": 5
                }
            },
            {
                "content": "coder (Vaswani et al., 2017). We design a new training schedule which separates the optimizers of the encoder and the decoder in order to accom- modate the fact that the former is pretrained while the latter must be trained from scratch. Finally, motivated by previous work showing that the com- bination of extractive and abstractive objectives can help generate better summaries (Gehrmann et al., 2018), we present a two-stage approach where the encoder is \ufb01ne-tuned twice, \ufb01rst with an extractive objective and subsequently on the ab- stractive summarization task. We evaluate the proposed approach on three single-document news summarization datasets representative of different writing conventions (e.g., important information is concentrated at the beginning of the document or distributed more evenly throughout) and summary styles (e.g., ver- bose vs. more telegraphic; extractive vs. abstrac- tive). Across datasets, we experimentally show that the proposed models achieve state-of-the-art results under both extractive and abstractive set- tings. Our contributions in this work are three- fold: a) we highlight the importance of document encoding for the summarization task; a variety of recently proposed techniques aim to enhance summarization performance via copying mecha- nisms (Gu et al., 2016; See et al., 2017; Nallap- ati et al., 2017), reinforcement learning (Narayan et al., 2018b; Paulus et al., 2018; Dong et al., 2018), and multiple communicating encoders (Ce- likyilmaz et al., 2018). We achieve better results with a minimum-requirement model without using any of these mechanisms; b) we showcase ways to effectively employ pretrained language models in summarization under both extractive and abstrac- tive settings; we would expect any improvements in model pretraining to translate in better summa- rization in the future; and c) the proposed models can be used as a stepping stone to further improve summarization performance as well as baselines against which new",
                "metadata": {
                    "filename": "1908.08345.pdf",
                    "type": "Text",
                    "start_index": 0,
                    "page_number": 2
                }
            },
            {
                "content": "were tokenized with BERT\u2019s subwords tokenizer. Extractive Summarization All extractive mod- els were trained for 50,000 steps on 3 GPUs (GTX 1080 Ti) with gradient accumulation every two steps. Model checkpoints were saved and evalu- ated on the validation set every 1,000 steps. We selected the top-3 checkpoints based on the evalu- ation loss on the validation set, and report the av- eraged results on the test set. We used a greedy al- gorithm similar to Nallapati et al. (2017) to obtain an oracle summary for each document to train ex- tractive models. The algorithm generates an oracle consisting of multiple sentences which maximize the ROUGE-2 score against the gold summary. When predicting summaries for a new docu- ment, we \ufb01rst use the model to obtain the score for each sentence. We then rank these sentences by their scores from highest to lowest, and select the top-3 sentences as the summary. During sentence selection we use Trigram Blocking to reduce redundancy (Paulus et al., 2018). Given summary S and candidate sen- tence c, we skip c if there exists a trigram over- lapping between c and S. The intuition is simi- lar to Maximal Marginal Relevance (MMR; Car- bonell and Goldstein 1998); we wish to minimize the similarity between the sentence being consid- ered and sentences which have been already se- lected as part of the summary. Abstractive Summarization In all abstractive models, we applied dropout (with probability 0.1) before all linear layers; label smoothing (Szegedy et al., 2016) with smoothing factor 0.1 was also used. Our Transformer decoder has 768 hidden units and the hidden size for all feed-forward lay- ers is 2,048. All models were trained for 200,000 steps on 4 GPUs (GTX 1080 Ti) with gradient ac- cumulation every \ufb01ve steps. Model checkpoints were saved and evaluated on the validation set ev- ery 2,500 steps. We selected the top-3 checkpoints based on their evaluation loss on the validation set, and report the averaged results on the test set.",
                "metadata": {
                    "page_number": 6,
                    "type": "Text",
                    "filename": "1908.08345.pdf",
                    "start_index": 0
                }
            },
            {
                "content": "Table 1: Comparison of summarization datasets: size of training, validation, and test sets and average document and summary length (in terms of words and sentences). The proportion of novel bi-grams that do not appear in source documents but do appear in the gold summaries quanti\ufb01es corpus bias towards extractive methods.\nDatasets # docs (train/val/test) avg. doc length avg. summary length % novel bi-grams words sentences words sentences in gold summary CNN 90,266/1,220/1,093 760.50 33.98 45.70 3.59 52.90 DailyMail 196,961/12,148/10,397 653.33 29.33 54.65 3.86 52.16 NYT 96,834/4,000/3,452 800.04 35.55 45.54 2.44 54.70 XSum 204,045/11,332/11,334 431.07 19.77 23.26 1.00 83.31",
                "metadata": {
                    "filename": "1908.08345.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: ROUGE F1 results on CNN/DailyMail test set (R1 and R2 are shorthands for unigram and bigram overlap; RL is the longest common subsequence). Re- sults for comparison systems are taken from the au- thors\u2019 respective papers or obtained on our data by run- ning publicly released software.\nModel R1 R2 RL ORACLE 52.59 31.24 48.87 LEAD-3 40.42 17.62 36.67 Extractive SUMMARUNNER (Nallapati et al., 2017) 39.60 16.20 35.30 REFRESH (Narayan et al., 2018b) 40.00 18.20 36.60 LATENT (Zhang et al., 2018) 41.05 18.77 37.54 NEUSUM (Zhou et al., 2018) 41.59 19.01 37.98 SUMO (Liu et al., 2019) 41.00 18.40 37.20 TransformerEXT 40.90 18.02 37.17 Abstractive PTGEN (See et al., 2017) 36.44 15.66 33.42 PTGEN+COV (See et al., 2017) 39.53 17.28 36.38 DRM (Paulus et al., 2018) 39.87 15.82 36.90 BOTTOMUP (Gehrmann et al., 2018) 41.22 18.68 38.34 DCA (Celikyilmaz et al., 2018) 41.69 19.47 37.92 TransformerABS 40.21 17.76 37.09 BERT-based BERTSUMEXT 43.25 20.24 39.63 BERTSUMEXT w/o interval embeddings 43.20 20.22 39.59 BERTSUMEXT (large) 43.85 20.34 39.90 BERTSUMABS 41.72 19.39 38.76 BERTSUMEXTABS 42.13 19.60 39.18",
                "metadata": {
                    "filename": "1908.08345.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: ROUGE Recall results on NYT test set. Re- sults for comparison systems are taken from the au- thors\u2019 respective papers or obtained on our data by run- ning publicly released software. Table cells are \ufb01lled with \u2014 whenever results are not available.\nModel R1 R2 RL ORACLE 49.18 33.24 46.02 LEAD-3 39.58 20.11 35.78 Extractive COMPRESS (Durrett et al., 2016) 42.20 24.90 \u2014 SUMO (Liu et al., 2019) 42.30 22.70 38.60 TransformerEXT 41.95 22.68 38.51 Abstractive PTGEN (See et al., 2017) 42.47 25.61 \u2014 PTGEN + COV (See et al., 2017) 43.71 26.40 \u2014 DRM (Paulus et al., 2018) 42.94 26.02 \u2014 TransformerABS 35.75 17.23 31.41 BERT-based BERTSUMEXT 46.66 26.35 42.62 BERTSUMABS 48.92 30.84 45.41 BERTSUMEXTABS 49.02 31.02 45.55",
                "metadata": {
                    "filename": "1908.08345.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: ROUGE F1 results on the XSum test set. Results for comparison systems are taken from the au- thors\u2019 respective papers or obtained on our data by run- ning publicly released software.\nModel R1 R2 RL ORACLE 29.79 8.81 22.66 LEAD 16.30 1.60 11.95 Abstractive PTGEN (See et al., 2017) 29.70 9.21 23.24 PTGEN+COV (See et al., 2017) 28.10 8.02 21.72 TCONVS2S (Narayan et al., 2018a) 31.89 11.54 25.75 TransformerABS 29.41 9.77 23.01 BERT-based BERTSUMABS 38.76 16.33 31.15 BERTSUMEXTABS 38.81 16.50 31.27",
                "metadata": {
                    "filename": "1908.08345.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5: Model perplexity (CNN/DailyMail; valida- tion set) under different combinations of encoder and decoder learning rates.\n\u02dclrE 1 0.1 0.01 0.001 2e-2 50.69 9.33 10.13 19.26 2e-3 37.21 8.73 9.52 16.88",
                "metadata": {
                    "filename": "1908.08345.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Figure 3: Proportion of novel n-grams in model gener- ated summaries.\nExtractive CNN/DM NYT LEAD 42.5\u2020 36.2\u2020 NEUSUM 42.2\u2020 \u2014 SUMO 41.7\u2020 38.1\u2020 Transformer 37.8\u2020 32.5\u2020 BERTSUM 58.9 41.9",
                "metadata": {
                    "filename": "1908.08345.pdf",
                    "page_number": 9,
                    "type": "Table"
                }
            },
            {
                "content": "Table 7: QA-based and ranking-based evaluation. Models with \u2020 are signi\ufb01cantly different from BERT- SUM (using a paired student t-test; p < 0.05). Table cells are \ufb01lled with \u2014 whenever system output is not available. GOLD is not used in QA setting, and LEAD is not used in Rank evaluation.\nCNN/DM NYT XSum Abstractive QA Rank QA Rank QA Rank LEAD 42.5\u2020 \u2014 36.2\u2020 \u2014 9.20\u2020 \u2014 PTGEN 33.3\u2020 -0.24\u2020 30.5\u2020 -0.27\u2020 23.7\u2020 -0.36\u2020 BOTTOMUP 40.6\u2020 -0.16\u2020 \u2014 \u2014 \u2014 \u2014 TCONVS2S \u2014 \u2014 \u2014 \u2014 52.1 -0.20\u2020 GOLD \u2014 0.22\u2020 \u2014 0.33\u2020 \u2014 0.38\u2020 BERTSUM 56.1 0.17 41.8 -0.07 57.5 0.19",
                "metadata": {
                    "filename": "1908.08345.pdf",
                    "page_number": 9,
                    "type": "Table"
                }
            }
        ]
    },
    "1909.02188.pdf": {
        "normalized_output": [
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "SNIPS",
                "Metric": "F1",
                "Result": "97.0"
            },
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "SNIPS",
                "Metric": "Accuracy",
                "Result": "99.0"
            },
            {
                "Task": "Dialogue Act Classification",
                "Dataset": "SNIPS",
                "Metric": "Overall-Accuracy",
                "Result": "92.9"
            },
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "ATIS",
                "Metric": "F1",
                "Result": "96.1"
            },
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "ATIS",
                "Metric": "Accuracy",
                "Result": "97.5"
            },
            {
                "Task": "Dialogue Act Classification",
                "Dataset": "ATIS",
                "Metric": "Overall-Accuracy",
                "Result": "88.6"
            }
        ],
        "source_documents": [
            {
                "content": "\ufb01lling. In addition, we perform the token-level intent detection to improve the intent detection performance and further ease the error propaga- tion. Experiments on two datasets show the effec- tiveness of the proposed models and achieve the state-of-the-art performance. Besides, we explore and analyze the effect of incorporating strong pre- trained BERT model in SLU tasks. With BERT, the result reaches a new state-of-the-art level. Acknowledgments We thank the anonymous reviewers for their help- ful comments and suggestions. This work was supported by the National Natural Science Foun- dation of China (NSFC) via grant 61976072, 61632011 and 61772153. References",
                "metadata": {
                    "start_index": 0,
                    "type": "Text",
                    "page_number": 9,
                    "filename": "1909.02188.pdf"
                }
            },
            {
                "content": "sig- ni\ufb01cantly outperforms all the baselines by a large margin and achieves the state-of-the-art perfor- mance. In the SNIPS dataset, compared with the best prior joint work Bi-Model, we achieve 0.7% improvement on Slot (F1) score, 0.8% improve- ment on Intent (Acc) and 3.1% improvement on Overall (Acc). In the ATIS dataset, we achieve 0.4% improvement on Slot (F1) score, 0.5% im- provement on Intent (Acc) and 0.8% improvement on Overall (Acc). This indicates the effective- ness of our Stack-Propagation framework. Espe- cially, our framework gains the largest improve- ments on sentence-level semantic frame accuracy, 2All experiments are conducted on the publicly datasets provided by Goo et al. (2018), Self-Attentive Model and Bi- Model don\u2019t have the reported result on the same datasets or they did different preprocessing. For directly comparison, we re-implemented the models and obtained the results on the ATIS and SNIPS datasets preprocessed by Goo et al. (2018). Because all baselines and our model don\u2019t apply CRF layer, we just report the best performance of SF-ID Network with- out CRF. It\u2019s noticing that our model does outperform SF-ID Network with CRF layer.",
                "metadata": {
                    "type": "Text",
                    "page_number": 5,
                    "start_index": 1791,
                    "filename": "1909.02188.pdf"
                }
            },
            {
                "content": "4 Experiments 4.1 Experimental Settings To evaluate the ef\ufb01ciency of our proposed model, we conduct experiments on two bench- mark datasets. One is the publicly ATIS dataset (Hemphill et al., 1990) containing audio record- ings of \ufb02ight reservations, and the other is the custom-intent-engines collected by Snips (SNIPS dataset) (Coucke et al., 2018). 1 Both datasets used in our paper follows the same format and par- tition as in Goo et al. (2018). The dimensionalities of the word embedding is 256 for ATIS dataset and 512 for SNIPS dataset. The self-attentive encoder hidden units are set as 256. L2 regularization is used on our model is 1 \u00d7 10\u22126 and dropout ra- tio is adopted is 0.4 for reducing over\ufb01t. We use Adam (Kingma and Ba, 2014) to optimize the pa- rameters in our model and adopted the suggested hyper-parameters for optimization. For all the ex- periments, we select the model which works the best on the dev set, and then evaluate it on the test set. 4.2 Baselines We compare our model with the existing baselines including: 1https://github.com/snipsco/ nlu-benchmark/tree/master/ 2017-06-custom-intent-engines For the Joint Seq, Attention BiRNN, Slot-gated Atten, CAPSULE-NLU and SF-ID Network, we adopt the reported results from Goo et al. (2018); Zhang et al. (2019); E et al. (2019). For the Self- Attentive Model, Bi-Model, we re-implemented the models and obtained the results on the same datasets.2 4.3 Overall Results Following Goo et al. (2018), we evaluate the SLU performance of slot \ufb01lling using F1 score and the performance of intent prediction using accuracy, and sentence-level semantic frame parsing using overall accuracy. Table 2 shows the experiment re- sults of the proposed models on SNIPS and ATIS datasets. From the table, we can see that our model sig- ni\ufb01cantly outperforms all the baselines by a large margin and achieves the state-of-the-art perfor- mance. In the SNIPS dataset, compared with the best prior joint work Bi-Model, we achieve 0.7%",
                "metadata": {
                    "start_index": 0,
                    "type": "Text",
                    "filename": "1909.02188.pdf",
                    "page_number": 5
                }
            },
            {
                "content": "which demonstrates the signi\ufb01cance and effectiveness of incorporating token-level intent information. The main reason for this can be that incorporating the token-level intent information can retain useful features for each token and ease the error propa- gation. Results are shown in the without self-attention row of Table 3. We can observe the self-attention mechanism can further improve the SLU perfor- mance. We attribute this to the fact that self- attention mechanism can capture the contextual information for each token. Without the self- attention mechanism, it will harm the intent de- tection and have bad in\ufb02uence on slot \ufb01lling task by joint learning. It is noticeable that even without the self- attention mechanism, our framework still per- forms the state-of-the-art Bi-model model (Li et al., 2018), which again demonstrates the effec- tiveness and robustness of our other framework components. 4.5 Effect of BERT Finally, we also conduct experiments to use pre- trained model, BERT (Devlin et al., 2018), boost SLU performance. In this section, we re- place the self-attentive encoder by BERT base model with the \ufb01ne-tuning approach and keep other components as same with our framework. to Table 4 gives the results of BERT model on ATIS and SNIPS datasets. From the table, the BERT model performs remarkably well on both two datasets and achieves a new state-of-the-art performance, which indicates the effectiveness of a strong pre-trained model in SLU tasks. We at- tribute this to the fact that pre-trained models can provide rich semantic features, which can help to improve the performance on SLU tasks. In addi- tion, our model + BERT outperforms the BERT SLU (Chen et al., 2019) which apply BERT for",
                "metadata": {
                    "filename": "1909.02188.pdf",
                    "start_index": 1801,
                    "page_number": 7,
                    "type": "Text"
                }
            },
            {
                "content": "Sentence Gold Slots watch O action B-movie name movie I-movie name Gold Intent WatchMovie",
                "metadata": {
                    "filename": "1909.02188.pdf",
                    "page_number": 1,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Slot \ufb01lling and intent detection results on two datasets. The numbers with * indicate that the improvement of our model over all baselines is statistically signi\ufb01cant with p < 0.05 under t-test.\nModel Slot (F1) SNIPS Intent (Acc) Overall (Acc) Slot (F1) ATIS Intent (Acc) Overall (Acc) Joint Seq (Hakkani-T\u00a8ur et al., 2016) 87.3 96.9 73.2 94.3 92.6 80.7 Attention BiRNN (Liu and Lane, 2016) 87.8 96.7 74.1 94.2 91.1 78.9 Slot-Gated Full Atten (Goo et al., 2018) 88.8 97.0 75.5 94.8 93.6 82.2 Slot-Gated Intent Atten (Goo et al., 2018) 88.3 96.8 74.6 95.2 94.1 82.6 Self-Attentive Model (Li et al., 2018) 90.0 97.5 81.0 95.1 96.8 82.2 Bi-Model (Wang et al., 2018) 93.5 97.2 83.8 95.5 96.4 85.7 CAPSULE-NLU (Zhang et al., 2019) 91.8 97.3 80.9 95.2 95.0 83.4 SF-ID Network (E et al., 2019) 90.5 97.0 78.4 95.6 96.6 86.0 Our model 94.2* 98.0* 86.9* 95.9* 96.9* 86.5* Oracle (Intent) 96.1 - - 96.0 - -",
                "metadata": {
                    "filename": "1909.02188.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: The SLU performance on baseline models compared with our Stack-Propagation model on two datasets.\nModel Slot (F1) SNIPS Intent (Acc) Overall (Acc) Slot (F1) ATIS Intent (Acc) Overall (Acc) gate-mechanism 92.2 97.6 82.4 95.3 96.2 83.4 pipelined model 90.8 97.6 81.8 95.1 96.1 82.3 sentence intent augmented 93.7 97.5 86.1 95.5 96.7 85.8 lstm+last-hidden - 97.1 - - 95.2 - lstm+token-level - 97.5 - - 96.0 - without self-attention 94.1 97.8 86.6 95.6 96.6 86.2 Our model 94.2 98.0 86.9 95.9 96.9 86.5",
                "metadata": {
                    "filename": "1909.02188.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4 gives the results of BERT model on ATIS and SNIPS datasets. From the table, the BERT model performs remarkably well on both two datasets and achieves a new state-of-the-art performance, which indicates the effectiveness of a strong pre-trained model in SLU tasks. We at- tribute this to the fact that pre-trained models can provide rich semantic features, which can help to improve the performance on SLU tasks. In addi- tion, our model + BERT outperforms the BERT SLU (Chen et al., 2019) which apply BERT for\nModel Slot (F1) SNIPS Intent (Acc) Overall (Acc) Slot (F1) ATIS Intent (Acc) Overall (Acc) Our model 94.2 98.0 86.9 95.9 96.9 86.5 Intent detection (BERT) - 97.8 - - 96.5 - Slot \ufb01lling (BERT) 95.8 - - 95.6 - - BERT SLU (Chen et al., 2019) 97.0 98.6 92.8 96.1 97.5 88.2 Our model + BERT 97.0 99.0 92.9 96.1 97.5 88.6",
                "metadata": {
                    "filename": "1909.02188.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            }
        ]
    },
    "1812.10235.pdf": {
        "normalized_output": [
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "ATIS",
                "Metric": "Accuracy",
                "Result": "98.99%"
            },
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "ATIS",
                "Metric": "F-Score (F-S)",
                "Result": "96.89%"
            },
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "Multi-domain Movie",
                "Metric": "Accuracy",
                "Result": "95.91%"
            },
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "Multi-domain Movie",
                "Metric": "F-Score (F-S)",
                "Result": "93.8%"
            },
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "Multi-domain Food",
                "Metric": "Accuracy",
                "Result": "99.49%"
            },
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "Multi-domain Food",
                "Metric": "F1</s>",
                "Result": "95.8%"
            },
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "Multi-domain Home",
                "Metric": "Accuracy",
                "Result": "99.27%"
            },
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "Multi-domain Home",
                "Metric": "F-Score (F-S)",
                "Result": "98.2%"
            }
        ],
        "source_documents": [
            {
                "content": "ac- curacy for intent detection task and F1-score for slot \ufb01lling task. 4.1 Training Setup The layer sizes for both the LSTM and BLSTM networks in our model are chosen as 200. Based on the size of our dataset, the number of hidden layers is chosen as 2 and Adam optimization is used as in (Kingma and Ba, 2014). The size of word embedding is 300, which are initialized ran- domly at the beginning of experiment. 4.2 Performance on the ATIS dataset Our \ufb01rst experiment is conducted on the ATIS benchmark dataset, and compared with the current existing approaches, by evaluating their intent detection accuracy and slot \ufb01lling F1 scores. A Intent Accuracy Table 1: Performance of Different Models on ATIS Dataset detailed comparison is given in Table 1. Some of the models are designed for single slot \ufb01lling task, hence only F1 scores are given. It can be observed that the new proposed Bi-model structures outperform the current state-of-the-art results on both intent detection and slot \ufb01lling tasks, and the Bi-model with a decoder also outperform that without a decoder on our ATIS dataset. The current Bi-model with a decoder shows the state-of-the-art performance on ATIS benchmark dataset with 0.9% improvement on F1 score and 0.5% improvement on intent accuracy. Remarks: 1. It is worth noticing that the complexities of encoder-decoder based models are normally higher than the models without using encoder- decoder structures, since two networks are used and more parameters need to be updated. This is another reason why we use two models with/without using encoder-decoder structures to demonstrate the new bi-model structure design. It can also be observed that the model with a decoder gives a better result due to its higher complexity. 2. It is also shown in the table that the joint model in (Liu and Lane, 2015, 2016a) achieves",
                "metadata": {
                    "filename": "1812.10235.pdf",
                    "page_number": 4,
                    "type": "Text",
                    "start_index": 1806
                }
            },
            {
                "content": "is performed on the ATIS benchmark dataset, in order to demonstrate a state-of-the-art result for both semantic parsing tasks. The other experiment is tested on our inter- nal multi-domain dataset by comparing our new algorithm with the current best performed RNN based joint model in literature for intent detection and slot \ufb01lling.",
                "metadata": {
                    "start_index": 3595,
                    "page_number": 1,
                    "type": "Text",
                    "filename": "1812.10235.pdf"
                }
            },
            {
                "content": "better performance on intent detection task with slight degradation on slot \ufb01lling, so a joint model is not necessary always better for both tasks. The bi-model approach overcomes this issue by generating two tasks\u2019 results separately. 3. Despite the absolute improvement of intent accuracy and F1 scores are only 0.5% and 0.9% on ATIS dataset, the relative improvement is not small. For intent accuracy, the number of wrongly classi\ufb01ed utterances in test dataset reduced from 14 to 9, which gives us the 35.7% relative improvement on intent accuracy. Similarly, the relative improvement on F1 score is 22.63%. 4.3 Performance on multi-domain data In this experiment, the Bi-model structures are fur- ther tested on an internal collected dataset from our users in three domains: food, home and movie. There are 3 intents for each domain, 15 semantic tags in food domain, 16 semantic tags in home do- main, 14 semantic tags in movie domain. The data size of each domain is listed as in Table 2, and the split is 70% for training, 10% for validation and 20% for test. Due to the space limitation, only the best per- formed semantic frame parsing model on ATIS dataset in literature,i.e. attention based BiRNN (Liu and Lane, 2016a) is used for comparison with our Bi-model structures. Table 2 shows a perfor- mance comparison in three domains of data. The Bi-model structure with a decoder gives the best performance in all cases based on its intent accu- racy and slot \ufb01lling F1 score. The intent accuracy has at least 0.5% improvement, the F1 score im- provement is around 1% to 3% for different do- mains. 5 Conclusion In this paper, a novel Bi-model based RNN seman- tic frame parsing model for intent detection and slot \ufb01lling is proposed and tested. Two substruc- tures are discussed with the help of a decoder or not. The Bi-model structures achieve state-of-the- art performance for both intent detection and slot \ufb01lling on ATIS benchmark data, and also surpass the previous best SLU model on",
                "metadata": {
                    "start_index": 0,
                    "page_number": 5,
                    "filename": "1812.10235.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "2 Background In this section, a brief background overview on using deep learning and RNN based approaches to perform intent detection and slot \ufb01lling tasks is given. The joint model algorithm is also discussed for further comparison purpose. 2.1 Deep neural network for intent detection Using deep neural networks for intent detection is similar to a standard classi\ufb01cation problem, the only difference is that this classi\ufb01er is trained un- der a speci\ufb01c domain. For example, all data in ATIS dataset is under the \ufb02ight reservation do- main with 18 different intent labels. There are mainly two types of models that can be used: one is a feed-forward model by taking the average of all words\u2019 vectors in an utterance as its input, the other way is by using the recurrent neural network which can take each word in an utterance as a vec- tor one by one (Xu and Sarikaya, 2014). 2.2 Recurrent Neural network for slot \ufb01lling The slot \ufb01lling task is a bit different from intent detection as there are multiple outputs for the task, hence only RNN model is a feasible approach for this scenario. The most straight-forward way is using single RNN model generating multiple se- manctic tags sequentially by reading in each word one by one (Liu and Lane, 2015; Mesnil et al., 2015; Peng and Yao, 2015). This approach has a constrain that the number of slot tags generated should be the same as that of words in an utter- ance. Another way to overcome this limitation is by using an encoder-decoder model containing two RNN models as an encoder for input and a decoder for output (Liu and Lane, 2016a). The ad- vantage of doing this is that it gives the system ca- pability of matching an input utterance and output slot tags with different lengths without the need of alignment. Besides using RNN, It is also possible to use the convolutional neural network (CNN) to- gether with a conditional random \ufb01eld (CRF) to achieve slot \ufb01lling task (Xu and Sarikaya, 2013). Joint model for two tasks It is also",
                "metadata": {
                    "type": "Text",
                    "filename": "1812.10235.pdf",
                    "page_number": 2,
                    "start_index": 0
                }
            },
            {
                "content": "Model F1 Score Recursive NN 93.96% 95.4% (Guo et al., 2014) Joint model with recurrent intent 94.47% 98.43% and slot label context (Liu and Lane, 2016b) Joint model with recurrent slot 94.64% 98.21% label context (Liu and Lane, 2016b) RNN with Label Sampling 94.89% NA (Liu and Lane, 2015) Hybrid RNN 95.06% NA (Mesnil et al., 2015) RNN-EM 95.25% NA (Peng and Yao, 2015) CNN CRF 95.35% NA (Xu and Sarikaya, 2013) Encoder-labeler Deep LSTM 95.66% NA (Kurata et al., 2016) Joint GRU Model (W) 95.49% 98.10% (Zhang and Wang, 2016) Attention Encoder-Decoder NN 95.87% 98.43% (Liu and Lane, 2016a) Attention BiRNN 95.98% 98.21% (Liu and Lane, 2016a) Bi-model without a decoder 96.65% 98.76% Bi-model with a decoder 96.89% 98.99%",
                "metadata": {
                    "filename": "1812.10235.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Performance Comparison between Bi-model Structures and Attention BiRNN\nDomain SLU model Size F1 Accuracy Score Attention BiRNN 979 92.1% 92.86% Movie Bi-model without a 979 93.3% 94.89% decoder Bi-model with a decoder 979 93.8% 95.91% Attention BiRNN 983 92.3% 98.48% Food Bi-model without a 983 93.6% 98.98% decoder Bi-model with a decoder 983 95.8% 99.49% Attention BiRNN 689 96.5% 97.83% Home Bi-model without a 689 97.8% 98.55% decoder Bi-model with a decoder 689 98.2% 99.27%",
                "metadata": {
                    "filename": "1812.10235.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1912.08777.pdf": {
        "normalized_output": [
            {
                "Task": "Summarization",
                "Dataset": "XSum",
                "Metric": "ROGUE-1",
                "Result": "47.21"
            },
            {
                "Task": "Summarization",
                "Dataset": "XSum",
                "Metric": "ROGUE-2",
                "Result": "24.56"
            },
            {
                "Task": "Summarization",
                "Dataset": "XSum",
                "Metric": "ROGUE-L</s>",
                "Result": "39.25"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-1",
                "Result": "44.17"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-2",
                "Result": "21.47"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-L",
                "Result": "41.11"
            },
            {
                "Task": "Summarization",
                "Dataset": "Newsroom",
                "Metric": "ROGUE-1",
                "Result": "45.15"
            },
            {
                "Task": "Summarization",
                "Dataset": "Newsroom",
                "Metric": "ROGUE-2",
                "Result": "33.51"
            },
            {
                "Task": "Summarization",
                "Dataset": "Newsroom",
                "Metric": "ROGUE-L",
                "Result": "41.33"
            },
            {
                "Task": "Summarization",
                "Dataset": "Multi-News",
                "Metric": "ROGUE-1",
                "Result": "47.52"
            },
            {
                "Task": "Summarization",
                "Dataset": "Multi-News",
                "Metric": "ROGUE-2",
                "Result": "18.72"
            },
            {
                "Task": "Summarization",
                "Dataset": "Multi-News",
                "Metric": "ROGUE-L",
                "Result": "24.91"
            },
            {
                "Task": "Summarization",
                "Dataset": "Gigaword",
                "Metric": "ROGUE-1",
                "Result": "39.12"
            },
            {
                "Task": "Summarization",
                "Dataset": "Gigaword",
                "Metric": "ROGUE-2",
                "Result": "19.86"
            },
            {
                "Task": "Summarization",
                "Dataset": "Gigaword",
                "Metric": "ROGUE-L",
                "Result": "36.24"
            },
            {
                "Task": "Summarization",
                "Dataset": "WikiHow",
                "Metric": "ROGUE-1",
                "Result": "41.35"
            },
            {
                "Task": "Summarization",
                "Dataset": "WikiHow",
                "Metric": "ROGUE-2",
                "Result": "18.51"
            },
            {
                "Task": "Summarization",
                "Dataset": "WikiHow",
                "Metric": "ROGUE-L",
                "Result": "33.42"
            },
            {
                "Task": "Summarization",
                "Dataset": "Reddit TIFU",
                "Metric": "ROGUE-1",
                "Result": "26.63"
            },
            {
                "Task": "Summarization",
                "Dataset": "Reddit TIFU",
                "Metric": "ROGUE-2",
                "Result": "9.01"
            },
            {
                "Task": "Summarization",
                "Dataset": "Reddit TIFU",
                "Metric": "ROGUE-L",
                "Result": "21.60"
            },
            {
                "Task": "Summarization",
                "Dataset": "BIGPATENT",
                "Metric": "ROGUE-1",
                "Result": "53.41"
            },
            {
                "Task": "Summarization",
                "Dataset": "BIGPATENT",
                "Metric": "ROGUE-2",
                "Result": "32.89"
            },
            {
                "Task": "Summarization",
                "Dataset": "BIGPATENT",
                "Metric": "ROGUE-L",
                "Result": "42.07"
            },
            {
                "Task": "Summarization",
                "Dataset": "arXiv",
                "Metric": "ROGUE-1",
                "Result": "44.67"
            },
            {
                "Task": "Summarization",
                "Dataset": "arXiv",
                "Metric": "ROGUE-2",
                "Result": "17.18"
            },
            {
                "Task": "Summarization",
                "Dataset": "arXiv",
                "Metric": "ROGUE-L",
                "Result": "25.73"
            },
            {
                "Task": "Summarization",
                "Dataset": "PubMed",
                "Metric": "ROGUE-1",
                "Result": "45.09"
            },
            {
                "Task": "Summarization",
                "Dataset": "PubMed",
                "Metric": "ROGUE-2",
                "Result": "19.56"
            }
        ],
        "source_documents": [
            {
                "content": "F Human Evaluation Details In all human evaluation experiments we used the same task template shown in Figure F.1, where workers were asked to rate 4 summaries for a document on a scale of 1 (poor summary) to 5 (great summary). The order in which the summaries are presented for each task was random per example. Each task was independently done by 3 different workers and we retained the median score across workers for each summary. We paid 1 USD per task and used the following critieria for workers to ensure high-quality: With this criteria we observed high reproducibility in the conclusions of the huamn evaluation. Multiple runs of the same experiment with different workers meeting this criteria yielded very similar results. The HITT template is provided at https://github.com/google-research/pegasus. In experiment 1, the four summaries corresponded to 3 models (PEGASUSLARGE pre-trained on HugeNews, C4, and TransformerBASE) that were \ufb01ne-tuned using all the supervised examples along with the reference (human) summary. We sampled 100 examples from each dataset (XSum, CNN/DailyMail, Reddit TIFU). In experiment 2, we evaluated 4 models (PEGASUSLARGE pre-trained on HugeNews \ufb01ne-tuned using different amounts of supervision, 10, 100, 1000, and all examples) alongside the human summary. To do this with the same template, for each example we randomly selected 4 out of the 5 summaries. This resulted in fewer ratings per model, but did not increase the work (and cost) of the task. We used a paired t-test to determine statistical signi\ufb01cance when comparing the ratings of two sets of summaries. Figure F.1: A screenshot of the Amazon MTurk HIIT.",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 17,
                    "start_index": 0,
                    "type": "Text"
                }
            },
            {
                "content": "Summarization, pp. 59\u201363, Copenhagen, Denmark, September 2017. Association for Computational Lin- guistics. doi: 10.18653/v1/W17-4508. URL https: //www.aclweb.org/anthology/W17-4508.",
                "metadata": {
                    "start_index": 0,
                    "filename": "1912.08777.pdf",
                    "page_number": 12,
                    "type": "Text"
                }
            },
            {
                "content": "Table I.27: Generated summaries by PEGASUSLARGE (HugeNews) on arXiv sampled by ROUGEL-F1. analysis of the hits , can yield a good determination of the intrinsic spatial precision and detection ef\ufb01ciency of the pixel plane under test . results from the 2002 beam test at the cern sps , where the sensor thickness was the same as the one used in ladder production , are presented and compared ROUGEL-F1",
                "metadata": {
                    "page_number": 46,
                    "type": "Text",
                    "filename": "1912.08777.pdf",
                    "start_index": 0
                }
            },
            {
                "content": "D Experiment Figures\u2019 Numbers Table D.1: The raw ROUGE1-F1, ROUGE2-F1 and ROUGEL-F1 scores reported in corresponding \ufb01gures. Figure[3] ROUGE scores reported in Figure 3 CNN/DailyMail WikiHow XSum Reddit TIFU R1/R2/RL R1/R2/RL R1/R2/RL R1/R2/RL No pretraining",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "type": "Text",
                    "start_index": 0,
                    "page_number": 15
                }
            },
            {
                "content": "Table 1: Results of PEGASUSLARGE and PEGASUSBASE on all downstream datasets compared with the previous SOTA, which are fetched from (Lewis et al., 2019; Shi et al., 2019; Fabbri et al., 2019; Koupaee & Wang, 2018; Kim et al., 2019; Subramanian et al., 2019; Song et al., 2019; Zhang & Tetreault, 2019; Kornilova & Eidelman, 2019). We only compared with previous abstractive models except on BillSum which had extractive results only. BIGPATENT, arXiv, PubMed and Multi-News datasets contain very long summaries and we truncate them to 256 tokens, in similar range compared to (Sharma et al., 2019; Cohan et al., 2018; Fabbri et al., 2019; Goodman et al., 2019). Best ROUGE numbers on each dataset and numbers within 0.15 of the best numbers are bolded.\nR1/R2/RL Dataset size TransformerBASE PEGASUSBASE Previous SOTA PEGASUSLARGE (C4) PEGASUSLARGE (HugeNews) XSum 226k 30.83/10.83/24.41 39.79/16.58/31.70 45.14/22.27/37.25 45.20/22.06/36.99 47.21/24.56/39.25 CNN/DailyMail 311k 38.27/15.03/35.48 41.79/18.81/38.93 44.16/21.28/40.90 43.90/21.20/40.76 44.17/21.47/41.11 NEWSROOM 1212k 40.28/27.93/36.52 42.38/30.06/38.52 39.91/28.38/36.87 45.07/33.39/41.28 45.15/33.51/41.33 Multi-News 56k 34.36/5.42/15.75 42.24/13.27/21.44 43.47/14.89/17.41 46.74/17.95/24.26 47.52/18.72/24.91 Gigaword 3995k 35.70/16.75/32.83 36.91/17.66/34.08 39.14/19.92/36.57 38.75/19.96/36.14 39.12/19.86/36.24 WikiHow 168k 32.48/10.53/23.86 36.58/15.64/30.01 28.53/9.23/26.54 43.06/19.71/34.80 41.35/18.51/33.42 Reddit TIFU 42k 15.89/1.94/12.22 24.36/6.09/18.75 19.0/3.7/15.1 26.54/8.94/21.64 26.63/9.01/21.60 BIGPATENT 1341k 42.98/20.51/31.87 43.55/20.43/31.80 37.52/10.63/22.79 53.63/33.16/42.25 53.41/32.89/42.07 arXiv 215k 35.63/7.95/20.00 34.81/10.16/22.50 41.59/14.26/23.55 44.70/17.27/25.80 44.67/17.18/25.73 PubMed 133k 33.94/7.43/19.02 39.98/15.15/25.23 40.59/15.59/23.59 45.49/19.90/27.69 45.09/19.56/27.42 AESLC 18k 15.04/7.39/14.93 34.85/18.94/34.10 23.67/10.29/23.44 37.69/21.85/36.84 37.40/21.22/36.45 BillSum 24k 44.05/21.30/30.98 51.42/29.68/37.78 40.80/23.83/33.73 57.20/39.56/45.80 57.31/40.19/45.82",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: A comparison of PEGASUSLARGE with other pretrained models on XSum, CNN/DailyMail and Gigaword. Best ROUGE numbers and numbers within 0.15 of the best numbers are bolded.\nR1/R2/RL XSum CNN/DailyMail Gigaword BERTShare (Rothe et al., 2019) 38.52/16.12/31.13 39.25/18.09/36.45 38.13/19.81/35.62 MASS (Song et al., 2019) 39.75/17.24/31.95 42.12/19.50/39.01 38.73/19.71/35.96 UniLM (Dong et al., 2019) - 43.33/20.21/40.51 38.45/19.45/35.75 BART (Lewis et al., 2019) 45.14/22.27/37.25 44.16/21.28/40.90 - T5 (Raffel et al., 2019) - 43.52/21.55/40.69 - PEGASUSLARGE (C4) 45.20/22.06/36.99 43.90/21.20/40.76 38.75/19.96/36.14 PEGASUSLARGE (HugeNews) 47.21/24.56/39.25 44.17/21.47/41.11 39.12/19.86/36.24",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Human evaluation side-by-side results on Likert (1-5) scale (higher is better). Scores are bolded if they are not worse than human-level performance by p < 0.01.\nDatasets XSum CNN/DailyMail Reddit TIFU mean (p-value) mean (p-value) mean (p-value) Experiment 1: pretrain comparison Human-written 3.0 (-) 3.1 (-) 3.2 (-) PEGASUSLARGE (HugeNews) PEGASUSLARGE (C4) TransformerBASE 3.0 (0.6) 3.1 (0.7) 2.0 (3e-10) 3.6 (0.0001) 3.5 (0.009) 2.9 (0.06) 3.2 (0.7) 3.1 (0.3) 1.4 (5e-23) Experiment 2: low resource Human-written 3.2 (-) 3.2(-) 3.3 (-) PEGASUSLARGE (HugeNews) 10 examples 2.8 (0.1) 3.4 (0.007) 2.6 (0.006) PEGASUSLARGE (HugeNews) 100 examples PEGASUSLARGE (HugeNews) 1000 examples PEGASUSLARGE (HugeNews) full supervision 3.2 (0.5) 3.4 (0.3) 3.4 (0.3) 3.4 (0.08) 3.6 (0.07) 3.3 (0.1) 2.1 (4e-8) 2.7 (0.01) 2.8 (0.05)",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: Results (ROUGE-1/ROUGE-2/ROUGE-L F scores) of PEGASUSLARGE (mixed, stochastic) on down- stream datasets. \u2021 We updated the BIGPATENT dataset to preserve casing, some format cleanings are also changed.\nXSum CNN/DailyMail NEWSROOM 47.60/24.83/39.64 44.16/21.56/41.30 45.98/34.20/42.18 Multi-News Gigaword WikiHow 47.65/18.75/24.95 39.65/20.47/36.76 46.39/22.12/38.41 Reddit TIFU BIGPATENT arXiv 27.99/9.81/22.94 52.29/33.08/41.66 \u2021 44.21/16.95/25.67 PubMed AESLC BillSum 45.97/20.15/28.25 37.68/21.25/36.51 59.67/41.58/47.59",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 9,
                    "type": "Table"
                }
            },
            {
                "content": "Model Learning rate Label smoothing Num of steps Batch size Objective Corpus Max input tokens Max target tokens PEGASUSBASE 0.1 0.0 500k 256 Ind-Orig c4 512 256 PEGASUSLARGE 0.1 0.0 500k 8192 Ind-Orig c4 or HugeNews 512 256 Fine-tuning of PEGASUSBASE in Figure 3, 4, 5, B.1 and Table 1 Dataset Learning rate Label smoothing Num of steps Batch size Beam size Beam alpha Max input tokens Max target tokens XSum 5e-4 0.1 50k 256 1 - 512 64 CNN/DailyMail 5e-4 0.1 50k 256 1 - 512 128 NEWSROOM 5e-4 0.1 50k 256 1 - 512 128 Multi-News 5e-4 0.1 50k 256 1 - 512 256 WikiHow 5e-4 0.1 50k 256 1 - 512 256 Reddit TIFU 5e-4 0.1 50k 256 1 - 512 128 BIGPATENT 0.01 0.1 300k 256 1 - 512 256 arXiv 5e-4 0.1 50k 256 1 - 512 256 PubMed 5e-4 0.1 50k 256 1 - 512 256 Gigaword 5e-4 0.1 50k 256 1 - 128 32 AESLC 5e-4 0.1 50k 256 1 - 512 32 BillSum 5e-4 0.1 50k 256 1 - 512 256 TransformerBASE in Table 1 Dataset Learning rate Label smoothing Num of steps Batch size Beam size Beam alpha Max input tokens Max target tokens BIGPATENT 0.01 0.1 300k 256 1 - 512 256 AESLC 5e-4 0.1 300k 256 1 - 512 32 Others 5e-3 0.1 300k 256 1 - Same as PEGASUSBASE Fine-tuning of PEGASUSLARGE in Table 1 and 2 Dataset Learning rate Label smoothing Num of steps Batch size Beam size Beam alpha Max input tokens Max target tokens XSum(C4) 1e-4 0.1 130k 256 8 0.8 512 64 XSum(HugeNews) 1e-4 0.1 80k 256 8 0.8 512 64 CNN/DailyMail(C4) 5e-5 0.1 220k 256 8 0.8 1024 128 CNN/DailyMail(HugeNews) 5e-5 0.1 170k 256 8 0.9 1024 128 NEWSROOM 4e-4 0.1 104k 256 8 0.8 512 128 Multi-News 5e-5 0.1 80k 256 8 0.9 1024 256 WikiHow 8e-4 0.1 50k 256 8 0.6 512 256 Reddit TIFU 1e-4 0.1 12k 256 8 0.6 512 128 BIGPATENT 5e-3 0.1 300k 256 8 0.7 1024 256 arXiv 8e-4 0.1 74k 256 8 0.8 1024 256 PubMed 2e-4 0.1 100k 256 8 0.8 1024 256 Gigaword 8e-4 0.1 90k 256 8 0.6 128 32 AESLC 2e-4 0.1 16k 256 8 0.6 512 32 BillSum 2e-4 0.1 100k 256 8 0.8 1024 256 Fine-tuning of PEGASUSLARGE in Figure 6 Dataset Learning rate Label smoothing Num of steps Batch size Beam size Beam alpha Max input tokens Max target tokens all 5e-4 0.1 2k 256 1 - Same as PEGASUSBASE",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 14,
                    "type": "Table"
                }
            },
            {
                "content": "XSum CNN/DailyMail WikiHow Reddit TIFU R1/R2/RL R1/R2/RL R1/R2/RL R1/R2/RL Pre-trained on c4 39.79/16.58/31.70 41.79/18.81/38.93 36.58/15.64/30.01 24.36/6.09/18.75 Pre-trained on HugeNews 41.63/18.47/33.48 42.34/19.22/39.49 34.93/14.67/28.63 24.11/5.99/18.57 ROUGE scores reported in Figure 4a XSum CNN/DailyMail WikiHow Reddit TIFU R1/R2/RL R1/R2/RL R1/R2/RL R1/R2/RL Random 39.28/16.23/31.21 41.80/18.91/38.88 36.27/15.47/29.67 24.04/6.01/18.47 Lead 39.22/16.12/31.09 41.70/18.78/38.85 35.30/14.79/28.85 23.48/5.78/18.00 Ind-Orig 39.79/16.58/31.70 41.79/18.81/38.93 36.58/15.64/30.01 24.36/6.09/18.75 Ind-Uniq 39.50/16.41/31.41 41.79/18.83/38.94 36.26/15.47/29.69 24.10/5.98/18.41 Seq-Orig 39.22/16.27/31.11 41.88/18.89/39.02 36.39/15.57/29.74 24.09/6.15/18.55 Seq-Uniq 39.50/16.39/31.40 41.98/19.03/39.11 36.69/15.61/29.95 24.25/6.17/18.67 MLM solely 37.22/14.48/29.62 39.33/17.34/36.65 32.20/13.19/27.05 21.00/3.96/16.27 MLM & Ind-Orig 39.08/16.21/31.20 41.48/18.70/38.63 35.99/15.29/29.57 24.19/6.16/18.70 ROUGE scores reported in Figure 4b XSum CNN/DailyMail WikiHow Reddit TIFU R1/R2/RL R1/R2/RL R1/R2/RL R1/R2/RL 15% 39.47/16.32/31.30 41.88/18.98/38.97 35.63/15.08/29.23 24.06/5.91/18.52 30% 39.61/16.51/31.48 41.83/18.82/38.96 36.26/15.47/29.69 24.05/6.05/18.55 45% 39.43/16.42/31.36 41.57/18.67/38.69 36.39/15.46/29.85 23.47/5.61/18.01 50% 39.19/16.20/31.16 41.49/18.60/38.64 36.15/15.36/29.56 23.92/5.83/18.33 60% 39.06/16.08/31.08 41.27/18.40/38.42 36.04/15.34/29.47 23.14/5.50/17.74 75% 36.94/14.21/29.14 40.17/17.52/37.37 34.32/13.72/27.96 21.72/4.32/16.45 ROUGE scores reported in Figure 5 XSum CNN/DailyMail WikiHow Reddit TIFU R1/R2/RL R1/R2/RL R1/R2/RL R1/R2/RL BPE 32k 39.23/16.17/31.13 41.86/18.97/38.97 35.22/14.88/28.87 24.04/6.04/18.57 Unigram 32k 38.94/15.99/30.97 41.75/19.08/38.91 36.94/15.68/30.28 24.17/6.07/18.54 Unigram 64k 39.17/16.33/31.24 41.89/19.19/39.03 37.58/16.02/30.71 24.47/6.32/18.90 Unigram 96k 39.33/16.40/31.24 42.22/19.31/39.34 37.38/15.94/30.63 24.10/6.22/18.73 Unigram 128k 39.26/16.27/31.14 41.76/19.08/38.89 37.66/16.04/30.83 23.74/5.95/18.33 Unigram 256k 38.55/15.92/30.62 41.98/19.11/39.08 36.94/15.49/30.08 23.63/5.95/18.33 ROUGE scores reported in Figure B.1 XSum CNN/DailyMail WikiHow Reddit TIFU R1/R2/RL R1/R2/RL R1/R2/RL R1/R2/RL No pretraining 30.83/10.83/24.41 38.27/15.03/35.48 32.48/10.53/23.86 15.89/1.94/12.22",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 15,
                    "type": "Table"
                }
            },
            {
                "content": "XSum CNN/DailyMail WikiHow Reddit TIFU R1/R2/RL R1/R2/RL R1/R2/RL R1/R2/RL No pretraining 30.83/10.83/24.41 38.27/15.03/35.48 32.48/10.53/23.86 100k-step 37.68/14.89/29.78 40.83/18.24/37.99 34.01/14.07/28.13 200k-step 38.72/15.74/30.74 41.40/18.53/38.57 34.91/14.64/28.70 300k-step 39.15/16.12/31.05 41.63/18.79/38.76 35.61/15.09/29.22 400k-step 39.45/16.34/31.37 41.81/18.89/38.95 36.14/15.41/29.64 500k-step 39.79/16.58/31.70 41.79/18.81/38.93 36.58/15.64/30.01",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 15,
                    "type": "Table"
                }
            },
            {
                "content": "Table E.1: The ROUGE1-F1, ROUGE2-F1 and ROUGEL-F1 scores of low resource summarization reported in Figure 6 along with previous SOTA in Table 1. With 100 examples, PEGASUSLARGE beats previous SOTA on ROUGE2-F1 metrics on BIGPATENT, Reddit TIFU, and BillSum dataset. With 1000 examples, PEGASUSLARGE beats previous SOTA metrics on Multi-News, WikiHow, Reddit TIFU, BigPatent, AESLC and BillSum.\nDataset 0 examples 10 examples 100 examples 1k examples 10k examples previous SOTA R1/R2/RL R1/R2/RL R1/R2/RL R1/R2/RL R1/R2/RL R1/R2/RL XSum 19.27/3.00/12.72 19.39/3.45/14.02 39.07/16.44/31.27 41.55/18.23/33.29 44.71/21.20/36.31 45.14/22.27/37.25 CNN/DailyMail 32.90/13.28/29.38 37.25/15.84/33.49 40.28/18.21/37.03 41.72/19.35/38.31 42.54/20.04/39.32 44.16/21.28/40.90 NEWSROOM 22.06/11.86/17.76 29.24/17.78/24.98 33.63/21.81/29.64 37.26/25.34/33.12 39.54/27.25/35.45 39.91/28.38/36.87 Multi-News 36.54/10.52/18.67 39.79/12.56/20.06 41.04/13.88/21.52 44.00/15.45/22.67 44.70/16.57/23.43 43.47/14.89/17.41 Gigaword 23.39/7.59/20.20 25.32/8.88/22.55 29.71/12.44/27.30 32.95/13.90/30.10 35.13/16.36/32.61 38.73/19.71/35.96 WikiHow 22.59/6.10/14.44 23.95/6.54/15.33 25.24/7.52/17.79 34.35/12.17/25.84 37.22/14.41/29.15 28.53/9.23/26.54 Reddit TIFU 14.66/3.06/10.17 15.36/2.91/10.76 16.64/4.09/12.92 23.34/6.85/18.46 25.47/8.18/20.33 19.0/3.7/15.1 BIGPATENT 25.61/6.56/17.42 28.87/8.30/19.71 33.52/10.82/22.87 36.85/12.58/24.54 34.81/12.39/24.13 37.52/10.63/22.79 arXiv 28.05/6.63/17.72 31.38/8.16/17.97 33.06/9.66/20.11 39.46/12.38/22.20 40.24/14.04/23.11 41.59/14.26/23.55 PubMed 28.17/7.57/17.85 33.31/10.58/20.05 34.05/12.75/21.12 40.15/15.56/24.05 41.75/16.74/24.80 40.59/15.59/23.59 AESLC 10.35/3.86/9.29 11.97/4.91/10.84 16.05/7.20/15.32 28.58/15.45/28.14 36.47/20.85/35.53 23.67/10.29/23.44 BillSum 41.02/17.44/25.24 40.48/18.49/27.27 44.78/26.40/34.40 46.47/30.58/37.21 50.81/34.49/40.96 40.80/23.83/33.73",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 16,
                    "type": "Table"
                }
            },
            {
                "content": "Document (ID #187) GP practices are being paid to help local NHS groups limit the number of patient referrals and cut costs, the doctors\u2019 magazine Pulse found. Appointments affected include scans and consultations with specialists - including those for cancer patients. The British Medical Association said such incentives were \u201dmisguided\u201d. At least nine clinical commissioning groups (CCGs) were offering GP practices payments for hitting targets, according to Pulse\u2019s investigation. In one case, Birmingham South Central CCG was offering practices more than 11,000 to reduce new outpatient attendances, follow-ups, A&E attendances and emergency admissions by 1%, compared with 2014/15. It said the schemes, which exclude cancer referrals, were designed to \u201dincentivise best quality practice\u201d and \u201ddrive improvements in the quality of primary medical care\u201d. \u201dOur priority is to ensure that patients have access to services that they need, when they need them,\u201d said a spokesperson for Birmingham South Central CCG. Another CCG told Pulse it had considered the \u201dfull impact\u201d of the incentive scheme and was \u201dcon\ufb01dent that there is no con\ufb02ict of interest\u201d. Pulse said that one scheme had already been looked at by the General Medical Council, the body which regulates medical standards in the UK, after local GP leaders expressed their concern. The magazine pointed out that initial hospital referrals for cancer patients should happen within two weeks of a GP \ufb01rst suspecting the condition. Dr Chand Nagpaul, chairman of the GPs committee of the doctors\u2019 trade union the British Medical Association, told BBC Radio 4\u2019s Today programme that such schemes were a \u201d\ufb01nancial contaminant\u201d to patient-doctor trust. He said: \u201dIt\u2019s short-sighted and misguided of CCGs to introduce such mechanisms, because they do lead to the potential for patients questioning the motives of GP referrals. \u201dWe believe it is far more appropriate for CCGs to introduce clinical pathways that ... ... ... Gold Some doctors in England are being offered thousands of pounds to cut the number of patients being sent to hospital, an investigation has found. Model Thousands of pounds are being paid to GPs in England to avoid sending patients to hospital, an investigation has found. ROUGE1-F1 68.18 Document (ID #206) The striker took his tally to \ufb01ve goals in two games when hitting the opener and winner in an ABAX Stadium thriller. Marriott \ufb01red Posh ahead two minutes into the second half with a low shot that crept in courtesy of a kiss off the inside of the far post. But Rotherham were soon back on level terms as Kieffer Moore, who like Marriott hit a hat-trick in his previous league appearance, headed in captain Lee Frecklington\u2019s left-wing cross. Marriott then steered a Marcus Maddison cross against a post to be denied by the woodwork for the second time after seeing an early effort hit the underside of the crossbar. Rotherham thought they had hit the front moments later only to see their celebrations cut short by a raised \ufb02ag when Jamie Proctor nodded in from close range. But a cracking contest was settled with 15 minutes to go when Marriott got the better of dithering defender Semi Ajayi and lobbed a glorious \ufb01nish over stranded Rotherham goalkeeper Richard O\u2019Donnell. Match report supplied by the Press Association. Match ends, Peterborough United 2, Rotherham United 1. Second Half ends, Peterborough United 2, Rotherham United 1. Corner, Rotherham United. Conceded by Michael Doughty. Kieffer Moore (Rotherham United) is shown the yellow card for hand ball. Corner, Rotherham United. Conceded by Michael Doughty. Attempt missed. Joe Newell (Rotherham United) right footed shot from a dif\ufb01cult angle on the left is close, but misses to the left. Attempt saved. Andrew Hughes (Peterborough United) left footed shot from outside the box is saved in the top left corner. Joe Mattock (Rotherham United) is shown the yellow card for a bad foul. Idris Kanu (Peterborough United) wins a free kick in the defensive half. Foul by Joe Mattock (Rotherham United). Attempt missed. Jack Marriott (Peterborough United) ... ... ... Gold Jack Marriott hit a second-half double as Peterborough continued their perfect League One start by beating Rotherham. Model Jack Marriott continued his goalscoring form in League One with a brace as Peterborough beat Rotherham 2-1. ROUGE1-F1 50.00 Document (ID #276) Thieves took the animal from Silver Star Pets in Pengam, along with dog leads, dog coats, \ufb01sh tanks, pet food and dog grooming equipment worth 35,000. The incident is one of four break-ins that occurred at the Britannia Enterprise Centre some time between Tuesday evening and Wednesday morning. Police are appealing for information. The other units broken into were Auto Stitch, George Monumental Masons and Pride and Groom where damage was caused to the doors but nothing was stolen. A taxi sign, however, was also stolen from one of the vehicles parked at the centre. Gold An African Grey parrot valued at 900 has been stolen from a pet shop in Caerphilly. Model A Yorkshire terrier worth 100,000 has been stolen from a pet shop in Stoke-on-Trent. ROUGE1-F1 48.48 Document (ID #70) Matthew Gillard, of Connsbrook Avenue in east Belfast, pleaded not guilty to charges including kidnapping and false imprisonment. The defendant, 25, also denied charges of common assault, assaulting a police of\ufb01cer and driving dangerously. The charges relate to incidents in east Belfast and Comber on Saturday 4 April. Newtownards Magistrates Court heard that the defendant and the woman were in his car in east Belfast when he began questioning her about who she was seeing. A detective giving evidence in court said the woman tried to get out of the Seat Toledo car but the defendant allegedly drove off at speed, through a red light on Bloom\ufb01eld Avenue. When the car stopped on the Belfast Road in Comber, the woman escaped along a lane but was carried back to the car by the defendant, the court heard. An off-duty police sergeant saw this and went to help the woman. When the sergeant tried to intervene, the defendant allegedly drove his car at the of\ufb01cer, forcing him to get out of the way. The sergeant was able to pull the keys from the ignition through the car\u2019s open window, but the defendant wrenched them from his grasp. He then drove to the Grand Parade area of east Belfast where the woman was released. He is also alleged to have sent the woman a message threatening that if she went to police about the incident he would \u201druin her life in every possible way\u201d. The court heard that the defendant handed himself over to police on Wednesday, despite being aware since Saturday that he was wanted by the PSNI. But during police interviews he refused to answer questions put to him. An application for bail was made but this was refused. Mr Gillard will appear again in court on 1 May. Gold A man accused of kidnapping his partner allegedly drove at a police of\ufb01cer who tried to save her, a court has heard. Model A man has appeared in court charged with kidnapping and assaulting a woman. ROUGE1-F1 34.29 Document (ID #124) Johnson made his \ufb01lm debut in the 1950s and featured in numerous \ufb01lms, alongside stars such as Frank Sinatra, Laurence Olivier and Charlton Heston. A founder member of the Royal Shakespeare Company, Johnson played several lead roles including Romeo and Mark Anthony in Julius Caesar. He also appeared in several TV dramas such as Lewis and Silent Witness. Johnson died after a short illness at the Royal Marsden Hospital in Chelsea, London. He is survived by his wife Lynne, who he married in 2004, and his four children. Johnson was born in Upminster, Essex, and he left his training at the Royal Academy of Dramatic Art (RADA) to join Sir John Gielgud\u2019s company. He joined the Royal Navy during World War Two and then made his \ufb01lm debut in 1959, when he appeared in the MGM \ufb01lm Never So Few, starring Frank Sinatra and Gina Lollobrigida. He also appeared in The Haunting (1963) and Khartoum (1966), opposite Laurence Olivier and Charlton Heston. His family said he was offered and turned down the role of James Bond after playing British spy Bulldog Drummond in Deadlier Than the Male (1967) and its sequel Some Girls Do (1969).",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 20,
                    "type": "Table"
                }
            },
            {
                "content": "(ID #255) The 34-year-old has been absent from the club since a training-ground row after the 5-1 Scottish Premiership loss at Celtic on 10 September. Alongside agent Eddy Jennings, he attended a meeting with club of\ufb01cials last week. Barton, who joined from Burnley in May, has repeatedly said he wants to return and \ufb01ght for his Rangers place. \u201dJoey Barton has been told he will return to full-time training following the conclusion of a disciplinary procedure,\u201d read a statement on Rangers\u2019 website. \u201dThe Auchenhowie [training base] management team will inform the player of appropriate training arrangements to which he must adhere for as long as he remains a Rangers player. \u201dNeither the club, nor the player, will comment further.\u201d Barton is also facing Scottish FA charges for breaching betting rules. He was charged with placing 44 bets on games between 1 July and 15 September. The former Manchester City, Newcastle United and QPR player, who has one England cap, left Burnley under freedom of contract at the end of last season, signing a two-year contract at Ibrox. He has made eight appearances for Mark Warburton\u2019s side, who are second in the Premiership. Barton was not available for comment but said earlier on Wednesday on Twitter that it was \u201da lovely day for a Iceman Hof session and a run!\u201d in reference to Dutch ice bath training guru Wim Hof. BBC Scotland\u2019s Richard Wilson The fact that Joey Barton is returning to full-time training does not necessarily mean the resumption of his Rangers career. Under Fifa rules, a player under contract at a club must be provided with training facilities. The fact that the player was suspended re\ufb02ects the view of the management team about the serious nature of a training-ground row in September. Any return to the \ufb01rst-team squad would need to be ... ... ... Gold Rangers say mid\ufb01elder Joey Barton \u201dwill return to full-time training\u201d following a club-imposed suspension. Model Rangers mid\ufb01elder Joey Barton is to return to full-time training after being suspended by the club. ROUGE2-F1 38.71 Document (ID #7) Operation Anagram was formed by Strathclyde Police in 2006 after Tobin raped and murdered 23-year-old Polish student Angelika Kluk in Glasgow. It also helped convict him of the murders of 18-year-old Dinah McNicol and 15-year-old Vicky Hamilton. Police said that while the operation was being scaled back it would never end. Det Sup David Swindle, who formed the operation, said anyone with information regarding Peter Tobin would still be able to contact the Anagram incident room via e-mail which would be monitored daily. \u201dSince Operation Anagram was formed in 2006, I could never have believed the momentum and magnitude it would have developed into,\u201d he said. \u201dThe public response and assistance from the media has been unbelievable. \u201dThe long-running investigation has brought some solace to the families of Vicky Hamilton and Dinah McNicol in knowing what happened to their loved ones.\u201d Det Sup Swindle said that although Anagram had not uncovered other murder victims of Tobin to date, he was \u201dproud\u201d of its \u201dsuccess in bringing closure\u201d to some bereaved families. He added: \u201dI leave Strathclyde Police after 34 years in the knowledge that we have tried our best to \ufb01nd out the truth of what Tobin has done and also with con\ufb01dence that the Anagram processes built up over the last four and a half years ensures a life long awareness of this cowardly, vicious serial killer Tobin. \u201dAny new information coming to light will continue to be researched which may bring similar closure to other families. \u201dThe search for the truth about what Peter Tobin has done will continue.\u201d Tobin is serving three life sentences for the murders of Vicky Hamilton, Dinah McNicol and Angelika Kluk. Ms Kluk was raped and murdered at a church in Glasgow in 2006. The bodies of Ms McNicol, from Essex, and Ms ... ... ... Gold A dedicated police operation set up to examine the activities of serial killer Peter Tobin is to be wound down. Model A police investigation into serial killer Peter Tobin is being scaled back. ROUGE2-F1 26.67 Document (ID #267) Norah Boyle, 85, suffered a head injury as Sabrina Duncan and Benter Ouma put her to bed at The Green Nursing Home in Kings Norton. She died 23 days later. The workers delayed calling 999 and pretended Mrs Boyle knocked her head on the head board. They must do 160 hours\u2019 unpaid work and pay 500 court costs each. The pair were not prosecuted for being responsible for the fall, but for being \u201dgrossly negligent in their response\u201d. Mrs Boyle died in hospital after developing pneumonia. In sentencing, judge Mark Wall said it was impossible to say whether reporting the incident any earlier would have made any difference to Mrs Boyle\u2019s eventual death. Her daughter, Ellen Boyle, said: \u201dI\u2019m appalled that that\u2019s what they got for what happened to my mum. \u201dI\u2019m appalled that my mother\u2019s life is only worth 12 months of a community order.\u201d Speaking after the court hearing Det Sgt Victoria Lee said the pair had delayed calling an ambulance while they came up with a cover story. \u201dWhile [Duncan and Ouma] plotted, Mrs Boyle laid in bed with a serious head injury, her head bleeding onto the pillow,\u201d she said. \u201dMost of us have relatives who are frail, disabled or vulnerable we expect them to be cared for professionally and compassionately.\u201d Duncan, 40, of Shartlands Close, Cotteridge, and Ouma, 31, of Summer\ufb01eld Crescent, Edgbaston, had pleaded guilty at an earlier hearing at Birmingham Crown Court to neglect and ill-treatment. Gold Two care workers who admitted neglect after a pensioner fell from a hoist at a Birmingham care home have been sentenced to 12-month community orders. Model Two care home workers who admitted covering up the death of a woman who fell in her bed have been given community orders. ROUGE2-F1 25.53 Document (ID #234) She will play Denker, a lady\u2019s maid to Dame Maggie Smith\u2019s character, the Dowager Countess of Grantham. Johnston, who has also appeared in Waking the Dead and Coronation Street, joins new stars Richard E Grant and Anna Chancellor, both of whom will play guests of the Granthams at Downton. The hit period drama will return to screens this autumn. Series four of the show, which followed the wealthy Grantham family and their servants, achieved an average of 11.9 million viewers in the UK. The very British drama has also been a huge hit in the US, winning both Emmy Awards and Golden Globes. More than 26 million viewers watched series four on Masterpiece on PBS, making it one of the highest rating shows on American television. Previous high pro\ufb01le guest stars include Shirley Maclaine who played Martha Levinson, Lady Grantham\u2019s mother, and Oscar-nominated actor Paul Giamatti who appeared in last year\u2019s Christmas special as her \u201dmaverick, playboy\u201d son. Series \ufb01ve will also feature 24 star Rade Sherbedgia as a Russian refugee who has \ufb02ed the revolution after World War 1. Earlier this year, executive producer Gareth Neame promised it would have \u201dall the usual highs and lows, romance, drama and comedy\u201d. Gold The Royle Family actress Sue Johnston is the latest star to join series \ufb01ve of ITV\u2019s Downton Abbey in a guest role. Model Former Emmerdale actress Sian Johnston is to join the \ufb01fth series of Downton Abbey. ROUGE2-F1 17.14 Document (ID #70) Matthew Gillard, of Connsbrook Avenue in east Belfast, pleaded not guilty to charges including kidnapping and false imprisonment. The defendant, 25, also denied charges of common assault, assaulting a police of\ufb01cer and driving dangerously. The charges relate to incidents in east Belfast and Comber on Saturday 4 April. Newtownards Magistrates Court heard that the defendant and the woman were in his car in east Belfast when he",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 21,
                    "type": "Table"
                }
            },
            {
                "content": "Document (ID #198) Media playback is not supported on this device Craig Cathcart put the visitors ahead before substitute Simon Church won and scored an 89th-minute penalty. \u201dThere were lots of positives out of it even if we\u2019d have come off and lost 1-0. They had a good mentality and attitude,\u201d said Coleman. Wales face another Euro 2016 warm-up game against Ukraine in Kiev on Monday. \u201dWe look forward to our next challenge now,\u201d added Coleman. \u201dThe team will change up again, and we\u2019ll see how they go again.\u201d Striker Church, currently on loan at Scottish Premiership side Aberdeen from Reading, was delighted with his equaliser from the spot. \u201dNorthern Ireland were a tough side to play against. They\u2019ve obviously done well to get where they are and it was a tough game,\u201d he said. \u201dWe wanted to do well because it was the last time a Wales crowd would see us before the Euros and we wanted to put in a good performance. \u201dI\u2019ve just got to keep going now and hopefully score some goals. This is a great squad to be part of.\u201d Gold Wales manager Chris Coleman said he was pleased with his team\u2019s performance after they came from behind to draw 1-1 with Northern Ireland in Cardiff. Model Wales manager Chris Coleman praised his side\u2019s attitude after they came from behind to draw 1-1 with Northern Ireland in Cardiff. ROUGEL-F1 80.00 Document (ID #25) The Senegal international, 26, joined for 9m from Lille in July 2015 and played 35 times as Villa were relegated from the Premier League last season. Other interested clubs have until the end of July to make a bid for Gueye. \u201dIf he wants to go, we are powerless,\u201d club chairman Dr Tony Xia posted on Twitter. Gueye only missed three league games for Villa in 2015-16 and scored his only goal for the club in their FA Cup fourth round win against Wycombe in January. It is believed the fee that has activated the departure clause is about 7m, with Villa bracing themselves for further offers. Gold Aston Villa cannot stop mid\ufb01elder Idrissa Gueye leaving the club after Everton triggered a release clause in the player\u2019s contract. Model Aston Villa mid\ufb01elder Idrissa Gueye has triggered a clause in his contract that will allow him to leave the club this summer. ROUGEL-F1 46.51 Document (ID #279) The early work, carried out on mice and pigs, reveals the protein-infused patch encourages the growth of healthy cells and leads to less scarring. Scarring can be common after a heart attack, making the heart pump less effectively and sometimes fail. Writing in the journal Nature, researchers say the patch may one day revolutionise treatment. During an attack, muscle cells in the heart die because of a lack of blood \ufb02ow and scientists believe repairing or replacing some of these cells may help reduce long-term damage. In this trial an international team of researchers soaked a collagen patch in a protein known as Fstl1 and stitched it on to the hearts of animals who had experienced heart attacks. Though the protein occurs naturally in healthy hearts, it becomes depleted in a key layer of the heart after an attack. Two weeks later the hearts began to grow fresh muscle cells and new blood vessels, while showing signs of pumping more effectively. Prof Pilar Ruiz-Lozano at Stanford University (which has patented the patch), said: \u201dMany were so sick prior to getting the patch that they would have been candidates for heart transplantation. \u201dThe hope is that a similar procedure could eventually be used in human heart attack patients who suffer severe heart damage.\u201d Commenting on the study in Nature, Prof Gordana Vunjak-Novakovic at Columbia University, said the work \u201dcould lead to entirely new modalities for treating heart infarction\u201d. But she cautioned that further studies needed to be done to understand whether this type of approach would work on larger animals and ultimately humans. Gold A prototype patch could help the repair the damage caused by a heart attack, scientists say. Model Scientists say they have developed a synthetic patch that can repair damaged hearts after a heart attack. ROUGEL-F1 36.36 Document (ID #177) Around 155 countries are expected to formally sign the deal at the UN, setting in motion events that could see the treaty operational within a year. The UN says the expected record turnout for the signing shows overwhelming global support for tackling rising temperatures. But some environmentalists have dismissed the event as a \u201ddistraction\u201d. Despite the absence of President Obama, around 60 world leaders are expected here at UN headquarters, including French President Francois Hollande and Prime Minister Trudeau from Canada. But their signatures alone will not be enough to make the Paris agreement operational. The legal requirements mean that each country will have to go through a process of rati\ufb01cation. For some this will require nothing more than the assent of the political leader as in the example of the United States. Others though, such as India and Japan, will have to take the document to their parliaments; some may need new laws. The European Union is expected to lag behind on this issue as it has not yet agreed with the 28 member states on how emissions cuts will be shared out. Each member state will also have to ratify the deal individually. Some countries, including the Marshall Islands, Palau, Fiji and Switzerland, have already completed this step and will be able to formally join the agreement on April 22. To become operational, the treaty needs at least 55 countries representing at least 55% of global emissions to complete all the steps. While this is a tough threshold to reach an unusual coalition of interests is making it possible. Firstly President Obama is keen to ensure the deal is operational before his successor takes of\ufb01ce next January. If the next President wants to take the US out of an established treaty they will have to wait for four ... ... ... Gold The \ufb01rst signi\ufb01cant step to putting the Paris Climate Agreement into practice will take place on Friday. Model World leaders are gathering in New York to sign the Paris Agreement on climate change, despite US President Barack Obama not attending. ROUGEL-F1 25.64 Document (ID #186) There could be \u201dserious implications\u201d for Gloucestershire Police, the police and crime commissioner has warned. Cotswold District Council\u2019s proposal to form a unity authority with West Oxfordshire has proved controversial. But CDC says the plans - dubbed \u2019Coxit\u2019 - are at an early stage but aim to improve accountability. CDC leader Lynden Stowe has said Gloucestershire\u2019s \u201dhistoric borders\u201d would remain under the proposals, which aim to improve the \u201dadministration and the delivery of council services\u201d. He told BBC Radio Gloucestershire a few weeks ago: \u201dWe would expect the police to patrol up to the existing county borders, exactly as now, so if there\u2019s a crime in Cirencester, Gloucestershire Police attend and if there\u2019s a burglary in Burford, Thames Valley Police attend.\u201d But Gloucestershire\u2019s PCC Martin Surl said he had taken legal advice on the \u201dCoxit\u201d proposal, and warned it could \u201dsignal the end of Gloucestershire Police as we know it\u201d. Under the Local Government and Public Involvement in Health Act 2007, he said the Cotswolds could become part of Thames Valley Police District. \u201dIf they break away to a new district, we can\u2019t continue",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 22,
                    "type": "Table"
                }
            },
            {
                "content": "Table I.4: Generated summaries by PEGASUSLARGE (HugeNews) on CNN/DailyMail sampled by ROUGE1-F1.\nCNN/DailyMail Document (ID #134) a us citizen has been killed in a mortar attack in yemen after he traveled to the country in an attempt to extricate his pregnant wife and daughter from the civil war there and \ufb02y them to california, family say. jamal al-labani was an oakland gas station owner, his cousin mohammed alazzani told kpix-tv. according to alazzani, al-labani was in yemen visiting his pregnant wife and the couple\u2019s two-and-a-half-year-old daughter. alazzani told kpix al-labani was trying to get his family out of the war-torn middle eastern nation and take them to oakland - but he couldn\u2019t because the us has withdrawn its diplomatic staff and the country has shut down most airports. rebels from the houthi islamist group have been battling to take aden, a last foothold of \ufb01ghters loyal to saudi-backed president abd-rabbu mansour hadi. they have advanced to the city center despite 11 days of air strikes by a saudi-led coalition of mainly gulf air forces. scroll down for video . family: jamal al-labani was in yemen visiting his pregnant wife and the couple\u2019s 2 1/2-year-old daughter . attack: jamal al-labani\u2019s family has said he was struck by mortar shrapnel after leaving a mosque tuesday and soon died . sunni muslim saudi arabia launched the air strikes on march 26 in an attempt to turn back the iran-allied shi\u2019ite houthis, who already control yemen\u2019s capital sanaa, and restore some of hadi\u2019s crumbling authority. the air and sea campaign has targeted houthi convoys, missiles and weapons stores and cut off any possible outside reinforcements - although the houthis deny saudi accusations that they are armed by tehran. career: al-labani reportedly wished to take his family to oakland, though there was no way to do so given what has been happening in yemen . the \ufb01ghting has failed so far ... ... ... Gold jamal al-labani was a oakland, california, gas station owner, as well as a husband and a father-of-three . al-labani traveled to yemen in an attempt to extricate his pregnant wife and daughter from the civil war there and \ufb02y them to california . he was unable to because the us withdrew its diplomatic staff in february . yemen also recently shut down most of its airports . al-labani was struck by mortar shrapnel after leaving a mosque tuesday in aden and soon died . al-labani\u2019s cousin has said houthi forces launched the mortar shelling Model jamal al-labani was in yemen visiting his pregnant wife and the couple\u2019s two-and-a-half-year-old daughter . al-labani\u2019s family said he was struck by mortar shrapnel after leaving a mosque tuesday and soon died . he was with his teenage nephew, who was also killed, when the attack occurred . al-labani\u2019s sons from an earlier marriage reside in california . ROUGE1-F1 50.93 Document (ID #256) kevin de bruyne\u2019s agent expects to go \u2019around the world\u2019 discussing his client as interest in the wolfsburg mid\ufb01elder increases ahead of the summer transfer window. patrick de koster, who has con\ufb01rmed that he held talks with manchester city earlier this season, admits that he could receive 20 phone calls a day about the belgium international as clubs prepare to strengthen their squads before the start of next season. city are keen to sign the ex-chelsea mid\ufb01elder as they look to reshape their squad but de koster insists no decision has been made and that the 23-year-old could even remain at the volkswagen arena. wolfsburg mid\ufb01elder has attracted interest from manchester city, bayern munich and paris saint-germain . de bruyne tussles with schalke\u2019s sead kolasinac during wolfsburg\u2019s 1-1 draw at the volkswagen arena . speaking to the guardian, de koster said: \u2019for the moment, there are no formal discussions. of course in the next few weeks i will be going around the world to talk about the situation with kevin but this is just informal information. \u2019i will talk to everybody but kevin is very, very happy with wolfsburg and the way they have treated him since he arrived from chelsea last year. \u2019there are still \ufb01ve games to play and hopefully they can make sure of a place in the champions league next season so it is a little bit too early to be making any decisions. he has a four-year contract at wolfsburg so we will have to see what they want to do. \u2019i have met the people from manchester city and we know each other. i\u2019ve never spoken to anyone from manchester united but a lot of other clubs have been in touch to \ufb01nd out some general information.\u2019 bundesliga champions bayern munich and ligue 1 ... ... ... Gold patrick de koster will go \u2019around the world\u2019 to talk about kevin de bruyne . the wolfsburg mid\ufb01elder is wanted by manchester city and bayern munich . de koster has admitted having talks with city chiefs this season . but he has not spoken to manchester united about a move for his client . de bruyne remains happy at wolfsburg and could yet remain at the club . Model kevin de bruyne has attracted interest from manchester city, bayern munich and psg . patrick de koster expects to go \u2019around the world\u2019 discussing his client . de bruyne has scored 10 league goals and provided 17 assists this season . ROUGE1-F1 49.50 Document (ID #141) matthew kenney smoked \ufb02akka and then ran naked . a \ufb02orida man who was high on a designer drug called \ufb02akka stripped and ran naked through traf\ufb01c in fort lauderdale to escape from imaginary killers who he believed stole his clothes and wanted to murder him. matthew kenney, 34, told police he smoked \ufb02akka before he streaked though traf\ufb01c early on saturday evening while only wearing a pair of sneakers. \ufb02akka, which can be injected, snorted, smoked, swallowed or taken with other substances, has been nicknamed \u2019$5 insanity\u2019 for its mind-bending effects and cheap cost. after he was arrested, kenney told police he would \u2019rather die than be caught by these unknown people\u2019, the sun sentinel reported. he added that \u2019if i got hit by a car they would stop chasing me\u2019 according to a fort lauderdale police reported. kenney has previous arrests for disorderly conduct, making a riot and possession of a controlled substance. he was hospitalized for a psychiatric evaluation. \ufb02akka is usually made from the chemical alpha-pvp, a synthetic version of the stimulant cathinone. that is the same type of chemical that is used to make bath salts. scroll down for video . kenney, 34, ran though traf\ufb01c early on saturday evening while only wearing sneakers in fort lauderdale, \ufb02orida . the suspect said he was escaping imaginary killers who he believed stole his clothes and wanted to murder him . the use of \ufb02akka a designer drug that can be even stronger than crystal meth or bath salts, is up in \ufb02orida . \ufb02akka resembles a mix of crack cocaine and meth and it has a a strong odor \u2019like a sweaty sock\u2019, wpbf 25 news reported. once ingested, the drug causes a feeling of euphoria, hallucinations and sometimes psychosis or even superhuman strength. the high ... ... ... Gold matthew kenney, 34, said he smoked \ufb02akka before he went streaking . was arrested on saturday after run through fort lauderdale, \ufb02orida . drug is made from same version of stimulant used to produce bath salts . it causes euphoria, hallucinations, psychosis and superhuman strength . kenney has prior arrests and was hospitalized for a psychiatric evaluation . Model matthew kenney, 34, told police he smoked \ufb02akka before he streaked through traf\ufb01c in fort lauderdale while only wearing a pair of sneakers . he said he was escaping imaginary killers who he believed stole his clothes and wanted to murder him . kenney has previous arrests for disorderly conduct, making a riot and possession of a controlled substance . ROUGE1-F1 40.00 Document (ID #197) i yield to no one in my love of the old days warm beer, cricket on the village green, bobbies on bicycles two by two, all that but it\u2019s rare a chance arises to compare the rose-tinted past with the brave new world, as it did on saturday evening when sky\u2019s high-octane premier league coverage went head-to-head with arsenal v reading in the fa cup semi-\ufb01nal on the bbc. as we know, the premier league has the money and prestige, but what the fa",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 23,
                    "type": "Table"
                }
            },
            {
                "content": "(ID #298) (cnn)the tulsa county reserve deputy who fatally shot a man instead of using his taser turned himself in to authorities tuesday at the tulsa county jail. video shows reserve deputy robert bates announcing he is going to deploy his taser after an undercover weapons sting on april 2 but then shooting eric courtney harris in the back with a handgun. bates was charged with second-degree manslaughter monday. he surrendered tuesday morning, accompanied by his attorney, clark brewster, and immediately posted bail of $25,000. as he exited the jailhouse, bates paused in front of television cameras for a moment but did not speak. his attorney reiterated that he believes the charge against his client is unwarranted. the tulsa county sheriff\u2019s of\ufb01ce says a sting operation caught harris illegally selling a gun. harris ran when of\ufb01cers came in for the arrest. authorities say bates thought he pulled out his taser but \u201dinadvertently\u201d \ufb01red his gun. harris\u2019 brother, andre harris, told cnn that he is pleased district attorney steve kunzweiler pressed charges. in his opinion, however, no type of force should have been used in the arrest of his brother. watching the video of the shooting, andre harris said he can see that three or more of\ufb01cers were already on top of his brother. that manpower should have been enough to arrest him, he said. \u201dit was a situation where i didn\u2019t necessarily think that a taser should even be used,\u201d andre harris said. scott wood, another bates\u2019 attorney, has said the shooting was an \u201dexcusable homicide.\u201d investigators\u2019 efforts to defend bates and the other deputies involved in the arrest have sparked a mounting chorus of criticism online. harris\u2019 relatives are demanding an independent investigation of what they call unjusti\ufb01ed brutality. they\u2019re also questioning why the 73-year-old bates \u2013 the ceo of an ... ... ... Gold reserve deputy robert bates surrenders to authorities, posts bail of $25,000 . bates is charged with second-degree manslaughter in the killing of eric harris . Model tulsa county reserve deputy robert bates turns himself in to authorities . bates is charged with second-degree manslaughter in the death of eric courtney harris . ROUGE2-F1 54.17 Document (ID #148) a former lager lout who ballooned to 24 stone has lost nearly half his body weight by giving up his favourite drink. rugby prop dale forrest, 26, of bolton, would sink up to 12 pints a night, but decided to shed the pounds after seeing a photo of himself while out with his \ufb01tness fanatic friends. in december 2013, the bank teller decided to ditch the booze, give up his favourite fatty readymeals and greasy takeaways and hit the gym. dale forrest would drink up to 12 pints a night - and ballooned to 24 stone - before giving up beer to lose weight . dale, pictured at his slimmest, was worried he would look like a \u2019beached whale\u2019 next to his friends on holiday . since then, mr forrest, who had a holiday planned with his mates and didnt want to look like a beached whale next to them, has lost 10.5 stone. he said: i can now go on nights out and feel con\ufb01dent - even without the dutch courage. i no longer feel like people are laughing at me and all it took was a bit of willpower. mr forrest struggled with his weight from a young age and in adulthood fell into unhealthy habits. before shedding the pounds mr forrest ate a cheese and sausage bap for breakfast, a big daddy box meal from kfc for lunch and a meat feast pizza for dinner. dale, pictured sitting down, said he would consume beer, alcopops and shots regularly on nights out . dale, pictured before his weight loss (right) and after losing 10 stone, gave up greasy takeaways for healthy foods and started going to the gym regularly, and saw the pounds fall off . despite being a rugby player, dale weighed 24 stone due to his ... ... ... Gold dale forrest would go out drinking regularly and would eat fatty food . dined on cheese and sausage bap for breakfast and kfc for lunch . decided to lose weight after seeing photos of him next to slim friends . started going to the gym and eating healthy foods and lost 10 stone . Model dale forrest would drink up to 12 pints a night on nights out with friends . 26-year-old from bolton worried he would look like a \u2019beached whale\u2019 on holiday . in december 2013 he decided to lose weight after seeing a photo of himself . ditched the booze and started going to the gym and lost more than 10 stone . ROUGE2-F1 26.17 Document (ID #260) chelsea will face paris saint-germain, the french team who knocked jose mourinhos side out of the champions league this season, in a pre-season friendly in july. the blues, who were sent crashing out on away goals at the last-16 stage following a 2-2 draw at stamford bridge, will play psg in north carolina on july 25. it is one of three games mourinhos side will feature in across the pond as they gear up to defend a probable premier league title. john terry leads the celebrations as chelsea close in on the premier league title with a 0-0 draw at arsenal . eden hazard, the pfa player of the year, will line-up for chelsea when they travel to the usa in the summer . new york red bulls - july 22 - new jersey . paris saint-germain - july 25 - charlotte, north carolina . barcelona - july 28 - washington d.c. \ufb01orentina - august 5 - stamford bridge . chelsea, 10 points ahead of arsenal with just four games to play, will also face the new york red bulls on july 22 and spanish giants barcelona six days later in washington. chelsea fans will then get to see their side before the premier league campaign kicks-off with a friendly against \ufb01orentina at stamford bridge on august 5. all four matches mark chelseas participation in this summers pre-season international champions cup with manchester united, who mourinhos side will not face, la galaxy, porto and san jose earthquakes also involved. im pleased we are able to announce our \ufb01xtures for what promises to be an exciting summer,\u2019 said chelsea chairman bruce buck. as promised, we face some excellent opposition across several iconic venues in the united states and to top it off we are delighted to be hosting \ufb01orentina at stamford ... ... ... Gold chelsea to play three matches inside six days in the united states . they will face new york red bulls, paris saint-germain and barcelona . \ufb01orentina will then travel to stamford bridge for friendly on august 5 . four matches will make up chelsea\u2019s participation in champions cup . read: chelsea interested in 43m antoine griezmann . Model jose mourinho\u2019s side will play psg in north carolina on july 25 . chelsea will also face the new york red bulls and barcelona . the blues will play \ufb01orentina at stamford bridge on august 5 . ROUGE2-F1 16.09 Document (ID #73) she\u2019s a best-selling singer, actress, beauty buff and one of the world\u2019s most stylish stars. and now, rita ora - who is only 24 and has already made the transition from a kosovan-born \ufb02edgling singer to one of the globe\u2019s most successful stars - has channeled her passion for fashion into a new adidas range. for her latest collaboration with adidas originals, rita has taken the brand\u2019s classics and put her own bold spin on them. the result? a collection that is as daring and vibrant as the designer herself. scroll down for video . rita ora has channeled her passion for fashion into a new adidas range, so femail caught up with the global star to \ufb01nd out her in\ufb02uences and plans for the future . speaking to femail about the inspiration behind her new range, which is emblazoned with graphic dragon prints and lands on may 1, rita said: \u2019the dragon print is all about my love of travel and soaking up different cultures. the print itself is inspired by asian culture and one of its most recognisable symbols. \u2019ive contrasted that with the \ufb01t, basketball style cuts, which were inspired by american culture. my white smoke pack is about the body-mind-soul connection, about taking a moment for re\ufb02ection. the",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 24,
                    "type": "Table"
                }
            },
            {
                "content": "Document (ID #291) it\u2019s truly squeaky bum time in the premier league relegation battle as just nine points separates the bottom seven teams. sportsmail asks some of the managers in and among the dog \ufb01ght what they feel is required for them to avoid the drop this season. question: \u2019what will it take for your club to stay in the premier league?\u2019 chris ramsey (qpr) \u2019if we win three games i think we will stay up. it might take less. i\u2019m not saying that\u2019s the de\ufb01nitive amount - but we really need to start winning, starting with this weekend. to do that, we need to concentrate right to the end and make sure that our performances stay similar with a bit more defensive resilience.\u2019 qpr manager chris ramsey (centre) feels they will avoid relegation if they win three more league games . tim sherwood (aston villa) \u2019i don\u2019t know how many points it will take. we\u2019ve done alright. nothing\u2019s been achieved yet. i\u2019m a new voice and given them a lot of belief and con\ufb01dence that they are better than what they were showing. they have managed to score a few more goals and have real belief they can go to places and win. something was probably a little bit missing previously. i want to make sure that i don\u2019t take my foot off pedal. i\u2019ll make sure the players don\u2019t.\u2019 tim sherwood (left) believes his appointment at aston villa has given them a lot of belief and con\ufb01dence . nigel pearson (leicester) \u2019we\u2019ve put ourselves in a position now where we have a more realistic chance. that\u2019s where we are. if it raises optimism elsewhere, \ufb01ne. if it applies a bit more pressure on other sides around us, \ufb01ne also. but as far as i\u2019m concerned it\u2019s about making sure we\u2019re back ... ... ... Gold just nine points separates the bottom seven clubs in the premier league . qpr boss chris ramsey says they need three more wins to survive . burnley host relegation rivals leicester in the league on saturday . Model just nine points separate the bottom seven teams in the premier league . qpr manager chris ramsey feels they will avoid relegation if they win three more games . tim sherwood believes his appointment at aston villa has given them a lot of belief and con\ufb01dence . ROUGEL-F1 41.03 Document (ID #139) a father whose 20-year-old daughter was found murdered in iowa last year made a desperate plea from china for u.s. authorities to do more in tracking down her killer, six months after the girl\u2019s body was discovered. and police in iowa seem to have now answered it, by issuing a warrant for the girl\u2019s boyfriend, according to reports. tong shao, a chemical engineering student at iowa state university, went missing in september 2014. after a three week search, police found her body stuffed in the trunk of her toyota camry in iowa city. shao\u2019s boyfriend, xiangnan li, 23, was listed as a person of interest in the case and is believed to have been the last person to see her alive, however he bought a one-way ticket to china in the days after his girlfriend went missing and has disappeared, cnn reported. wanted for murder: an arrest warrant has reportedly been issued for , xiangnan li, 23 (right), the boyfriend of tong shaom 20 (left), a university of iowa student found murdered in september after going missing . li had transferred to iowa from rochester institute of technology to be closer to tong. the two had meet studying english in beijing in 2011. they had checked into a hotel room together on september 5, 2014. the pair had stayed at the same hotel three times before and the owner knew them. according to police records obtained by cnn, two days earlier tong had accidentally called li - or \u2019pocket dialed\u2019 him - and he stayed on the line for 30 minutes, overhearing a conversation. tong was complaining about li to a friend and said things that \u2019were not nice\u2019, the records noted. the owner of the hotel told investigators li left the hotel either on the night of september 6 ... ... ... Gold tong shao, 20, was an international student from china attending iowa state university . her body was found in the trunk of her car in iowa city on september 26 . police believe it had been for three weeks . she died of blunt force trauma and asphyxiation . her boyfriend, xiangnan li, 23, was the last to see her, but \ufb02ew to china on september 8, before shao was of\ufb01cially missing . according to tong\u2019s father, an arrest warrant has now been issued . however li has disappeared . Model tong shao, 20, was found murdered in the trunk of her car in september . her boyfriend, xiangnan li, 23, was a person of interest in the case . li bought a one-way ticket to china in the days after tong went missing . he is believed to have been the last person to see her alive . ROUGEL-F1 35.97 Document (ID #293) a skier in switzerland proved that his dog is certainly his best friend when he brought it along to a snowy slope for a day of skiing. videoed descending the crisp ski runs of the small resort of minschuns in val mstair, adrian schaffner is initially featured ascending the mountain on a button lift. perched on his shoulders looking entirely at ease is his dog sintha an appenzeller mix according to the owner, who noted alongside the upload of the original video that he only knows the breed of the dog\u2019s mother. the dog named sintha appears to be entirely at ease as it sits across its owners shoulders . once at the top, mr schaffner points his skis down the mountain and takes off at speed with the dog remaining calmly sat on his back. the dog appears to be enjoying the sensation of speed as it points its face into the wind and the camera angle changes to show the skiers descent. after a long ski to the bottom, mr schaffner comes to a stop and the dog jumps from his shoulders and onto the ground. mr schaffner points his skis down the mountain and begins skiing at speed and the dog remains calmly sat on his back . the dog appears to be enjoying the sensation of speed and points its face into the wind . the skier smiles at the camera as the excited dog begins barking and running off in the snow. the video concludes with the dog who obviously loves snow chasing after some more skiers as they make their way down another section of mountain. discussing the video, mr schaffner wrote: she grew up on a farm in the mountains pretty wild and more or less without any supervision. mr ... ... ... Gold adrian schaffner skis at speed with pet dog on his shoulders . dog called sintha appears content and leans into the wind . video concludes with dog jumping off and running in snow . footage was captured in ski resort in val mstair, switzerland . Model adrian schaffner took his dog sintha on a day of skiing in minschuns, switzerland . the dog appears to be at ease as it sits across its owner\u2019s shoulders . after a long ski to the bottom, mr schaffner comes to a stop and the dog jumps from his shoulders and onto the ground . the skier smiles at the camera as the excited dog begins barking and running off in the snow . ROUGEL-F1 23.01 Document (ID #55) danny willett gave a rules of\ufb01cial, who had been in his line-of-sight, a verbal blast which was clearly audible to spectators surrounding the 17th green at the masters on thursday. englishman willett vented his anger after his second shot from beyond the green trickled all the way across the putting surface and left the preacher\u2019s son facing a bogey or worse. \u2019of anyone you should know the rules,\u2019 willett shouted loudly at the of\ufb01cial, who was sitting about 60 yards away in a golf cart, outside the gallery ropes. danny willett waits to play a shot on the \ufb01fth during the \ufb01rst round at 2015 masters on thursday . the 27-year-old englishman carded a one-under 71 during his \ufb01rst ever round at augusta . willett was still visibly angry as he left the green after salvaging a bogey, though he had calmed down by time he spoke to reporters about 30 minutes later. he said his ire had been raised because the of\ufb01cial had been in his line-of-sight as he was preparing to play his shot. \u2019we were being timed (for slow play), which i can appreciate,\u2019 willett said after carding a one-under 71 at augusta national. \u2019it\u2019s a little bit tricky out here, so it takes a little bit of time, but you\u2019d like to think the referee that\u2019s timing you knows exactly where to put his buggy and where not to put his buggy.\u2019 willett, 27, a two-time european tour winner, is",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 25,
                    "type": "Table"
                }
            },
            {
                "content": "(ID #91) MacMedan, USA TODAY Penelope Cruz stars in the latest \u2019Pirates of the Caribbean 4\u2019 movie. By Dan MacMedan, USA TODAY Penelope Cruz stars in the latest \u2019Pirates of the Caribbean 4\u2019 movie. It was a physically arduous six-month shoot for a massive tentpole \ufb01lm, replete with swamp- wading and sword\ufb01ghting. And right before \ufb01lming began on Pirates of the Caribbean: On Stranger Tides, Cruz discovered buried treasure of a different sort. \u201dI was pregnant through the whole movie. I found out at the beginning,\u201d says Cruz, who, with her typical directness, immediately broke the news to Depp and director Rob Marshall. \u201dI found out before we started, and I wanted them to know because I did not want to go in with any secrets, for protection and for the honesty of my relationship with them. For six months they were all taking such good care of me.\u201d It helped that Cruz, 37, handled her pregnancy with pirate-worthy panache, having little morning sickness, nausea or dizziness. She avoided any dangerous stunts in the \ufb01lm, which opens today, and relied heavily on her dance background to learn the intricate \ufb01ght choreography required of her mercenary buccaneer Angelica, who\u2019s vying with Depp\u2019s Jack Sparrow to \ufb01nd the Fountain of Youth. For Cruz, it felt \u201dgood to be working\u201d while expecting, she says. \u201dI had a lot of free days, and once in a while, I had a free week. (The shoot) was very balanced, very easy. I traveled around the world. The whole summer I spent in Hawaii. It was good. I have only good memories\u201d of the shoot. Fast-forward to a May morning in Manhattan. Cruz\u2019s infant son ... ... ... Gold And she has scored her \ufb01rst lead in a summer blockbuster, as a sexy, feisty swashbuckler opposite her old pal Johnny Depp. Model Penelope Cruz had just scored her \ufb01rst lead in a summer blockbuster, as a sexy, feisty swashbuckler opposite her old pal Johnny Depp. ROUGE1-F1 84.44 Document (ID #228) Seven species of bees in Hawaii have been classi\ufb01ed as endangered, the \ufb01rst time the insect has been protected by federal law. The U.S. Fish & Wildlife Service has granted seven species of yellow-faced bees native to the islands protection under the Endangered Species Act, which will hopefully allow authorities to implement recovery programs, access funding and limit their harm from outside sources, Gregory Koob of USFW told The Associated Press. The yellow-faced bees population faces a range of threats, like habitat destruction, invasive species both animal and vegetable, and all manner of natural phenomena such as wild\ufb01res, hurricanes, tsunamis and droughts. This news represents an important step for bees in general. They and many other pollinating species like butter\ufb02ies are in danger across the world: About 40 percent of invertebrate pollinating species are facing extinction, according to a U.N. report released in February. This could have devastating effects on global agriculture, as about 75 percent of the worlds crops rely on pollination to grow. The ruling also grants protection to 39 plant species found in Hawaii and three other native animals: the band-romped storm-petrel, the orangeblack Hawaiian damsel\ufb02y and the anchialine pool shrimp. Bees in certain parts of the U.S. are facing a different threat, as aerial spraying, aimed to combat Zika-spreading mosquitos, is killing hives en masse. Gold A species of bee in Hawaii has been added to the endangered species list, a \ufb01rst for the insect. Model Seven species of bees in Hawaii have been classi\ufb01ed as endangered, the \ufb01rst time the insect has been protected by federal law. The U.S. Fish & Wildlife Service has granted seven species of yellow-faced bees native to the islands protection under the Endangered Species Act, which will hopefully allow authorities to implement recovery programs, access funding and limit their harm from outside sources, Gregory Koob of USFW told The Associated Press. ROUGE1-F1 28.57 Document (ID #227) Culture Connoisseurs consistently offer thought-provoking, timely comments on the arts, lifestyle and entertainment. More about badges \u2014 Request a badge Washingtologists consistently post thought-provoking, timely comments on events, communities, and trends in the Washington area. More about badges \u2014 Request a badge This commenter is a Washington Post editor, reporter or producer. This commenter is a Washington Post contributor. Post contributors arent staff, but may write articles or columns. In some cases, contributors are sources or experts quoted in a story. More about badges \u2014 Request a badge Washington Post reporters or editors recommend this comment or reader post. You must be logged in to report a comment. You must be logged in to recommend a comment. Gold PHOTOS: The legendary heavyweight boxer was one of the most important political, social and athletic \ufb01gures of the 20th century. Model A look at some of our favorite images of the week. ROUGE1-F1 19.35 Document (ID #98) You may have heard Skip Bayless is a Dallas Cowboys fan. And unlike in past years, that is worth celebrating (which Bayless does quite frequently on Undisputed). At 12-2 the Cowboys are running away from the rest of the NFC, have clinched a playoff berth and have a rookie duo in Dak Prescott and Ezekiel Elliott that appears poised to make a deep playoff push in 2017 and beyond. With such a bright future, many Cowboys fans will be searching for a way to suitably express their jubilation. Enter Bayless and DJ Steve Porter with How Bout Them Cowboys. There are a few observations to be made right off the bat. 1) What is that table on set made of and where can it be ordered? The abuse it has taken without breaking is remarkable. 2) There are an extraordinary number of famous people taken aback by Skips passion in 90 seconds. At a glance ... I have a feeling Im about to unleash, youre Cowboy hating, is it Dak is it Zeke? My Cowboys have arrived, are you sleep deprived? You have to eat humble pie Monday after Monday, they just keep making plays, here we go, every dog has its day Dak attack, the MVP, Dak attack, you decree, ah ah ah, kee kee kee, week after week, I do agree Romo need to check that team chemistry, Dez Bryant chasing greatness oh thats fancy 3) A special consideration should be made by the Recording Academy to bestow an honorary Grammy to the whites of Shannon Sharpes eyes for their performance throughout the video. Is it a hit single? Only time will tell. Will the Cowboys use this as inspiration to reach their \ufb01rst Super Bowl since 1996? If they do, someone better reinforce that table. Gold It won\u2019t end the QB controversy, but it is catchy. Model Skip Bayless is a Dallas Cowboys fan. And unlike in past years, that is worth celebrating (which Bayless does quite frequently on Undisputed). At 12-2 the Cowboys are running away from the rest of the NFC, have clinched a playoff berth and have a rookie duo in Dak Prescott and Ezekiel Elliott that appears poised to make a deep playoff push in 2017 and beyond. With such a bright future, many Cowboys fans will be searching for a way to suitably express their jubilation. Enter Bayless and DJ Steve Porter with How Bout Them Cowboys. There are a few observations to be made right",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 26,
                    "type": "Table"
                }
            },
            {
                "content": "Document (ID #193) 2011 - 12:33 pm \u2014 Gregory Booth, the CEO of Zippo, which makes perhaps the worlds most iconic cigarette lighters, stopped by the Forbes studio recently to talk about how to sell a commodity product at decidedly non-commodity prices. You must be logged in to post a comment Log in with your Forbes account Gold The CEO of the world\u2019s most iconic maker of cigarette lighters talks about how to sell a commodity product \u2013 at decidedly non-commodity prices. Model Gregory Booth, the CEO of Zippo, which makes perhaps the world\u2019s most iconic cigarette lighters, stopped by the Forbes studio recently to talk about how to sell a commodity product - at decidedly non-commodity prices. ROUGE2-F1 61.02 Document (ID #32) TESTED 2013 Lincoln MKZ Hybrid WHAT IS IT? A Lincoln-branded upscale hybrid sedan with its Ford roots showing. HOW MUCH? $36,820 base, $42,415 as tested. The price is the same as an equivalent nonhybrid MKZ. WHAT MAKES IT RUN? A 141-horsepower 2-liter 4-cylinder, with an 88-kilowatt electric motor and a 1.4-kilowatt-hour lithium-ion battery for 188 net horsepower; continuously variable transmission. IS IT THIRSTY? The E.P.A. rating is 45 m.p.g. all around in the city, on the highway and combined. ALTERNATIVES Lexus ES 300h, Ford Fusion Hybrid. Part of the fun of a hybrid car is listening for the gasoline engine to start and analyzing how smoothly it makes the transition from electric to gas. Unless you \ufb02oor the accelerator, the MKZ Hybrid is so quiet it s hard to tell what s moving you forward. With active noise control, which sends sound-canceling frequencies through the infotainment system, the car errs on the side of isolation: if engagement with the road is your goal, this isn t your ride. Rather, the appeal is the combination of an impressive mileage rating and an understated wood-and-leather ambience. Ford calls the MKZ Hybrid America s most fuel-ef\ufb01cient luxury sedan. Consumers are showing an active interest in upscale gas sippers, a category that never existed in the past. This is the second iteration of the MKZ Hybrid, which shares a powertrain with the Ford Fusion Hybrid. Both cars were extensively reworked for 2013, with striking but quite different new designs. The MKZ s extroverted styling is one of the best things about it. The winged grille treatment recalls a subtler form on the streamlined Stout Scarab of the 1930s. The lithium-ion battery replaces the nickel-metal-hydride pack in ... ... ... Gold The appeal of the 2013 Lincoln MKZ Hybrid is its impressive 45 miles per gallon rating and understated wood-and-leather ambience. Model The appeal of the Lincoln MKZ Hybrid is the combination of an impressive mileage rating and an understated wood-and-leather ambience. ROUGE2-F1 52.38 Document (ID #85) Go up to your attic right now, or down to your basement, or to your late parents storage unit that you still pay for each month even though youre unfamiliar with its exact contents. Wherever it is you keep things that are ancient and dusty and mysterious, go there immediately and see if an elderly African American woman stares back at you from the canvas of an oil painting. Because if she does, you may have found one of the worlds rarest paintings. John Kelly writes \u201dJohn Kelly\u2019s Washington,\u201d a daily look at Washington\u2019s less-famous side. Born in Washington, John started at The Post in 1989 as deputy editor in the Weekend section. It would be the only oil painting of an African American woman who came over on a slave ship. Thats how valuable it would be, said Jim Johnston. Jim is a Bethesda lawyer and author. His infatuation with a different oil painting inspired him to write From Slave Ship to Harvard: Yarrow Mamout and the History of an African American Family, published in 2012 by Fordham University Press. Yarrow Mamout was taken from West Africa in the 18th century and sold into slavery in Maryland. He was owned by a Georgetown family but eventually gained his freedom. He was well known in Georgetown, a practicing Muslim who made bricks and owned land. His portrait was painted in 1819 by Charles Willson Peale and displayed in Peales Philadelphia museum. Yarrow was also painted by James Alexander Simpson, a Georgetown painter. Today the Alexander painting of Yarrow hangs in the Peabody Room of the Georgetown public library branch. But in 1825 it hung at Ninth and Pennsylvania NW. We know this because the newspapers at the time were full of mentions of a new attraction: the Columbia Museum. The ... ... ... Gold In 1825, a painting of an African American woman was displayed in Washington. Where is it now? Model Go up to your attic right now, or down to your basement, or to your late parents storage unit that you still pay for each month even though youre unfamiliar with its exact contents. Wherever it is you keep things that are ancient and dusty and mysterious, go there immediately and see if an elderly African American woman stares back at you from the canvas of an oil painting. Because if she does, you may have found one of the worlds rarest paintings. ROUGE2-F1 6.00 Document (ID #163) The age when North American clubs look to England and the wider British Isles with a childs embrace, beseeching input from the mother countrys coaching bosom to take the domestic game on, has long since passed. Or so some critics would argue. A new, forward-thinking generation of coaches not steeped in the 4-4-2 and an unadorned, direct style of football rule the waves in this epoch, they postulate. And this legion of coaches are increasingly young, fresh and, most importantly, American. While there might be some historical merit to the spirit of the argument against the British-style coach, it is perhaps itself a little outdated, not to say somewhat harsh on someone like Carl Robinson and his stylish Vancouver Whitecaps, for instance. Some 20 years on from the birth of Major League Soccer, some of the men who as players helped mold the countrys top division into what it has become today now form an integral portion of the younger cohort of the domestic coaching ranks. Broadly successful elder statesmen like Bruce Arena and Sigi Schmid are followed by a seemingly blossoming rank and \ufb01le. Head coaches such as Sporting Kansas Citys Peter Vermes. DC Uniteds Ben Olsen. New England Revolutions Jay Heaps. Columbus Crews Gregg Berhalter. New York Red Bulls Jesse Marsch. Behind them, too, are some well-thought-of assistants. Then there are the \ufb01rmly rooted foreign coaches partly schooled in MLS ways after successful stints in the league. Robinson would be one. As would FC Dallas head coach Oscar Pareja. Related: Would a pan-American Champions League bene\ufb01t soccer in the US? Yet the country has yet to see a coach leave home shores and make a name with instant recognition. Amid the chattering classes of the global games chief draws in Europe, that is not always easy. It is ... ... ...",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 27,
                    "type": "Table"
                }
            },
            {
                "content": "To account for injuries and roster moves announced late in the week, we will be re-simulating games on Thursdays through the season. Note: our predictions use the latest available team and player information, while NFL SimMatchup includes all players that were or will be available at any point during the 2015 season so that theoretical and \u201dwhat if\u201d scenarios can be simulated. Week 4 was a prove it week for multiple teams, and most passed the test. The undefeated Atlanta Falcons went up against the Houston Texans in our Game of the Week, and as predicted, they continued their hot start to the season with a big win. Our Lock of the Week was the Colts winning against the Jags, and even without Andrew Luck, Indy pulled it off. For the week, our NFL simulation engine \ufb01nished 9-6 and 4-4 against the spread. Nailed It: The NFL simulation engine had no problems with the Denver-Minnesota game. The engine predicted the Broncos would win by an average score of 27-20. The actual \ufb01nal score? Broncos 23, Vikings 20. Wide Right: Its \ufb01tting that the Buffalo Bills show up here at least once, right? The NFL simulation engine predicted the Bills would beat the Giants by an average score of 26-20 and win 64.4 percent of the time, but the Giants took care of business and upset the Bills 24-10 instead. Track our 2015 performance or view our accuracy last season. Week 5 Game to Watch Seattle at Cincinnati: The undefeated Bengals will play host to the reigning NFC champs ... ... ... Gold View WhatIfSports.com\u2019s NFL predictions for Week 5. Model View WhatIfSports.com\u2019s NFL predictions for Week 5. ROUGEL-F1 100.00 Document (ID #127) Poverty comes in many forms: a middle-aged man making $10 an hour, a single mother with mouths to feed, a person with a criminal record and few employers willing to look past it. But the path out is often the same: acquiring a new skill, whether its taking college classes or learning how to boost a lousy credit score. Within the next few years, nearly two-thirds of all job openings in the United States will require at least some sort of certi\ufb01cation, according to Georgetown University research. But funding for career training has declined drastically. In Massachusetts, the state budget for programs that provide these services has been slashed in half in the past decade. Soon, however, lower-income residents may have more tools to lift themselves up. President Obama just launched an initiative to help workers upgrade skills, and last month, the White House held a summit with employers, nonpro\ufb01ts, tech innovators, and unions committed to expanding and improving educational opportunities for millions of workers. Its very, very, very dif\ufb01cult to move up without accumulating higher-level skills, said Jerry Rubin, chief executive of Jewish Vocational Service, a Boston nonpro\ufb01t that provides worker training. Without them, people will be employed, but they will be at or below the poverty line. Many who have lifted themselves out of desperate situations say they could not have gotten there on their own or without a lot of effort. Here are a few of their stories. A study by a Boston nonpro\ufb01t found that poor people often face high interest rates that make it nearly impossible to pay down debts. Nathaniel Awan spent last fall learning how to build a tool box and a spice rack, among other construction skills, just a few miles from where he grew up in the Four Corners neighborhood ... ... ... Gold Poverty comes in many forms, but the path out is often the same: acquiring a new skill. Model Poverty comes in many forms: a middle-aged man making $10 an hour, a single mother with mouths to feed, a person with a criminal record and few employers willing to look past it. ROUGEL-F1 23.53 Document (ID #258) The chief executive of Wells Fargo, one of the countrys largest banks, stumbled his way through more than four hours of blistering questioning by House members Thursday, struggling to defend the bank over a scandal involving its sales practices. For the second time in as many weeks, John G. Stumpf sat before lawmakers and apologized for the scandal that resulted in the \ufb01ring of 5,300 employees and a $185 million \ufb01ne. This go-round didnt go much better than the \ufb01rst time. \u201dFraud is fraud and theft is theft. What happened at Wells Fargo over the course of many years cannot be described any other way,\u201d said Rep. Jeb Hensarling (R-Tex.), chairman of the House Financial Services Committee. Wells Fargo has turned into a \u201dschool for scoundrels,\u201d said Rep. Carolyn B. Maloney (D-N.Y.). \u201dIve come to the conclusion that Wells Fargo should be broken up, said Rep. Maxine Waters (D-Calif.). \u201dIts too big to manage and Im moving forward to break up the bank. Stumpf, 63, stayed calm as he questioned by more than a dozen members of the committee. But he often struggled to \ufb01nish a sentence before lawmakers cut him off to ask him another question or complain that he was dodging the current one. \u201dI am fully accountable for all unethical sales practices in our retail banking business, and I am fully committed to \ufb01xing this issue, strengthening our culture, and taking the necessary actions to restore our customers trust,\u201d he told the committee. After being pummeled in the Senate last week, Stumpf told the House committee he recommended that the companys board rescind $41 million of unvested stock he had been awarded. Carrie Tolstedt, who led the bank\u2019s community banking unit where the misconduct occurred, will have to forfeit about $19 million, and both executives will not ... ... ... Gold Stumpf agreed to forfeit $41 million after the bank became entangled in sales scandal. Model The chief executive of Wells Fargo, one of the countrys largest banks, stumbled his way through more than four hours of blistering questioning by House members Thursday, struggling to defend the bank over a scandal involving its sales practices. ROUGEL-F1 14.81 Document (ID #95) Y\u2019all think a little toy buzzer is going to keep the truth from tumbling out from these jaws? With news that Olivia\u2019s (other) ex Edison Davis (Norm Lewis) is joining the race, Shondaland has completed the fantasy: two women, a Hispanic man, a black man and a racist redneck who is almost certainly but not actually Donald Trump. \u2019SCANDAL\u2019 RECAP SEASON 5, EPISODE 14: BILLIONAIRE BUFFOON RUNS FOR PRESIDENT BUT NOT THE ONE YOU\u2019RE THINKING OF Yes somehow, this election still isn\u2019t interesting. Cyrus (Jeff Perry) is \ufb01ghting for control of Francisco Vargas (Ricardo Chavira) with the candidate\u2019s brother, which is mind-numbingly boring. Olivia (Kerry Washington) and Huck (Guillermo Diaz) have set up a focus group on Mellie\u2019s (Bellamy Young) campaign, which lends itself to a hilariously ridiculous new slogan: Mellie Grant, woman of the people. Remember when she got drunk on hooch and ate fried chicken for months because a secret society killed her son? Quinn (Katie Lowes) goes undercover as a WASP to gather information on Vanessa, because Liv\u2019s still freaking out about Jake\u2019s (Scott Foley) love life. And Hollis Doyle is shooting really big guns and posing for cameras. Election 2016, y\u2019all. It\u2019s weird in all realities. Mellie\u2019s trek to Gettysburger to relate to the commonfolk back\ufb01res when she tells reporters that she and Karen (remember her?) used to go every Sunday after church. But dun-dun-dun! Gettysburger is closed on Sundays. So we have Burgergate. End the meme. End it now. \u2019SCANDAL\u2019 RECAP 5X12: FITZ IS DATING, CYRUS IS SCHEMING, OLIVIA IS SPYING After some more WASP undercover work by Quinn, we \ufb01nally get some marginally interesting information: Jake is stealing money from Vanessa to give to Papa Pope (Joe Morton) to fund a Super Pac for Edison. Which, at least somewhat, explains why ... ... ...",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 28,
                    "type": "Table"
                }
            },
            {
                "content": "Document (ID #114) Size really does seem to matter when it comes to cancer risk. Being tall undoubtedly has its bene\ufb01ts. You can see in a crowd and grab objects off high shelves. But with the good comes the bad. The taller you are, the higher your odds of developing cancer, and a new paper has added weight to this. key points Key points: Taller people have more cells in their body, as well as higher levels of a protein that encourages cells to divide and grow For every 10cm over the average height, a person\u2019s risk for cancer increases 10 per cent New analysis of data from big cancer studies supports this, and also \ufb01nds a few speci\ufb01c cancers to be more or less strongly correlated with height Leonard Nunney, an evolutionary biologist at the University of California, Riverside, looked at massive cancer databases to \ufb01nd out how the number of cells in a person\u2019s body, using height as a proxy, might affect their risk of developing cancer. Reporting in the Proceedings of the Royal Society B, he found being taller and having more cells did mean more cancer overall: For every 10 centimetres over the average height, the risk of developing any cancer increased by around 10 per cent. This \ufb01ts with previous studies. \u201dIf you were comparing a 5-foot guy to a basketball player who\u2019s over 7 feet tall, then that basketball player has around twice the risk of cancer across the board,\u201d Professor Nunney said. He also found that taller people were at much higher risk of melanoma, and women speci\ufb01cally had greater odds of developing thyroid cancer. But it doesn\u2019t mean tall people should panic. \u201dNow, you can\u2019t do anything about your height, but what you can do is tell extremely tall individuals that they should be aware ... ... ... Gold A new study on cancer won\u2019t make for pleasant reading for tall people. Consider this quote from lead researcher Leonard Nunney of the University of California Riverside: \u201dIf you were comparing a 5-foot guy to a basketball player who\u2019s over 7 feet tall, then that basketball player has around twice the risk of cancer across the board,\u201d he tells Australia\u2019s ABC. Yes, Nunney\u2019s study in the Proceedings of the Royal Society B suggests that taller people are at greater risk for cancer. The surprisingly simple reason? They have more cells, and thus more things go can wrong in those cells, explains the Guardian. Speci\ufb01cally, the study found that cancer risk rises by 10% for every 4 inches people are above average height5 feet 4 for women and 5 feet 9 for men, reports CNN. The \ufb01ndings held true for 18 of 23 cancers studied. \u201dNow, you can\u2019t do anything about your height, but what you can do is tell extremely tall individuals that they should be aware of this, and if they have any concerns, to get checked,\u201d says Nunney. Cancers of the colon and kidney were among those with the strongest correlation to height. A researcher not involved with the study says it backs up previously seen links between height and cancer Model Taller people are more likely to develop cancer than shorter people, according to a new study. The study, published in the Proceedings of the Royal Society B, found that for every 10 centimeters over the average height of a person, the risk of developing cancer increases by 10%. \u201dIf you were comparing a 5-foot guy to a basketball player who\u2019s over 7 feet tall, then that basketball player has around twice the risk of cancer across the board,\u201d Leonard Nunney, an evolutionary biologist at the University of California, Riverside, tells Australia\u2019s News Network. The study also found that taller people were at much higher risk of melanoma, thyroid cancer, and cervical cancer. \u201dNow, you can\u2019t do anything about your height, but what you can do is tell extremely tall individuals that they should be aware of this, and if they have any concerns, to get checked,\u201d Nunney says. \u201dIt just suggests more vigilance, I think.\u201d One theory is that taller people have more cells in their bodies, which encourages them to grow. But Nunney also found that cancers in the skin, stomach, and mouth were more closely linked to height. \u201dAcross other cancers, there\u2019s no real geographical variation,\u201d he says. ROUGE1-F1 65.88 Document (ID #145) Image copyright Crossrail Image caption The plague victims\u2019 bones reveal clues to their harsh lives in medieval London Skeletons unearthed in London Crossrail excavations are Black Death victims from the great pandemic of the 14th Century, forensic tests indicate. Their teeth contain DNA from the plague bacterium Yersinia pestis and their graves have been dated to 1348-50. Records say thousands of Londoners perished and their corpses were dumped in a mass grave outside the City, but its exact location was a mystery. This discovery solves a 660-year-old mystery. It\u2019s a hugely important step forward Jay Carver , Lead archaeologist, Crossrail Archaeologists now believe it is under Charterhouse Square near the Barbican. They plan to expand their search for victims across the square - guided by underground radar scans, which have picked up signs of many more graves. Crossrail\u2019s lead archaeologist Jay Carver says the \ufb01nd \u201dsolves a 660-year-old mystery\u201d. \u201dThis discovery is a hugely important step forward in documenting and understanding Europe\u2019s most devastating pandemic,\u201d he said. \u201dFurther excavations will follow to see if - as we expect - we are coming across a much bigger mass burial trench.\u201d Media playback is unsupported on your device Media caption The teeth of the skeletons contain plague bacterial DNA Between 1347 and 1351 the \u201dGreat Pestilence\u201d swept westward across Europe killing millions of people. It later became known as the Black Death. The plague The plague is one of the oldest identi\ufb01able diseases known to man Plague is spread from one rodent to another by \ufb02eas, and to humans either by the bite of infected \ufb02eas or when handling infected hosts Recent outbreaks have shown that plague may reappear in areas that have long been free of the disease Plague can be treated with antibiotics such as streptomycin and tetracycline Source: World ... ... ... Gold Skeletons dug up in London last year are indeed the remains of people who died from the Black Plagueand who suffered a tough life before falling ill, the BBC reports. Forensic analysis shows that teeth taken from at least four of the 12 corpses discovered during excavation for a rail line contained trace amounts of plague DNA, indicating exposure. Early burials found at the site, from the late 1340s, are nice and orderly, with bodies wrapped in white shrouds, but skeletons from a second outbreak in the 1430s are tossed in with what appear to be upper-body injuriesevidence of \u201da period of lawlessness and social breakdown,\u201d Phys.org reports. Among other signi\ufb01cant details: Several skeletons suffered from malnutrition and 16% had rickets. Many had back damage, signalling stressful manual labor. Analysis of one victim is amazingly detailed: He was born outside of London, breastfed, had bad tooth decay as a boy, and worked in manual labor before dying as a young man of the Black Death. Archaeologists suspect that thousands more Black Death victims lie nearby, and a dig is planned for this summer. DNA experts are analyzing the plague genome in victims\u2019 teeth in case there\u2019s more to learn about the disease, which still kills 2,000 people per year ( Model It\u2019s a \u201dhugely important\u201d \ufb01nd, one that could shed light on the Black Death, the plague that wiped out at least 60% of London\u2019s population between 1347 and 1351, the BBC reports. Archaeologists digging for railway tracks in London\u2019s Crossrail station uncovered 25 skeletons, along with pottery dated to the mid-14th century, that have DNA from the bacterium Yerinia pestis dated to 13 to 50 years old. That\u2019s a long way from the 660-year-old record showing that thousands of Londoners died from the plague and their bodies were dumped in a mass grave outside the City of London. \u201dWe can start to answer questions like: where did they come from and what were their lives about?\u201d says Crossrail archaeologist Jay Carver. \u201dI\u2019m amazed of the reasons for why the Black Death was so devastating.\u201d DNA from 12 of the skeletons has con\ufb01rmed that they had contact with the plague prior to their death, and researchers plan more excavations to see if they\u2019re coming across a much bigger plague trench. \u201dWe can see that Londoners weren\u2019t living an easy life,\u201d Carver says, \u201dand that\u2019s possibly one of the explanations for why the Black Death was so devastating.\u201d Antibiotic-resistant strains",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 29,
                    "type": "Table"
                }
            },
            {
                "content": "Document (ID #261) A murder suspect remains on the run following a weekend shooting in Montpelier. It left a former Vermont Frost Heaves basketball player dead. WCAX News is learning the victim and the shooter may have been friends. It appears to have been a night out at the bar gone wrong. Jayveon Caballero, 29, is wanted for second-degree murder. Nearly 48 hours after he allegedly shot a man to death in Montpelier, police still don\u2019t know where he is. \u201dHe may have gotten on a bus yesterday morning in White River Junction, a Greyhound bus, heading south,\u201d said Maj. Glenn Hall, Vermont State Police. Police say Caballero fatally shot Markus Austin, 33. A friend told investigators Austin was invited out with friends to join Caballero, who he called \u201dmy boy Jay.\u201d But at the end of the night a \ufb01ght broke out. \u201dWe do know from the investigation that there was an altercation outside Gusto\u2019s bar in Barre,\u201d said Hall. The alleged shooter\u2019s girlfriend, Desiree Cary, 22, told police that she got mad and hit Austin as they left the bar. Austin allegedly hit her back, injuring her jaw bone. Court documents show Caballero appeared upset and then went to \ufb01nd Austin at a Barre Street apartment. \u201dHe walked over to the victim\u2019s car and shot him and walked back,\u201d said Jeff Sarpong, witness. Sarpong says he woke up to someone yelling outside and a gunshot, and then looked out his window. \u201dI see him turn around, wave with a gun,\u201d said Sarpong. He said Caballero appeared to show sympathy just seconds after pulling the trigger. \u201dHe then just kept on asking, \u2019Are you OK?\u2019\u201d Sarpong said. \u201dAfter he noticed what he had done and that he wasn\u2019t responsive, he just drove away,\u201d said Sarpong. In an attempt to locate Caballero, police ... ... ... Gold Police in the country\u2019s smallest state capital are investigating something that hasn\u2019t happened there in living memory: a murder. Police in Vermont say they have issued an arrest warrant for 29-year-old Jayveon Caballero, who is accused of fatally shooting Markus Austin in Montpelier early Sunday after an altercation, the Times Argus reports. Witnesses told investigators that Austin, 33, assaulted Caballeros girlfriend, Desiree Cary, during the altercation outside a bar in the nearby city of Barre early Sunday. Police say Caballero later confronted Austin outside his apartment and shot him dead. The body was found in the parking lot of Austin\u2019s apartment complex. A Vermont State Police spokesman says Caballero, who knew Austin before the shooting, \ufb02ed the town and probably the state after the shooting, WCAX reports. The police spokesman says Caballero is believed to have boarded a Greyhound bus bound for New York City on Sunday morning, which made numerous stops in other states along the way. Later Sunday, Cary, 22, was arrested on apparently unrelated charges of selling heroin and crack cocaine. With around 7,500 residents, Montpelier has the smallest population of any state capital, the AP notes. Police Chief Anthony Facos says this is the city\u2019s \ufb01rst murder since a woman shot her husband in the 1920s. Model A man has been shot dead outside an apartment complex in what police say is the \ufb01rst murder in the nation\u2019s smallest capital city in almost a century. An arrest warrant was issued for Jayveon Caballero on a murder charge following the Sunday morning shooting in Montpelier, the AP reports. Police were seeking Caballero in the killing of Markus Austin, whose body was found in the apartment complex\u2019s parking lot. Caballero is believed to have boarded a bus on Sunday in White River Junction. Police said the bus was headed to New York City but had several intermediate stops where he could have gotten off. Police were seeking Caballero in the killing of Austin, whose body was found in the apartment complex\u2019s parking lot. Caballero and Austin had been involved in an altercation outside Gustos bar in earlier Sunday morning, police said. During the altercation Austin assaulted Caballero\u2019s girlfriend, Desiree Cary, witnesses said. The victim, 33-year-old Markus Austin, was shot outside his apartment at 191 Barre Street in Montpelier around 4:30am Sunday. Caballero later confronted Austin outside Austin\u2019s apartment and shot him, police said. Caballero\u2019s girlfriend, pleaded not guilty to drug charges and was released. Montpelier has about 7,500 residents, making it the nation\u2019s least populous capital. ROUGE2-F1 25.81 Document (ID #176) Sandra Chesterman, left, puts a braclet on the wrist of Christine Kirby during a birthday party for her son Jayden Kirby, 1, on Sunday, Feb. 9, 2014, in Fremont, Calif. Chesterman\u2019s daughter Kristina was killed by an alleged drunk driver last September while coming home from nursing school in Chico. Jayden was given months to live before he received part of Kristina Chesterman\u2019s liver. Jayden and Christina share the same birth stone and that gem is embeded into the bracelet. (Aric Crabb/Bay Area News Group) ( ARIC CRABB ) LIVERMORE \u2013 When she was still in high school, Kristina Chesterman wrote out her bucket list. Flying a plane was on it; so was running through a poppy \ufb01eld and breaking up a \ufb01ght between two boys over her affections. She also wanted to save a life. The aspiring nurse\u2019s ambitions came to a halt in September, when she was killed by a suspected drunken driver near Chico State, where she attended school. Though Chesterman, 21, didn\u2019t get to mark much off her list, she has saved more lives than she hoped \u2013 and is profoundly affecting many others. Five Northern Californians have been saved through Kristina\u2019s choice to donate her organs. And her grateful friends and family are making the rest of her bucket list their own. A photograph of Kristina Chesterman is displayed during a birthday party for Jayden Kirby on Sunday, Feb. 9, 2014, in Fremont, Calif. Chesterman was killed by an alleged drunk driver last September while coming home from nursing school in Chico. Jayden, 1, was given months to live before he received part of Kristina Chesterman\u2019s liver. (Courtesy of the Chesterman Family) ( Chesterman Family ) Chesterman\u2019s mother, Sandra, of Livermore, said her daughter wanted to help people from an early age. She routinely gave ... ... ... Gold Kristina Chesterman, 21, was studying to be a nurse when she was killed by a suspected drunk driver last yearbut she managed to save lives anyway. A registered organ donor, Chesterman gave \ufb01ve people, including a baby, new life, and now the woman who received her heart wants to do something in return. Susan Vieira, 64, has vowed to check off everything on Chesterman\u2019s bucket listwritten down on a piece of paper her mom only recently found. (One of the items? \u201dSave someone\u2019s life.\u201d Another? \u201dBe in four places at once.\u201d) Vieira had completed several of the tasks already, including learning to \ufb02y a plane and riding a camel, ABC News reports. And now, \u201dtogether, we will \ufb01nish her bucket list,\u201d Vieira says. Other points on the list include running through a poppy \ufb01eld and riding in a hot-air balloon, the San Jose Mercury News reported earlier this year. \u201dI\u2019d like to think all the things I continue to accomplish in my life, Im taking Kristina with me,\u201d Vieira continues; Chesterman\u2019s mom adds that she \u201dfelt an instant connection\u201d to the woman she just met. Chesterman\u2019s friends are also helping to complete the bucket list, including Model When 21-year-old Kristina Chesterman was killed by a suspected drunk driver in September, she left behind a bucket list of things she wanted to do before she died. Among them: \ufb02y a plane, break up a \ufb01ght between two boys over her affections, and donate her organs. Now, \ufb01ve Northern Californians have been saved thanks to her decision, the San Jose Mercury News reports. \u201dI know she would\u2019ve been so proud,\u201d says Chesterman\u2019s mother, Sandra. \u201dIt hasn\u2019t been an easy process for us, but at the same time, it\u2019s brought us so much comfort.\u201d One of those saved was 1-year-old Jayden Kirby, who might have died as an infant had he not received part of Chesterman\u2019s liver. Jayden\u2019s mother, Christine, \ufb01gured out the name of the mystery donor after hearing news reports, the Mercury News reports. \u201dI wanted her to see the impact Kristina had,\u201d she says. \u201d(The transplant) absolutely saved (Jayden\u2019s) life.\u201d Chesterman\u2019s aunt, Patricia Picard, texted the couple, asking if Zak Pappachan was a match. Within hours, he had",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 30,
                    "type": "Table"
                }
            },
            {
                "content": "Document (ID #263) This July 24, 2016 photo provided by Niels Alpert, Betsy Davis, third from left, has a laugh with her friends during a going away party in Ojai, Calif. In early July, Davis emailed her closest friends... (Associated Press) SAN DIEGO (AP) In early July, Betsy Davis emailed her closest friends and relatives to invite them to a two-day party, telling them: \u201dThese circumstances are unlike any party you have attended before, requiring emotional stamina, centeredness and openness.\u201d And just one rule: No crying in front of her. The 41-year-old artist with ALS, or Lou Gehrig\u2019s disease, held the gathering to say goodbye before becoming one of the \ufb01rst Californians to take a lethal dose of drugs under the state\u2019s new doctor-assisted suicide law for the terminally ill. \u201dFor me and everyone who was invited, it was very challenging to consider, but there was no question that we would be there for her,\u201d said Niels Alpert, a cinematographer from New York City. \u201dThe idea to go and spend a beautiful weekend that culminates in their suicide that is not a normal thing, not a normal, everyday occurrence. In the background of the lovely fun, smiles and laughter that we had that weekend was the knowledge of what was coming.\u201d Davis worked out a detailed schedule for the gathering on the weekend of July 23-24, including the precise hour she planned to slip into a coma, and shared her plans with her guests in the invitation. More than 30 people came to the party at a home with a wraparound porch in the picturesque Southern California mountain town of Ojai, \ufb02ying in from New York, Chicago and across California. One woman brought a cello. A man played a harmonica. There were cocktails, pizza from her favorite local joint, and a ... ... ... Gold Betsy Davis spent the last years of her life losing control of her body as her illness worsenedbut she could still control how she died, and she \u201dturned her departure into a work of art,\u201d friend and cinematographer Niels Alpert says of the California artist\u2019s death. The 41-year-old, who in 2013 was diagnosed with ALS, also known as Lou Gehrig\u2019s disease, threw a party for more than 30 friends and family that lasted through the weekend before her doctor-assisted suicide late last month, People reports. Davis, a painter and performance artist who invited guests from across the country to what she called a \u201drebirth,\u201d died under the state\u2019s physician-assisted suicide law, which took effect on June 9. \u201dYou\u2019re all very brave for sending me off on my journey,\u201d she wrote in her invitation, which warned guests that \u201demotional stamina\u201d would be required, the AP reports. \u201dThere are no rules,\u201d she wrote. \u201dWear what you want, speak your mind, dance, hop, chant, sing, pray, but do not cry in front of me. OK, one rule.\u201d After a weekend that included music, pizza, cocktails, individual chats with every guest, and a screening of favorite movie The Dance of Reality, Model In early July, Betsy Davis emailed her closest friends and relatives to invite them to a two-day party, telling them: \u201dThese circumstances are unlike any party you have attended before, requiring emotional stamina, centeredness, and openness.\u201d And just one rule: No crying in front of her. The 41-year-old artist with ALS, or Lou Gehrig\u2019s disease, held the gathering to say goodbye before becoming one of the \ufb01rst Californians to take a lethal dose of drugs under the state\u2019s new doctor-assisted suicide law for the terminally ill, the AP reports. Davis worked out a detailed schedule for the gathering on the weekend of July 23-24, including the precise hour she planned to slip into a coma, and shared her plans with her guests in the invitation. More than 30 people came to the party at a home with a wraparound porch in the picturesque Southern California mountain town of Ojai, \ufb02ying in from New York, Chicago, and across California. One woman brought a cello. A man played a harmonica. There were cocktails, pizza from her favorite local joint, and a screening in her room of one of her favorite movies, The Dance of Reality, based on the life of a Chilean \ufb01lm director. As the weekend drew to a close, her friends kissed ROUGEL-F1 23.06 Document (ID #87) The text on the video above was edited on June 1, 2018 to make it clear that it is impossible to know why the polar bear pictured was starving. An earlier version of the video went too far in suggesting that climate change was responsible ( read more ). This story was updated on January 19, 2018 to re\ufb02ect the more speci\ufb01c location of where the photographs were taken. When photographer Paul Nicklen and \ufb01lmmakers from conservation group Sea Legacy arrived on Somerset Islandnear the larger Baf\ufb01n Islandin the Canadian Arctic in late summer, they came across a heartbreaking sight: a starving polar bear on its deathbed. Nicklen is no stranger to bears. From the time he was a child growing up in Canada\u2019s far north the biologist turned wildlife photographer has seen over 3,000 bears in the wild. But the emaciated polar bear, featured in videos Nicklen published to social media on December 5, was one of the most gut-wrenching sights he\u2019s ever seen. \u201dWe stood there crying\ufb01lming with tears rolling down our cheeks,\u201d he said. Video shows the polar bear clinging to life, its white hair limply covering its thin, bony frame. One of the bear\u2019s back legs drags behind it as it walks, likely due to muscle atrophy. Looking for food, the polar bear slowly rummages through a nearby trashcan used seasonally by Inuit \ufb01shers. It \ufb01nds nothing and resignedly collapses back down onto the ground. In the days since Nicklen posted the footage, he\u2019s been asked why he didnt intervene. \u201dOf course, that crossed my mind,\u201d said Nicklen. \u201dBut it\u2019s not like I walk around with a tranquilizer gun or 400 pounds of seal meat.\u201d And even if he did, said Nicklen, he only would have been prolonging the bear\u2019s misery. Plus, feeding wild polar bears ... ... ... Gold An estimated 2.5 billion people saw the image: a starving polar bear struggling across an Arctic landscape. \u201dThe mission was a success, but there was a problem: We had lost control of the narrative,\u201d writes Cristina Mittermeier in National Geographic. Accompanied by a photographic team, she snapped shots of the dying bear last year while colleague Paul Nicklen shot videoall part of their \u201dmission to capture images that communicate the urgency of climate change,\u201d she writes. \u201dWhen Paul posted the video on Instagram, he wrote, \u2019This is what starvation looks like.\u2019\u201d He also wondered if all 25,000 polar bears would die like this and urged people to reduce their carbon footprint. But he didn\u2019t say climate change had killed this bear. That nuance vanished when National Geographic posted the video with the subtitles \u201dThis is what climate change looks like\u201d and the Washington Post ran a dramatic headline about \u201dgut-wrenching\u201d images. People also responded dramatically, expressing gratitude for validation of climate science, anger the crew hadn\u2019t fed the bear, or a stubborn refusal to acknowledge global warming. \u201dPerhaps we made a mistake in not telling the full storythat we were looking for a picture that foretold the future and that we didnt know what had happened to this particular polar bear Model \u201dWe stood there crying\ufb01lming with tears rolling down our cheeks.\u201d That\u2019s Paul Nicklen\u2019s take on a video he posted in December showing a starving polar bear on an island in the Canadian Arctic, National Geographic reports. The video was shot by Nicklen, a biologist and contributor to National Geographic, during a trip with the conservation group Sea Legacy. It shows the bear on Somerset Island, near the larger Baf\ufb01n Island, as it searches a garbage can for food. At one point, the bear shows signs of starvation. \u201dWhen scientists say bears are going extinct, I want people to realize what it looks like,\u201d Nicklen tells National Geographic. \u201dBears are going to starve to death. This is what a starving bear looks like.\u201d The video has since",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 31,
                    "type": "Table"
                }
            },
            {
                "content": "Table I.13: Generated summaries by PEGASUSLARGE (HugeNews) on Gigaword sampled by ROUGE1-F1.\nGigaword Document (ID #269) zairean rebels , led by laurent-desire kabila , on saturday rejected calls by the united nations for a cease\ufb01re , saying it could only be called after talks with kinshasa . Gold zairean rebels reject un call for cease\ufb01re Model zairean rebels reject un cease\ufb01re call ROUGE1-F1 92.31 Document (ID #208) a zairean diplomat was among three african men arrested for smuggling ## kilograms -lrb- ### pounds -rrb- of marijuana into sweden , the swedish news agency tt reported friday . Gold zairean diplomat among three arrested for pot Model zairean diplomat arrested for drug smuggling ROUGE1-F1 61.54 Document (ID #198) prime minister benjamin netanyahu has proposed completing talks on the \ufb01nal status of palestinian territories in the next six months instead of the scheduled two years , according to israeli public television . Gold UNK netanyahu wants to \ufb01nish talks in six months Model netanyahu proposes completing \ufb01nal status talks in six months ROUGE1-F1 55.56 Document (ID #92) reinsurer hannover re ag said thursday that its second-quarter net pro\ufb01t doubled as demand increased for its products , particularly life and health reinsurance . Gold hannover re s #nd-quarter pro\ufb01t doubles Model hannover re says #q net pro\ufb01t doubled ROUGE1-F1 42.86 Document (ID #281) the us energy department -lrb- doe -rrb- announced tuesday that an experimental project in canada to inject carbon dioxide into oil \ufb01elds for its permanent storage in geologic formations proved to be successful . Gold carbon dioxide permanent storage in oil \ufb01eld a success : us energy department Model canadian project to store carbon dioxide in oil wells successful ROUGE1-F1 36.36 Document (ID #149) los angeles might have lost its bid for an expansion team to houston on wednesday , but fans who like to watch football and the tv networks were winners . Gold no l.a. team good for tv viewers Model los angeles loses bid for n\ufb02 team ROUGE1-F1 26.67 Document (ID #286) football fans in the bangladesh capital dhaka will be able to watch the world cup live on big screens at ## city locations , of\ufb01cials said friday . Gold the afp world news summary Model world cup on big screens in dhaka ROUGE1-F1 16.67 Document (ID #258) the suicide bomb attacks in saudi arabia were a cowardly and disgraceful terrorist atrocity , \u201d prime minister tony blair said wednesday . Gold two britons missing after saudi suicide blasts Model blair calls saudi attacks cowardly and disgraceful terrorist atrocity ROUGE1-F1 12.50 Document (ID #223) leading the preakness \ufb01eld at the far turn , edgar prado sensed victory as he prepared peace rules for the \ufb01nal push to the \ufb01nish line . Gold prado watches potential victory fade in stretch Model peace rules wins preakness ROUGE1-F1 0.00 Document (ID #39) chelsea owner roman abramovich and england captain david beckham top english soccer \u2019s rich lists . Gold second test scoreboard Model abramovich beckham top rich list ROUGE1-F1 0.00",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 32,
                    "type": "Table"
                }
            },
            {
                "content": "Document (ID #42) a national human rights museum under the planned ministry of culture is scheduled to open on jan. # , #### , council for cultural affairs chairman emile sheng said friday . Gold human rights museum to open in #### Model national human rights museum to open in #### ROUGE2-F1 90.91 Document (ID #57) greece international central defender sotiris kyrgiakos says he intends to join liverpool after receiving an offer from the premier league club . Gold kyrgiakos says intends to join liverpool Model greece defender kyrgiakos to join liverpool ROUGE2-F1 40.00 Document (ID #201) world number three david nalbandian said wednesday that he was optimistic of being \ufb01t to compete in wimbledon after being forced to retire injured in the french open semi-\ufb01nals last week . Gold nalbandian optimistic for wimbledon \ufb01tness Model nalbandian optimistic of wimbledon return ROUGE2-F1 25.00 Document (ID #13) the weather was cool during georgia tech \u2019s football practice monday , but coach george o\u2019leary was hot and sweaty after the workout . Gold o\u2019leary gets defensive after loss to bc Model o\u2019leary feels the heat ROUGE2-F1 18.18 Document (ID #36) japanese electronics maker sharp corp. announced plans friday to ramp up its output of \ufb02at screen televisions with new production lines in japan and mexico in response to \ufb02ourishing demand . Gold sharp to expand \ufb02at tv production in UNK mexico Model sharp to ramp up \ufb02at screen tv output ROUGE2-F1 13.33 Document (ID #291) nearly a year ago , south carolina supreme court chief justice jean UNK said that fair , swift justice has taken a backseat to an assembly line process that diminishes the quality of hearings afforded south carolinians . Gold fair swift justice demands diversity Model s.c. supreme court chief justice backs off criticism ROUGE2-F1 0.00 Document (ID #210) charlie UNK baf\ufb02ed the cubs with his knuckleball , matt kemp and casey blake homered , and the los angeles dodgers defeated chicago #-# in the national league to win their third game in a row on saturday . Gold UNK s UNK fools cubs as dodgers win #-# Model dodgers beat cubs for third straight win ROUGE2-F1 0.00 Document (ID #173) marc rosset upset boris becker in straight sets to pace switzerland over germany in the opening round of the world team cup monday , while spain beat sweden #-# . Gold switzerland spain win opening matches Model rosset upsets becker in world team cup ROUGE2-F1 0.00 Document (ID #96) in chicago , frank thomas and albert belle hit consecutive homers for the seventh time this season , and chicago beat baltimore to stop a six-game losing streak . Gold toronto : the sixth Model thomas belle power cubs past orioles ROUGE2-F1 0.00 Document (ID #59) his leftist fringe party does n\u2019t even have ### members , but ##-year-old budiman UNK is accused of being at the heart of a communist plot to topple the government . Gold UNK weekly sports calendar Model fringe party leader accused of plotting to topple government",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 33,
                    "type": "Table"
                }
            },
            {
                "content": "Table I.15: Generated summaries by PEGASUSLARGE (HugeNews) on Gigaword sampled by ROUGEL-F1.\nGigaword Document (ID #246) a group of puerto rican businessman will visit chile to explore possible new markets , this u.s. commonwealth \u2019s economic development bank announced . Gold puerto rican trade mission to visit chile Model puerto rican businessmen to visit chile ROUGEL-F1 76.92 Document (ID #248) wednesday \u2019s evening rubber prices in singapore cents per kilo provided by the singapore commodity exchange : UNK Gold evening rubber prices Model wednesday \u2019s evening rubber prices ROUGEL-F1 75.00 Document (ID #2) after spending ## years in prison for a killing he did n\u2019t commit , ken marsh \ufb01nally is poised to get some payback . Gold $ ###,### for ## years wrongly held in prison Model man exonerated after ## years in prison ROUGEL-F1 50.00 Document (ID #15) prime minister of antigua and barbuda baldwin spencer left here monday for hong kong , winding up his four-day visit to shanghai . Gold antigua and barbuda pm leaves shanghai for hong kong Model antigua and barbuda prime minister ends shanghai visit ROUGEL-F1 47.06 Document (ID #188) cuban sports of\ufb01cials and coaches censored the possible elimination of boxers \u2019 protective head gear in the amateur boxing competitions , cuba \u2019s of\ufb01cial press granma \u201d said on friday . Gold cuba sports UNK UNK ask to keep boxers protective gear Model cuban sports of\ufb01cials censor boxers head gear ROUGEL-F1 35.29 Document (ID #164) russia \u2019s defense industry has been badly hit by the global \ufb01nancial crisis , deputy prime minister sergei ivanov , a former defense minister , said tuesday . Gold russia \u2019s defense sector hit by \ufb01nancial crisis : govt of\ufb01cial Model \ufb01nancial crisis hits russian defense industry ROUGEL-F1 25.00 Document (ID #99) one day after lakers owner jerry buss left his interest in phil jackson subject to interpretation , new york knicks president isiah thomas did anything but , telling reporters thursday that he believed a second meeting with jackson soon would take place . Gold jackson at top of knicks list Model thomas says he \u2019ll meet with jackson ROUGEL-F1 15.38 Document (ID #265) us defense secretary donald rumsfeld said thursday the killing of al-qaeda \u2019s leader in iraq , abu musab UNK , was a signi\ufb01cant victory in the battle against terrorism but not the end of the violence . Gold rumsfeld calls zarqawi death signi\ufb01cant victory Model rumsfeld hails killing of al-qaeda leader in iraq ROUGEL-F1 13.33 Document (ID #242) rankings re\ufb02ect sales for the week ending oct. ## , at almost #,### bookstores plus wholesalers serving ##,### other retailers -lrb- gift shops , department stores , newsstands , supermarkets -rrb- , statistically weighted to represent all such outlets nationwide . Gold best sellers : UNK books Model top ## book sales ROUGEL-F1 0.00 Document (ID #51) the rope on the \ufb02agpole is still broken .",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 34,
                    "type": "Table"
                }
            },
            {
                "content": "of the body before leaving for school. Feeling fresh not only looks good, but youll feel rejuvenated and con\ufb01dent.Most boys begin to have a more distinct body odor in middle school. Combat this by wearing deodorant. Avoid using strong colognes such as Axe unless it is in small proportions. If the smell is overbearing, ladies wont want to hang out too close. If you feel the need to use cologne, use it sparingly. Try choosing a deodorant that smells refreshing with a hint of musk.; , Image is important for \ufb01rst impressions. Dont give her the impression that you have a shabby personality because of your clothing. Upgrade some of your t-shirts, jeans, and button down shirts for nicer versions. Swap out your jeans for corduroy, slacks, or khakis. Grab some polos instead of a band t-shirt. Try out a \ufb02annel instead of hoodie.Even if your middle school requires uniforms, you can still pull off a sharp look by caring for your clothes. Do not let your clothes get shabby or smelly. As a general rule, dont wear a top more than one day. Pants should only be worn two to three times before a wash. While looks are not the only thing that matters, its often the \ufb01rst thing that a girl will notice about you. Con\ufb01dence is something that girls can easily pick up on. Try setting small goals for yourself like learning the piano (middle school is the perfect age to begin an instrument). Exercise is another way to feel comfortable in your body and boost your self-esteem. You can reach a con\ufb01dent state of mind by dressing ... ... ... Gold Maintain your daily hygiene. Dress well. Feel good in your skin. Have good posture. Model Practice good hygiene. Dress well. Be con\ufb01dent. Maintain a good posture. Be friendly. ROUGE1-F1 51.85 Document (ID #56) Lip-plumping products come in many forms: glosses, balms, sticks, gels, and pots. Applying these to your lips can temporarily make your lips look fuller, often by irritating them.The plumping effect will only last for a couple of hours, but you can renew it by reapplying the product to your lips. Note that the effects wont be as drastic as those achieved through cosmetic surgery.; , Ingredients like cinnamon, ginger mint, wintergreen, and capsicum will boost blood \ufb02ow to your lips, causing them to redden and swell i.e., to look fuller., If you wish to combine your lip-plumping product with a lipstick or gloss, apply the plumper to your lips \ufb01rst for maximum effectiveness. Dermatologists recommend against overusing lip-plumping products, as these may cause your lips to become dry and scaly.Try to save the lip plumpers for special occasions. If you want more from your lip-plumping products, consider trying a treatment plumper. Manufacturers claim that treatment plumpers can stimulate your lips into producing more collagen and elastin, making your lips fuller for a longer period of time. Treatment plumpers are available for purchase online and in person at shops that carry beauty products.They may be more expensive than traditional plumpers. Common ingredients in treatment plumpers include peptides, marine collagen, and human growth factors. Gold Invest in a lip-plumping product. Know which lip-plumping ingredients to look for. Use a lip plumper as a primer. Dont overuse lip plumpers. Look into treatment plumpers. Model Apply a lip-plumping product. Use natural lip-plumping ingredients. Combine lip-plumping products. Try a treatment plumper. ROUGE1-F1 50.00 Document (ID #95) To do this they should offer a variety of ways to collect customer card details and process payments. These are often de\ufb01ned by your own technical expertise. It could mean providing a simple means of redirecting customers from your site to their hosted payment pages. Or maybe you require a way for you to authorise payments remotely. Whatever your requirement, a dedicated PSP should have the solutions to meet it. One size does not \ufb01t all, so dont let anyone persuade you otherwise.; , A \ufb01xed monthly fee starts at around 10, but there are some cheaper options available, starting as low as 0.05 per transaction. As a rule of thumb, the higher your transaction volume, the lower the unit cost. And remember to compare like with like. Ask if its a complete solution with inclusive fees or whether youll be paying the bank separately on top of that. Will the PSP with those oh-so-attractive prices charge huge set-up fees for enabling services such as deferred payments, a virtual terminal, or subscriptions that come as standard with other PSPs?. Look beyond the headline \ufb01gures and consider the overall long-term operational cost of partnering with each PSP. For example, does the PSP have limited support for fraud management? The cost of \ufb02imsy security could do much more damage to your bottom line than any fractional differences in rates. As an online merchant you need to make sure you can also manage them as ef\ufb01ciently as possible. Which is why an effective PSP should provide you with the right reporting and administration tools to manage your merchant services... Will you have access to real-time transaction statements? Can you export valuable customer data? Is the reporting sophisticated enough to allow you to answer questions that can better target future sales. For ... ... ... Gold Enquire about their technical solutions Technically your PSP must be capable of supporting all of your plans. Look for genuinely competitive rates Costs (set-up fees and cost per transaction) can vary greatly between competing PSPs, so you need to shop around for the best rates. Establish if there are any upfront costs The real costs associated with accepting online card payments do not start and end at the rate quote. Manage reporting and administration facilities Accepting online payments is only one part of the e-commerce equation. Enquire about their technical support A service is not a service unless its fully backed up. Find out if you would be protected against fraud When accepting card payments online, fraud is potentially the biggest threat to your pro\ufb01ts. Make sure you can trust their services Online, the difference between a prospect and a customer is trust. Look out for a full service history Okay, its not a car, but the principle is the same. Model Establish what your PSP needs to do to meet the needs of your online merchant business. Look at the pricing offered by the PSP. Consider the level of support youll get from the PSP. Look at the tools youll be using to manage your merchant services. Consider the need for fraud management systems (PMS).Theres no doubt that the quality of a PSPs fraud solution will depend upon its support for merchant services solutions in other words, its ability to meet the needs of your online merchant business. Check the reliability of the PSPs customer service. ROUGE1-F1 31.58 Document (ID #3) If you hang up on a telemarketer right away, youll probably be placed on the no answer list, and will be called again eventually. You dont want to engage a telemarketer in conversation either, especially if you have absolutely no interest in their product or service. The easiest way to handle a telemarketer is to say, Please put me on your do not call list. If the telemarketer keeps interrupting you or a robot calls you, you might just have to hang up. If the same number persistently calls, report it to the FCC by calling 1-888-CALL-FCC or going to https://www.fcc.gov/. If you live in the",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 35,
                    "type": "Table"
                }
            },
            {
                "content": "Document (ID #29) Microwave frozen spinach for two minutes at 50% power. Break the spinach apart into smaller pieces. Microwave it again, at 50% power, checking and stirring it whenever a minute passes by, until its defrosted.Alternately, you can use one packed cup of fresh baby spinach, washed and \ufb01nely chopped with the stems removed.; , Wrap the spinach in a clean kitchen towel. Squeeze and wring the towel to remove the liquid.Another option is to press the wet spinach into a sieve over the sink., Make sure your hands are clean and free of jewelry. Place the ground meat in the bowl \ufb01rst, and press it down in the middle to form a well. In the well, put the spinach, garlic, egg, milk, bread crumbs, Parmesan cheese, salt and pepper, and all but three tablespoons of the chopped onion.Add a small dash of hot sauce, if desired.Since youre working with youre hands, wash them well \ufb01rst, with soap and a nail brush. If you wear any rings, remove them and set them aside somewhere safe (away from the sink drain).You may want to crack the egg into a separate bowl before adding it, to ensure that no egg shell gets into the meat mixture. Preheat the oven to 400\u25e6 F (204\u25e6 C). Mix the meat mixture well. Make twelve large meatballs, approximately 11 2 each., Place the meatballs on a nonstick cookie sheet or shallow baking pan. Drizzle extra-virgin olive oil over the meatballs.Cook the meatballs for twenty to twenty-eight minutes, until they are no longer pink., Add butter and a drizzle of extra-virgin olive oil to a small sauce pot over medium heat. Allow the butter to melt.Pour in the mushrooms and/or chopped onion. Cook until the vegetables are tender, about two minutes., Whisk in the \ufb02our, and cook for one minute. ... ... ... Gold Defrost the spinach. Drain defrosted spinach. Combine the ingredients in a large mixing bowl. Form the meatballs with your hands. Cook the meatballs in the oven. Sautee mushrooms and/or onions. Add \ufb02our, milk and stock. Mix in cheese and seasonings. Combine meatballs with sauce. Serve meatballs with spaghetti squash. Char broccoli in the oven. Serve meatballs with starch. Model Defrost the spinach. Drain the spinach. Combine the spinach and other ingredients in a large bowl. Make the meatballs. Cook the meatballs. Make the sauce. Make the sauce. Serve the meatballs and sauce. Make the spaghetti squash. Cook the spaghetti squash. Serve the meatballs. ROUGE2-F1 25.74 Document (ID #193) While such snacks are not necessarily healthy, they do tend to be lower calorie than other candy options. If you want to buy snacks in the theater, these are generally your best choice of candy. In addition to being lower calorie than other varieties, they are chewier. This means they\u2019re more dif\ufb01cult to eat. If you take longer to \ufb01nish your candy, you\u2019re less likely to return to the concession stand for a second helping.Twizzlers have the lowest calorie count, containing only about 165 calories. Milk duds contain 370 calories, while Starbursts contain 480 calories. Keep in mind, if you\u2019re on a speci\ufb01c diet plan that restricts sugar, this is not the best route for you. While these snacks are lower calorie, the bulk of the calories come from sugar.; , Many movie theaters now sell protein bars at the counter. You can also sneak one in from a nearby gas station or drug store. Lower calorie protein bars may contain some ingredients you\u2019re craving, like nuts and chocolate, while still having some nutrients.Remember to read nutritional labels. Some protein bars may be misleading. In general, it\u2019s best to skip bars with a lot of added sugar. Swedish Fish are much lower in sugar than the average movie snack. They also tend to have a tougher texture, making chewing them dif\ufb01cult. If you slow down the pace of your eating, you will eat less overall.A serving of Swedish Fish is about 19 pieces, roughly a large handful, and contains only 140 calories and 29 grams of sugar. Movie theater containers tend to be smaller, so you may \ufb01nd a container that has a single serving. If you can\u2019t \ufb01nd a smaller container, try splitting one with a friend. As it can be tricky to \ufb01nd candy that\u2019s lower calorie at ... ... ... Gold Go for Twizzlers, Milk Duds, or Starbursts. Buy a protein bar. Look for Swedish Fish. Ask about the theater\u2019s food policy. Model Buy chewier candy. Try a lower calorie protein bar. Go for Swedish Fish. Find a smaller container. Bring your own snacks. ROUGE2-F1 19.51 Document (ID #189) Never interrupt, shout over, or insult another person (even if you feel that they really, really deserve it). Religion and government are separate, and religion and abortion rights are separate. Condemning someone\u2019s entire religion is cruel and divisive. Not all religious people are anti-choice; for example, it is possible for some self identi\ufb01ed Christians to be pro-choice. Even if the person in front of you is a \ufb02aming bigot, think of the nice religious people, and don\u2019t insult their religion. Pro-choice people have long pointed out how \u201dbaby\u201d and \u201dunborn child\u201d are manipulative ways to refer to an unwanted fetus. But calling all fetuses \u201dclumps of cells\u201d is also an insensitive use of rhetoric, because there are some fetuses that are truly loved and wanted, and to those parents the fetus is far more special. Stick with the factual term fetus. Dismissing fetuses as mere \u201dclumps of cells\u201d can be deeply upsetting to people who miscarried a loved, wanted fetus.Don\u2019t accidentally minimize the hardship of losing a beloved fetus. Transgender men, nonbinary people, and some intersex people of any gender can also get pregnant. Be sure not to step on the toes of transgender people as you \ufb01ght for the rights of everyone with a uterus. You are not obligated to change anyone\u2019s mind, nor do you have to keep talking to someone who says very rude or upsetting things. This may be personally upsetting to you. Say \u201dI don\u2019t want to talk about this anymore,\u201d \u201dI need a break,\u201d or \u201dI need to get some air\u201d if you can\u2019t keep calm. Any decent person will respect your wishes. If the person makes you feel unsafe, leave right away. Call police if you are being threatened or stalked. Gold Be polite and courteous when defending your beliefs. Never insult someone else\u2019s religion. Don\u2019t manipulate rhetoric. Remember that not all pregnant people are women. Take a break if you need to. Model Don\u2019t demonize the other person\u2019s religion. Don\u2019t call all fetuses \u201dclumps of cells.\u201d, Stay calm. ROUGE2-F1 12.24 Document (ID #272) Instead of eating a ton of sugar, which is tempting to do when you\u2019re tired, try eating complex carbohydrates and protein. Sugar can give you a rush of energy, but then your energy will crash. Complex carbohydrates, on the other hand, offer more sustained energy, especially when you pair them with protein.For instance, try some natural peanut butter on whole-wheat bread or a piece of fruit with a slice of cheese. Dehydration can lead to fatigue. Therefore, staying hydrated will help keep your energy up. Try sipping on water throughout the day to make sure you get enough. The Institute of Medicine recommends that men drink 13 cups and women drink 9 cups of water every day.Other liquids can contribute to your water intake, such as juice and coffee. However, drinking too much juice can pack on extra calories and cause a sugar crash, so try to limit your intake. Also, though you can count caffeinated beverages, you shouldn\u2019t make them the majority of the liquid you drink in a day., Keeping something in your mouth, such as a piece of gum, can help increase your alertness. The best time for this type of trick is when you\u2019re in a meeting that you absolutely need to stay awake for.",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 36,
                    "type": "Table"
                }
            },
            {
                "content": "Table I.18: Generated summaries by PEGASUSLARGE (HugeNews) on WikiHow sampled by ROUGEL-F1.\nWikiHow Document (ID #241) No matter what size the paint spill, carefully use a putty knife or any sort of \ufb02at tool to scoop up paint that is sitting on top of the carpet, not yet soaked down into the \ufb01bers.Scoop up as much excess paint as you can without spreading the paint around. Wipe off the collected paint with a paper towels, and throw the towels away in the garbage.; , Use a paper towel or clean rag to blot and absorb as much wet paint as you can.Lift your towel up and down and gently dab at the paint. Adjust the towel so you are always dabbing at the paint with a clean section. Dont rub or try to scrub the paint out; that will merely spread the paint around and push it deeper into the carpet. Mix a cleaning solution of 1 cup of warm water with 1 4 - 1 teaspoon of mild dish soap.Use another clean towel to drip on a small amount of cleaning solution onto the paint. Let the cleaning solution sit on the carpet for a few of minutes., After the solution has had some time to soak on the paint, dip a clean paper towel or rag into the cleaning solution and continue to blot at the wet paint.Always adjust the cloth so you are blotting with a clean section. Keep blotting at the stain until your cloth no longer absorbs any of the paint.Blot from the outside of the paint, moving inward; this will prevent you from spreading out the stain even further.If the stain still remains, repeat the cleaning process with the soapy water, or move on to using vinegar to blot at the paint.Vinegar works well with removing water-based paints like latex paint., Once the paint stain is removed, use a ... ... ... Gold Collect excess paint. Blot the paint with a clean cloth. Mix a cleaning solution. Blot the paint with the cleaning solution. Dry the area. Call in a professional. Model Scoop up excess paint. Blot at the paint with a paper towel. Apply a cleaning solution to the wet paint. Blot at the paint with a clean cloth. Dry the carpet. Call a professional carpet cleaner. ROUGEL-F1 59.37 Document (ID #150) This area does not need to be perfectly square or rectangular unless you are planning on using sod instead of simply replanting your grass. Drive wooden or plastic stakes around the area to be leveled.; , Attach strings to stretch between the stakes, a few inches above the ground. Attach a string level to the strings to determine the high point. This will usually be your starting point and the rest of the ground will be brought to here, but you can also lower some ground if that makes more sense for your project. Using a tape measure and your level, adjust the strings until you can see how much height needs to be added or taken away across the area you are adjusting. Keep in mind that you may want to adjust the grade of your land to help combat drainage problems. Your land should be graded 1\u201d for every 4\u2019, leading away from your home. If you are leveling a small area and it is mostly level, you probably will not need to remove the grass. However, if you have a large area and a lot of leveling to do, removing the grass will be much easier. A simple shovel is all that\u2019s necessary for a reasonable amount of space. Depending on how much ground you have to cover and what will be going on the ground afterwards, you\u2019ll need to level your land with differing mixtures of soil, sand, and compost/manure fertilizer. If you want to grow grass in this area, the cover will need to be nutrient rich. If you are simply wanting to place a small shed or pool, soil and sand will do just \ufb01ne. Use a garden rake to spread the material evenly, checking using your level and a measuring tape to make ... ... ... Gold Stake off your area to level. Use a string level. Adjust the strings. Adjust for grade. Remove the grass if necessary. Add your ground cover. Spread the topsoil. Tamp the soil. Let it settle. Spread your seeds. Lightly cover with more soil. Water lightly. Reseed as necessary. Alternatively, buy sod. Model Determine the shape of the area to be leveled. Set a string level. Adjust the string level. Decide if you will need to remove the grass. Level the land. Compact the soil. Wait for the soil to settle. Purchase the grass. Plant the grass. ROUGEL-F1 29.79 Document (ID #266) For the best texture and taste, insist on meat that is organic and/or free of arti\ufb01cial hormones.Specify that you want a raw pork belly that hasnt already been cured or sliced. For a fattier bacon, ask for meat that came from the hogs belly and/or chest. For a meatier bacon, ask for meat that covered the hogs spare ribs.Whole pork bellies typically weigh between 10 and 12 pounds, but if you are experimenting with cure recipes, ask for a smaller cut in case you end up not liking your concoction.; , If you purchased your pork belly from a commercial vendor other than a farm and plan on curing it as soon as you get home, skip this step, since the meat has already been chilled. But if you buy direct from a farm, ask the farmer how long ago the hog was slaughtered, because you need to chill your meat within 24 hours of that time.In either case, bring along a cooler stocked with ice to keep the pork belly relatively cool on the ride home if it takes you more than half an hour.Once home, place the pork belly in a sealable container to keep your fridge clean and prevent cross-contamination with other items. For a quicker chill, set the container inside the fridge wherever it is coldest. To further reduce the chance of cross-contamination, set it in the meat compartment, away from other products. Keep the refrigerators temperature at 40 degrees Fahrenheit and let the pork belly sit until its core temperature drops to 42 degrees, which will help kill bacteria. If you are chilling multiple bellies, set each one in its own container rather than stacking the meat directly on top of each other. Either ask your butcher to do it for you when you purchase ... ... ... Gold Buy a fresh pork belly. Chill your meat. Cut the skin off. Decide on a curing agent. Mix a basic cure. Try a saltier cure with sage and thyme. Cure your bacon with honey. Rub the cure into the meat. Refrigerate the pork belly. Rinse your meat. Store your meat safely. Finished. Model Purchase a pork belly. Chill the pork belly. Slice the skin off the pork belly. Mix the curing salts. Soak the pork belly in the salts. Rub the pork belly with the curing salts. Let the pork belly rest. Slice the pork belly. Fry the pork belly. Slice the pork belly. ROUGEL-F1 27.18 Document (ID #38) Read books and learn all you can about dementia so you better understand what you are dealing with. There are a wide range of products available to help make both the dementia patient and the caregivers life easier.; , People with dementia are for the most part caring, smart, funny and always wanting to help others. They respond to kindness and they need a lot of patience. Sometimes people with dementia become anxious or angry. At these times diversion is most important. They like to be kept busy with games, crafts, singing, or just talking about the past. Their abilities will be different because the stage of their dementia is different. It will be hard for a mate not to grieve the loss of his/her mate of many years, or for the child not to grieve the loss of a loving parent. Attend support groups and seminars, and know you are not alone. For those who are caring for relatives suffering with dementia, there are a range of dementia products and dementia aids designed to make both carer and patients\u2019 lives easier. Many drug stores and medical supply shops may carry them and you can \ufb01nd sites on the web which carry a whole range of items speci\ufb01cally geared towards patients with dementia. These products cover a range of categories, such as memory aids for dementia, mobility support aids, products to improve bathing safety, assistive devices, nutritional items for speci\ufb01c needs, products to help deal with and manage incontinence, special clothing and aids for vision and hearing. Some of these products are designed to help daily life. Dementia products such as mobility aids and hearing devices provide essential day to day support and good nutritional products can provide valuable calories, nutrients and support a balanced diet, or act as a ... ... ... Gold Look for information. Be kind and patient. Use diversion. Find support. Consider buying dementia aids.",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 37,
                    "type": "Table"
                }
            },
            {
                "content": "Document (ID #185) this happened 5/6 years ago so my whole family every xmas day goes around to my aunties for celebrations. my cousin (of course) was there and he asked if i wanted to play cops and robbers. i accepted of course. now, next to the side of my aunts house is a little area with a small fence, a covered water tank and super duper sharp stones. my cousin (who was the cop) was gaining on me. i (tried) to jump over the fence, aaand i failed the jump and went crashing onto the gravel, my leg hitting the sharpest bit and, then the next thing i knew it had a nasty gash. Gold played cops and robbers with my cousin on xmas day, gashed my leg failing a jump Model i tried to play cops and robbers and ended up with a nasty gash in my leg. ROUGE1-F1 42.42 Document (ID #295) this happened about 3 years ago at my best friend\u2019s apartment. i had come over to meet our friend\u2019s new boyfriend and go out for a fun night of bar hopping and meeting up with more friends. i wanted to do my makeup and hair since i had showered before coming over and didn\u2019t have time prior. now, my best friend keeps her bathroom extremely hot since she has a pet snake and it was the middle of winter...so i started to kinda sweat and remembered that i forgot deodorant and i was two seconds from smelling like a hot dude in a sauna with very poor hygiene. i start panicking and getting even more sweaty when i notice the spray bottle on a small shelf. the outside of it was black and purple, i wrongly assumed it was safe to put on the ol pits and go crazy spraying them nice and good. i felt better, panic attack relieved and i come out of the bathroom. fast forward maybe 30 seconds and i start to feel like my under arm skin is slowly and mercilessly being ripped off. i\u2019m wincing in pain and the three of them are just like, what...the hell? i confess and tell what i did and my best friend just starts dying from laughter, then goes into a \ufb01t of concern because i need to get the crap off of my skin asap. i washed it off and had really really horrible chemical burns on my arm pits for about 10 days and couldn\u2019t put deodorant on for the duration while my skin healed. needless to say i\u2019m sure everyone loved my natural smell for that period of time... probably one of the more painful (mostly painful) and a little embarrassing moments of my life, ... ... ... Gold i sprayed home fragrance on my pits, burned my skin and smelled gross for over a week/uncomfortable discussion with my family doctor Model forgot deodorant in my best friend\u2019s bathroom and sprayed it on my arm pits. ROUGE1-F1 31.58 Document (ID #204) throwaway here for obvious reasons.. today my friends and i decided to go off-roading in nowhereland. we packed up all our stuff, made the roughly hour drive off to the mountains to make a \ufb01re, go \ufb01shing and just talk about life until we got too tired to stay any longer. we got everything packed up and brought along one of my friends\u2019 dog because she\u2019s awesome and loves the outdoors. the dog was \ufb02ipping out in the suv on the way to the path because she knew was a kick-ass day she was about to have breaking out of her normally lame, domesticated dog life. my friends decided to drink during the off-roading adventure, which was \ufb01ne because i volunteered to drive since i cannot drink alcohol (mouth is wired shut [long story but i can\u2019t drink alcohol for a while]) so we were playing it safe. the dog couldn\u2019t be any happier and was about to jump out of the truck (literally) when we got there so the dog\u2019s owner let her get out and run along side of us while we drove the dirt road up to the destination for the \ufb01re. as i was driving, the dog went in and out of vision, mostly biting the tires as most dogs do, playing around. the owner kept asking us (the two guys up front) if we could see her. we said yes, and kept driving. as i was driving at no more than 5-10mph along the dirt road, i could hear the dog biting at the tires playfully, but we just laughed it off bc we thought she was having fun. the horrible, seconds-long event that ensued was me feeling the dreaded \u2019double-thud\u2019 under the tires and heard the dog yelp in pain. i instantly stopped the ... ... ... Gold we were off roading, dog jumped in the way of the truck i was driving, i killed the dog by accident. Model i ran over my friends\u2019 dog with an suv while off-roading. ROUGE1-F1 24.24 Document (ID #297) i and a friend i will call tony were meeting two other friends (steven and logan) for dinner before our upcoming gaming session. steven and logan arrive at smash burger about a minute before us. we beheld a sight that will forever be ingrained in my mind. i just wish i could have memorialized it for posterity. it had been snowing that day, and there were 5 inches already on the ground. steven and logan pulled into the parking lot in his truck. logan gets out of the truck but slips and falls on a patch of ice covered under the snow. steven walks around the truck to see where logan disappeared. he was greeted by the site of a baby seal \ufb02opping on the ice \ufb02oor struggling to get up. when tony and i pulled in, steven was dragging logan across the parking lot to a drier patch of ground. he was regaining his dignity just as steven was telling us the story. i just wish i had the sense to capture the moment. but, my mind was still processing what i was seeing. dammit, this almost surpasses steven\u2019s own [embarrassing \ufb02abalanche of 2009](http://www.reddit.com/r/askreddit/comments/977hp/anyone with pictures or video of a 400lb man/) Gold went to meet friends for dinner. was met with the sight of a snow eel being dragged over a patch of ice. laughed too hard to take picture. Model friend dragged a baby seal across a parking lot covered in snow. ROUGE1-F1 20.00 Document (ID #213) obligatory \u201dnot today\u201d but last thursday night/friday morning in the wee hours. this is a loooong one so bear with me. not throw-away but the embarrassment is real. i have an rv. for thanksgiving week, my wife, daughter, and i traveled to myrtle beach and stayed in an ocean-side spot. this year, my wife\u2019s best friend from childhood and her son \ufb02ew down and stayed with us making it a bit more crowded than usual but we enjoyed it. wife and i slept in our bed in the back, kids slept in the bunks, wife\u2019s friend slept in the pull out in the living room. worked out just \ufb01ne! \ufb02ash to late thursday night. i woke up not feeling great. a bit nauseous, stomach doing back\ufb02ips, kind of loopy. it got to the point that i almost woke my wife to get her to grab a garbage can because i didn\u2019t know if i\u2019d make it to the bathroom to puke if it came to that. i eventually slipped back to sleep... only to be awoken by a strident, loud beeping. **alarm!** my sleepy brain \ufb01rst tried to convince me it was just an alarm clock but i popped up and reoriented myself and realized it was the combination carbon monoxide/propane detector going off. i quickly popped out of bed and got down on my hands and knees to press the silence button \u2013 the alarm is mounted on side of the bed frame directly below me \u2013 while my brain tried to \ufb01gure out what was going on. my immediate thought was \u201dfalse alarm\u201d. these things are (supposedly) notoriously buggy and tend to deteriorate over time. i pulled it out and groggily read everything on it but couldn\u2019t really tell what it was complaining about. as i continued to ... ... ... Gold propane gas alarm thought we were all going to die... from my own apparent \ufb02atulence.** Model i almost killed my wife\u2019s best friend with carbon monoxide poisoning. ROUGE1-F1 7.41",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 38,
                    "type": "Table"
                }
            },
            {
                "content": "this job is to operate the ghost train carts as they go in one door and come out the other, while allowing customers on. using a operating desk, with three buttons on it, i am able to control when cars go in, when the ones in the middle can be moved up and when the ones that have just come out the bottom (with people in them) can be moved up once the customer has left the cart. so...when the customer comes out the bottom door of the ghost train, they are about a 20ft gap away from due to the space for the other carts to take up. because of this i try to make them leave their cart asap to prevent another coming out and crashing into them, and some times i\u2019ll press the button to control that cart just a tad to move it so they get the idea to move out quickly. one day a woman comes out the bottom of the ghost train....and just sits there. it\u2019s a busy day, im trying to deal with a lot of customers, and i have 6 other people breathing down my neck trying to get on. i tell them i need to get this woman off before they can go on. so, i hit the bottom and shunt her car while she\u2019s only starting to try and sit up, she looks shocked at me. i do it again, she looks even more shocked. i start staring at her, implying \u201dplease hurry up. you\u2019re taking your time\u201d and she slowly moves her way out of the car, grabbing the bars around her and slowly ... ... ... Gold i tried to get a woman out of her ghost train cart quickly by shunting it, i forgot she was disabled. Model i shunted a woman with cerebral palsy out of a ghost train. ROUGE2-F1 19.35 Document (ID #259) yesterday, i tried to cook in a bit of a rush so i could get to work on time. i set the heat to high to heat up my noodles quickly, and in my haste i didnt notice that some grease had spilled on the eye earlier in the day. i left the noodles to cook, and went to the bathroom. i come back, and the pot is engulfed in \ufb02ames. i panicked, moved the pot off of the stove, and put out the \ufb01re. after that, i opened up the doors in the house to let it air out so the \ufb01re alarm wouldnt go off. now, i had two of my familys dogs locked up so they wouldnt run out. but, i couldnt \ufb01nd the third. i had assumed that he was in my sisters room, and resumed trying to save the food. well, my sister came downstairs after a few minutes and was looking for her dog. turns out, the dog had escaped. so, i of course had to stop cooking so i could look for the dog. it took about twenty minutes to \ufb01nd him, and i ended up being late for work and getting chewed out by my manager. Gold tried to cook in a rush, almost burned my house down, had a dog escape, and ended up being late to work. Model tried to cook noodles, ended up setting the house on \ufb01re, and lost my sisters dog in the process. ROUGE2-F1 15.00 Document (ID #251) so a little background \ufb01rst: i\u2019m on my school\u2019s student council, and today was our junior prom assembly. we had a lot going on, much of it relying on a fast internet connection to work (live cameras streamed over appletv, etc.) we realized that as everyone settled into the auditorium, we\u2019d have a large portion of the student body connecting to the school wi\ufb01, and we were worried about slowdown. our live camera was basically useless without a good internet connection, so i had the bright idea to hook up the appletv to my personal hotspot. it connected automatically, i ran the feed for a few seconds, worked like a charm, so we moved on. assembly starts, about 15 minutes in and everything is going great. i\u2019m backstage when i see our principal run up to microphone and tell everyone to exit the school immediately, as there is a potential terror threat in the area (i live in an area where things like this never happen; we\u2019ve had one lockdown in my 10 years of schooling here) obviously, everyone\u2019s more than a little worried. everyone \ufb01les out, myself included. as i\u2019m standing outside, a friend walks up and tells us what he found out from a teacher. one of the students, while trying to connect to the wi\ufb01, had \u201ddiscovered\u201d a network... my network: \u201dtaliban secure communications.\u201d needless to say, the student went to a teacher, things escalated, and the school ordered a full evacuation. i talked to the principal (a very, very awkward conversation) and got everything sorted out. everyone\u2019s parents are panicking, checking their kids out of school now. my personal hotspot is now called \u201di love school.\u201d Gold turned on personal hotspot with the name \u201dtaliban secure communications\u201d for an assembly, school got evacuated because of a supposed terror threat. Model i connected my school\u2019s live camera to my personal hotspot, one of the students found out and evacuated the school. ROUGE2-F1 4.88 Document (ID #228) unlike the majority of these, this occurred about 20 minutes ago i am a pc gamer, which means i have a desk with tons of junk on it. sometimes i will walk in and grab a snack to eat before i start anything, which this time turned out to be a nice bag of chipsticks. i sit down and was so excited to eat them that i teared open the bag. apparently these have a very low tolerance to force, so as i rip them open the i watch each individual yellow stick of glory \ufb02y everywhere. these not only landed in all the small keyboard gaps but in every little gap imaginable. obviously before i moved i ate each individual stick to try and minimise the situation. so im now sitting here typing this up with a tiny dog vacuum trying clean up this mess. Gold back of the packet doesn\u2019t state the clock inside Model i ripped open a bag of chipsticks and they \ufb02ew everywhere ROUGE2-F1 0.00 Document (ID #29) this started two days ago and it followed up today. so we had a new sales guy start at the of\ufb01ce. i handle most of the i.t. helpdesk/network admin stuff as well as work in sales (its a private company). i was training him on our systems on his \ufb01rst day and it got to around noon and a little bit past so we went out to lunch. chipotle! we\u2019re talking about stuff getting to know each other. people always tend to ask questions about college, family blah blah blah. he asks me if i had any siblings, now this question is always tricky. well, my sister died 7 years ago, and i\u2019m a pretty chill guy so i don\u2019t like to bring it up and be the debby downer so i usually do a little white lie. if i have no connection whatsoever back to my real life i say 1, if not i say 2. so i said i have 2, i **was** the middle child growing up, (haven\u2019t lied yet). hbu? he goes on saying he has an older sister.. nice dodged",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 39,
                    "type": "Table"
                }
            },
            {
                "content": "the bus to take us home when a lunch/dinner lady who was replacing the bin liners came up to us and asked if we\u2019re twins (duh). note: she doesn\u2019t know/understand english very well. her: are you twins? me: yes her: are you from saudi? me: no, africa (my mothers indian however my fathers half tanzanian half arab so to make things easy i just say i\u2019m african since its more plausible as i have afroish hair) her: ah, you come here to study? me: yes her: i see you around college and yeah me: yeah her: are you going to university? me: yes her: how long have you been here? me: i\u2019m in my second year her: so you stay at your cousins? (this is where i thought huh i thought she asked my ethnicity and the situation was getting really awkward) me: yes then she left and i realised i made her think i\u2019ve come over to england to study and am staying at my cousins when in fact i was born in england and have stayed here my whole life... throughout the ordeal my twin was sat quiet in his phone -.- edit: sorry for the formatting, i\u2019m posting on my phone. Gold made a lunch lady believe i\u2019ve immigrated to study and stay at my cousins because of how awkward the situation was when in fact i\u2019ve lived in england my whole life Model made a lunch lady think i was born in england and have stayed here my whole life. ROUGEL-F1 44.00 Document (ID #224) so this happened last night and proceeded into today. went out with this girl for drinks, we started fooling around. went back to my car and she went ape-shit on my neck. didn\u2019t notice anything at the time, woke up in the morning with a nice bruise on my neck. went to work in a very corporate setting. thought that my dress shirt buttoned all the way up would be enough to conceal it but nope. boss inquires about the bruise, i say \u201dyeah i don\u2019t know where i got that, they keep popping up all over my body\u201d. i then proceeded to show him bruises on my arms (i was trying to climb trees whilst drunk over the weekend) as a cover up. boss became concerned and said i need to see a doctor right away as it could be something more serious like leukemia. he has given me next monday off to see the doctor... Gold girl attacked my neck, boss sent me to the doctor. Model went out with a girl, she ape-shit on my neck, woke up with a bruise, tried to cover it up by showing boss bruises on my arms. ROUGEL-F1 21.05 Document (ID #128) a few days ago me and my younger cousin (male) were playing with nerf guns shooting eachother around the house. being older and more experienced with weapons i got him good quite a few times and we were having a great time. his family had bought some knock-off nerf darts that seemed like a great deal, until they arrived and had solid tips. so instead of feeling like a mild \ufb02ick these feel like a very strong \ufb02ick. now so far these darts had caused no real injuries, just a bit more pain than your average nerf dart. then i foolishly decided to bring out the big guns, namely a pipe i used as a blowgun that was the perfect diameter for nerf darts. i have used blowguns for a good few years and am pretty good with them in both accuracy and power, so i knew to regulate my power when shooting at people. it was still more powerful, so in an attempt to draw my \ufb01re and catch me ammoless my cousin held a cowboy hat out from behind a doorway for me to shoot exactly like in a western. so seeing as i\u2019m shooting an innanimate object i took a deep breath and lined up my shot. i could see my other even younger cousin (female) in the background watching me but due to my blowgun experience i assumed she\u2019d be \ufb01ne. so i let this breath go and instead of the thump of it hitting the hat my cousin crumples and runs outside crying. i follow her and she is crying and saying i shot her and her eye is blurry and its not getting better. she was being very brave given the context and i tried to console and reassure her it\u2019d be \ufb01ne so ... ... ... Gold while playing nerf with one cousin i accidentally shot another in the eye causing her iris to bleed (which if jerked too much could cause blindness) with a fake nerf dart shot powerfully from a blowgun. Model shot my cousin in the eye with a blowgun, ruined their holiday, stopped them enjoying my mothers birthday party and ruined my friendship with my cousin all due to a single unlucky shot with knock-off nerf darts. ROUGEL-F1 18.92 Document (ID #175) regret and guilt are two of the worst things that one can feel. unlike most tifu\u2019s this one actually occurred today. me and my little brother who is 10 years old were playing football. sometimes my little brother is really annoying and this was a prime case. he decided to kick my shin, now i\u2019m notorious for being quick to anger and can be violent, i know not one of my better traits. so i pushed him and to be honest i pushed him pretty hard. he fell and presumably stuck out his left arm to break his fall. however he instead landed on his wrist. at \ufb01rst it looked like he was ok but then he cried out in pain and screamed and shouted very loudly. at \ufb01rst i though he was bluf\ufb01ng because he often was melodramatic and did things like this. however after 5 minutes it became apparent that he was completely serious. i lifted him and sat him down on a soft chair. since i have done \ufb01rst aid training i applied ice to his wrist. then i called my mum and told her everything including that it was all my fault. i decided that lying wasn\u2019t my best option and just spilt the beans and told her everything. she was really pissed off and told us that she would be home in ten minutes. when i heard the knock on the door i knew it was not going to be good. she was really angry and shouted at me before looking at my brother. she saw his wrist and turned pale. it was black or horrible looking like something you could \ufb01nd on google images but it was de\ufb01nitely swollen. she decided to take him to a&e, on the way out she grilled me about ... ... ... Gold i managed to accidentally break my brothers arm. my aims to buy a car before uni have faced a major setback and my hopeful future in a medical",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 40,
                    "type": "Table"
                }
            },
            {
                "content": "Table I.22: Generated summaries by PEGASUSLARGE (HugeNews) on BIGPATENT sampled by ROUGE1-F1.\nBIGPATENT Document (ID #235) the left portion of \ufb01g1 and 2 shows part of a processing device in the form of a punching mechanism or installation head 2 that is used for pressing nuts m into a piece of sheet metal ( not shown ). the nuts m are fed by a sorting unit ( not shown ) through the feed device 4 by means of a \ufb02exible or solid feed line 26 designed , for example , as a hose . the punching mechanism 2 features a two - part 6a , b guide piece 6 that is \ufb02exibly braced against an upper press part ( not shown ) that moves up and down ; this press is also provided with a feed channel 8 . a sliding plunger 10 in the feed channel 8 is used to drive a nut m through the feed channel 8 during a downward movement of the upper press part and to press it into the sheet metal ( not shown ). during the downward movement of the plunger 10 , the nut m is held in position by two retaining catches 12 indicated in \ufb01g2 . furthermore , \ufb01g1 shows a sensor device 14 which determines the presence of a nut m . the remaining design of the punching device 2 is irrelevant for understanding the present application and will not be described further . the feed device 4 consists of a nozzle 20 in the form of a plate - like component which is equipped with a nozzle channel 22 going through it for the nuts m . the nozzle channel 22 features a region 24 of expanded cross section which holds one end section of the feed line 26 coming from the sorting unit . in the portion of the nozzle 20 including region ... ... ... Gold the invention describes a feed device that feeds assembly parts such as nuts , bolts , and rivets , from a sorting unit to the feed channel of a processing device for pressing or mounting of assembly parts to sheet metal . the feed device consists of a nozzle with a nozzle channel through it connected on one end to a feed line coming from the sorting unit . a protrusion of the nozzle can deploy into a recess opening of a guide piece of the processing device so that the nozzle channel will open laterally into the feed channel of the device . to connect the nozzle to the guide piece , two levers linked to the nozzle are used ; they are pretensioned by springs in a closed position surrounding the guide piece and can be moved manually , possibly with the assistance of a tool , into an open position . in this manner the feed device can be easily connected to the processing device and then detached from it . Model a feed device for feeding a processing device with a feed line , particularly for feeding nuts to a punching mechanism , features a nozzle equipped with a nozzle channel that is connected to the processing device . the nozzle is equipped with a clamping plate that holds the feed line securely in the nozzle . the nozzle is releasably connected to a guide piece of the processing device by means of two levers that are pivotably mounted on the nozzle . the levers are tensioned in the closed position by springs and can be moved into a connected position by the application of pressure . the connection between the nozzle and the guide piece is released by a release mechanism when the levers are moved into the connected position . ROUGE1-F1 55.94 Document (ID #298) it will be readily apparent to one skilled in the art that various substitutions and modi\ufb01cations may be made in the invention disclosed herein without departing from the scope and spirit of the invention . the term a or an as used herein in the speci\ufb01cation may mean one or more . as used herein in the claim ( s ), when used in conjunction with the word comprising , the words a or an may mean one or more than one . as used herein another may mean at least a second or more . the term animal as used herein refers to any species of the animal kingdom . in preferred embodiments it refers more speci\ufb01cally to humans , animals in their wild state , animals used as pets ( birds , dogs , cats , horses ), animals used for work ( horses , cows , dogs ) and animals which produce food ( chickens , cows , \ufb01sh ), farm animals ( pigs , horses , cows , sheep , chickens ) or are themselves food ( frogs , chickens , \ufb01sh , crabs , lobsters , shrimp , mussels , scallops , goats , boars , cows , lambs , pigs , ostrich , emu , eel ) and other animals well known to the art . the term effective amount as used herein is de\ufb01ned as the amount of the composition required to produce an effect in a host which can be monitored using several endpoints known to those skilled in the art . in a speci\ufb01c embodiment , these endpoints are surrogate markers . the term feed conversion ef\ufb01ciency as used herein is de\ufb01ned as the ... ... ... Gold the intramuscular electroporated injection of a protease - resistant growth hormone - releasing hormone cdna into rat dams at 16 days of gestation resulted in the enhanced long - term growth of the f1 offspring . the offspring were signi\ufb01cantly heavier by one week of age and the difference was sustained to 10 weeks of age . consistent with their augmented growth , plasma igf - i concentration of the f1 progeny was increased signi\ufb01cantly . the pituitary gland of the offspring was signi\ufb01cantly heavier , and contained an increased number of somatotropes and lactotrophs , and is indicative of an alteration in cell lineages . these unique \ufb01ndings demonstrate that enhanced ghrh expression in pregnant dams can result in intergenerational growth promotion , by altering development of the pituitary gland in the offspring . Model the present invention relates to the use of growth hormone releasing hormone and growth hormone secretagogue receptors in the manufacture of medicaments for the treatment of growth de\ufb01ciencies in newborn animals .",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 41,
                    "type": "Table"
                }
            },
            {
                "content": "Table I.23: Generated summaries by PEGASUSLARGE (HugeNews) on BIGPATENT sampled by ROUGE2-F1.\nBIGPATENT Document (ID #127) in \ufb01g1 , a container in the form of a tube 100 is provided , the tube 100 comprising a chamber 102 having a wall 104 , an upper portion 106 , and a lower portion 108 , the lower portion 106 holding a freeze - dried material comprising a biological sample ; the freeze - dried material is hereinafter referred to as a cake 110 . typically , the cake 110 is in a disc - shaped form . the tube 100 comprises a physical structure 112 to inhibit movement of the freeze - dried biological sample from the lower portion 108 of the chamber 102 to the upper portion 106 of the chamber 102 . the tube 100 comprises a \ufb01rst end 114 , which may initially be open in order to enable insertion of the material prior to freeze - drying , and subsequently closed with a seal ( not shown ). the external dimensions of the tube 100 may be \ufb01xed at an industry standard for ease of handling , for example , automated handling by existing laboratory equipment . typically , the tube 100 is 8 mm in diameter and 18 . 3 mm long . typically , the internal dimensions of the tube are large enough in both diameter and depth to accommodate standard dispensing tools , for example pipetting needles in automated dispensing systems ; this has the advantage that there is no need for readjustment or calibration of existing tools . the biological sample may comprise an enzyme , such as a polymerase , reverse transcriptase or any other enzyme , blood , tissue , serum or any other biological substance . in \ufb01g2 a and 2 b , a tube 100 a is provided in which a physical structure 112 in ... ... ... Gold embodiments of the invention relate to a container holding , and a method of storing , freeze - dried biological samples . in particular , there is provided a container holding a freeze - dried material comprising a biological sample , the container comprising a chamber having an upper portion and a lower portion , the chamber comprising a wall and the lower portion being \ufb02uidly connected to the upper portion such that , when liquid is received at the upper portion , the received liquid can pass to and accumulate in the lower portion . further , the freeze - dried material is located in the lower portion , and the container comprises a physical structure in the form of a stop protruding inwards from the wall , the physical structure being for inhibiting the freeze - dried material from moving from the lower portion of the chamber to the upper portion of the chamber . this ensures that the biological sample can be kept in a lower portion of the tube , ensuring that , when the biological sample is subsequently reconstituted by inserting water , substantially all of the biological sample is dissolved . Model a container for containing a biological sample , the container comprising : a chamber having a wall , an upper portion , and a lower portion , the lower portion holding a freeze - dried material comprising a biological sample ; and a physical structure to inhibit movement of the freeze - dried biological sample from the lower portion of the chamber to the upper portion of the chamber . the invention also relates to methods of preparing a biological sample , and to systems and methods for dispensing a biological sample . ROUGE2-F1 38.28 Document (ID #155) the automated edge \ufb01nishing apparatus and method of the present invention comprises hardware components and associated software , providing the ability to learn a trajectory of a workpiece by &# 34 ; guarded moves ,&# 34 ; machining the workpiece by moving the workpiece along the trajectory and controlling the trajectory in real time . similar workpieces can be machined from the learned trajectory without expensive \ufb01xtures by the sensors and software locating the tool and adjusting the trajectory . the invention is useful for performing edge \ufb01nishing on intricately designed workpieces or workpieces with close edge \ufb01nishing tolerances . the con\ufb01guration of the preferred system is shown in \ufb01g1 and 2 . the preferred invention comprises an xy table 10 , a force sensor 12 , cutting tool 14 , xy table position control hardware 32 , and a control system as shown in \ufb01g2 . the preferred xy table 10 comprises a daedal series 200000 , or the like , powered by tlm120 linear motors from trilogy systems corporation , or the like . the linear actuators comprise three - phase brushless linear motors , with a stationary magnet track and a moving coil assembly . the linear motors are driven by dc ampli\ufb01ers , such as pwm ampli\ufb01ers . both table axes are equipped with incremental encoders with a resolution of 12 , 700 counts / inch . the innermost level of table control is position control , by a controller 32 such as a dmc - 520 motion controller from galil motion control , as shown in \ufb01g2 . this board controls position of both table axes using proportional plus derivative ( pd ) control action . the resulting position - controlled table is the starting point for the force control system discussed below . a ... ... ... Gold the disclosure is directed to an apparatus and method for automated edge \ufb01nishing using hybrid position / force control of an xy table . the disclosure is particularly directed to learning the trajectory of the edge of a workpiece by &# 34 ; guarded moves &# 34 ;. machining is done by controllably moving the xy table , with the workpiece mounted thereon , along the learned trajectory with feedback from a force sensor . other similar workpieces can be mounted , without a \ufb01xture on the xy table , located and the learned trajectory adjusted Model an apparatus and method for edge \ufb01nishing comprising a position - controlled linear table , a cutting tool , a force sensor , and a control system . the position - controlled table allows the workpiece to be moved past the cutting tool . the table is equipped with two linear motors and associated encoders for position control . the control system can be programmed to allow the workpiece to be cut from a learned trajectory in real time . the cutting tool can be programmed to cut the workpiece along the trajectory , or it can be programmed to cut the workpiece on a normal path . the system combines the two major elements of edge \ufb01nishing : position control and force control . the position control allows the workpiece to be moved past the cutting tool , while the force control allows the workpiece to be moved in the normal direction . the system components can be placed in modular form , allowing rapid upgrade and replacement of existing systems .",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 42,
                    "type": "Table"
                }
            },
            {
                "content": "Document (ID #185) referring to \ufb01g3 , one aspect of the disclosed system for connecting a sensor to a controller , generally designated 100 , may include a sensor 102 , an electronic controller 104 and a power source 106 , such as a battery ( e . g ., a 12 v automotive battery ). the system 100 may be connected to ground 108 , such as a vehicle chassis . the wiring inductance l w , wiring resistance r w and current noise source n of the system 100 may represent ground noise created by transient currents in the ground path of the controller 104 . in one aspect , sensor 102 may be a pedal feel emulator ( not shown ) that indicates a driver &# 39 ; s brake request and the controller 104 may be associated with a front right electric caliper ( not shown ) and may generate and communicate a braking signal to the caliper based upon signals received from the pedal feel emulator . the controller 104 may include resistors r 10 , r 11 , r 12 and capacitors c 6 , c 7 . the input to the controller 104 from the sensor 102 may be in the form of a single wire 110 that supplies a current . for example , a single pin connector may be used to connect the sensor 102 to the controller 104 . the use of a single wire connection between the sensor 102 and the controller 104 may provide several advantages , including reduced costs and manufacturing time . the current supplied by the wire 110 may be converted to a signal voltage by resistor r 11 , which may be \ufb01ltered by a low pass \ufb01lter 112 created by resistors r 10 , r 12 ... ... ... Gold a sensor to controller connection system including a power source , a controller in communication with the power source , and a sensor in commu- nication with the power source and the controller , the sensor including sensor electronics and a current source , the current source having a control input and an output , the control input being applied by the sensor electronics and the output being applied to the controller , wherein the current source controls an electric signal communicated to the controller from the sensor based upon the control input . Model a system including a sensor having a \ufb01rst terminal for receiving a \ufb01rst voltage and a second terminal for receiving a second voltage , the second voltage being a voltage greater than the \ufb01rst voltage ; a controller having a \ufb01rst terminal for receiving the \ufb01rst voltage and a second terminal for receiving the second voltage ; and a current source connected between the second terminal of the sensor and the \ufb01rst terminal of the controller . ROUGEL-F1 28.93 Document (ID #237) referring now to \ufb01g2 - 4 a preferred embodiment of the control system 10 of the present invention is shown mounted upon the underside of gun 11 and comprised of an emd 12 , electronic circuitry denoted schematically by box 13 , a battery 14 , and electrical switch trigger 15 . the illustrated gun is comprised of a forestock 22 which supports barrel 16 , receiver portion 17 located at the rear extremity of said barrel , magazine 18 , conventional trigger 19 with associated pistol grip 20 , and shoulder stock 21 which contains a compressed propellant gas . the emd is intended to produce reciprocating linear movement of a push rod 23 . the emd may be a solenoid , either of an in - line type or clapper or rotary type . alternatively , the emd may be a servo type device using an arm , lever or gear system to activate rod 23 . all such devices are characterized in that a pulse of electrical energy produces a controlled mechanical force , and the discontinuation of said pulse either produces a reverse force or permits interaction of a reverse force such as may be produced by a spring - biased conventional trigger . the emd may be secured to forestock 22 by brackets or removable fastening means . a push rod 23 , extending from said emd is slideably positioned by guide 25 mounted by bracket 35 beneath receiver portion 17 . in those embodiments wherein the emd is a solenoid , guide 25 may not be required . the length of rod 23 and its positioning by guide 25 is such as to cause the distal extremity 26 of rod 23 to contact trigger 19 . said distal extremity may be equipped with coupling ... ... ... Gold a system for controlling the \ufb01ring of a paint ball gun includes an electromechanical device that produces linear motion interactive with the conven- tional trigger of the gun . electronic circuitry produces pulses that activate the emd . the pulses are based upon control signals produced by either the timing of paint balls entering the \ufb01ring chamber of the gun or the detection of the presence of a single paint ball within the chamber . a manually operated electrical switch trigger activates the electronic circuitry . an adjustably predetermined number of paint balls will \ufb01re based upon each depression of the electrical switch trigger while minimizing the chopping of paint balls in the \ufb01ring chamber . Model a paint ball gun control system which permits selective \ufb01ring of paint balls in response to the depression of the trigger . the system includes an",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 43,
                    "type": "Table"
                }
            },
            {
                "content": "Document (ID #34) consider a set of objects which should be ranked on the basis of information about their bilateral relationships . similar problems arise , among others , in social choice theory xcite , sports xcite , psychology xcite , internet search xcite , and bibliometrics xcite . we discuss a universal version of the problem involving arbitrary preference intensities as well as incomplete and multiple comparisons . the main contribution of this paper is the presentation of an impossibility theorem : consistency requiring that if an object is ranked at least as high as another in two independent problems , then it is ranked as high as the other in the uni\ufb01ed problem , too and self - consistency a less known but intuitive property , introduced in xcite , which prohibits to assign a lower rank for an object with a better or equivalent performance than another can not be met simultaneously by any ranking method on the set of all problems . domain restrictions and weakening of the properties are also investigated in order to get some positive results . since self - consistency is an axiom dif\ufb01cult to debate , we can not demand consistency from a ranking method . at the \ufb01rst glance , it is a somewhat surprising and controversial result making the aggregation of ( incomplete ) preferences impossible . one may argue that the whole concept of paired comparisons - based ranking is \ufb02awed . however , loss of consistency may be regarded as a necessary sacri\ufb01ce for dimension reduction since the outcome should be an inherently transitive ranking even from intransitive data . this interpretation is reinforced by the connection between consistency and independence of irrelevant matches ( which requires the relative ranking of two objects to be independent of the outcome of ... ... ... Gold assume a set of objects is given with information about their bilateral relationships , allowing for incomplete and multiple comparisons as well as different preference intensities . an axiomatic approach is applied for the problem of ranking the objects . consistency requires the preservation of relative ranking if two sets of such preferences are aggregated . self - consistency assigns the same rank for objects with the same performance , furthermore , an object should be ranked strictly higher if it shows an obviously better performance than another . it is revealed that these two properties can not be satis\ufb01ed simultaneously . the impossibility holds under various restrictions on the domain . however , a positive result emerges if only the aggregation of ranking problems with the same comparison structure is allowed , for example , we have two round - robin tournaments . Model consider a set of objects which should be ranked on the basis of information about their bilateral relationships . similar problems arise , among others , in social choice theory , sports , psychology , internet search , and bibliometrics . we discuss a universal version of the problem involving arbitrary preference intensities as well as incomplete and multiple comparisons . the main contribution of this paper is the presentation of an impossibility theorem : consistency requiring that if an object is ranked at least as high as another in two independent problems , then it is ranked as high as the other in the uni\ufb01ed problem , too and self - consistency a less known but intuitive property , introduced in xcite , which prohibits to assign a lower rank for an object with a better or equivalent performance than another can not be met simultaneously by any ranking method on the set of all problems . domain restrictions and weakening of the properties are also investigated in order to get some positive results . ROUGE1-F1 48.61 Document (ID #289) machine learning methods are used widely within high energy physics ( hep ) . one promising approach , used extensively outside of hep for applications such as handwriting recognition , is that of support vector machines ( svms ) , a supervised learning model used with associated learning algorithms for multivariate analysis ( mva ) . developed originally in the 1960s , with the current standard version proposed in 1995 xcite , svms aim to classify data points using a maximal margin hyperplane mapped from a linear classi\ufb01cation problem to a possibly in\ufb01nite dimensional hyperspace . however this means svms , like other mva classi\ufb01ers , have a number of free parameters which need to be tuned on a case by case basis . this motivates a number methods for ensuring the classi\ufb01er is suf\ufb01ciently generalised such that when used on an unseen dataset the performance can be accurately predicted . in this paper a brief overview of svms is given in section [ sec : svm ] , with an example using svms shown in section [ sec : checker ] . generalisation is discussed in section [ sec : gen ] with an illustrative example of how this can improve performance given in section [ sec : check2 ] . consider the problem of linear classi\ufb01cation with the svm where the training set , xmath0 , is linearly separable . we de\ufb01ne a separating hyperplane given by xmath1 , where xmath2 , the weight vector , is perpendicular to the hyperplane , and xmath3 , the bias , determines the distance of the hyperplane from the origin ( \ufb01g . ) . a decision function de\ufb01ned by xmath4 is used to positively and negatively classify xmath5 , the points in the training set . without further constraint the ... ... ... Gold we review the concept of support vector machines ( svms ) and discuss examples of their use . one of the bene\ufb01ts of svm algorithms , compared with neural networks and decision trees is that they can be less susceptible to over \ufb01tting than those other algorithms are to over training . this issue is related to the generalisation of a multivariate algorithm ( mva ) ; a problem that has often been overlooked in particle physics . we discuss cross validation and how this can be used to improve the generalisation of a mva in the context of high energy physics analyses . the examples presented use the toolkit for multivariate analysis ( tmva ) based on root and describe our improvements to the svm functionality and new tools introduced for cross validation within this framework . Model support vector machines ( svms ) are one of the most promising methods for machine learning in high energy physics . svms aim to classify data points using a maximal margin hyperplane mapped from a linear classi\ufb01cation problem to a possibly in\ufb01nite dimensional hyperspace . however this means svms , like other mva classi\ufb01ers , have a number of free parameters which need to be tuned on a case by case basis . this motivates a number methods for ensuring the classi\ufb01er is suf\ufb01ciently generalised such that when used on an unseen dataset the performance can be accurately predicted . in this paper a brief overview of svms is given , with an example using svms shown in section [ sec : checker ] . generalisation is discussed with an illustrative example of how this can improve performance given in section [ sec : gen ] .",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 44,
                    "type": "Table"
                }
            },
            {
                "content": "(ID #294) the pure quantum states are traditionally associated with the wave function xcite or a vector in the hilbert space xcite . the mixed quantum states are described by the density matrix xcite or the density operator xcite . there exist several representations of quantum states in terms of the quasidis- tribution functions like the wigner function xcite and the husimi kano function xcite . the diagonal representation of quantum states was suggested in xcite ( see also xcite ) . it was studied and applied in xcite . in this representation , a quantum state is represented in terms of weighted sum of coherent - state xmath0 projectors . the properties of all the quantum - state representations considered are associated with the properties of the density operator which is hermitian , trace - class nonnegative operator . this means , in particular , that all the eigenvalues of the density operators must be nonnegative . in the quantum domain , the multipartite systems have a speci\ufb01c property connected with strong correlations of the quantum subsystems . this property provides the entanglement phenomenon xcite . in the diagonal representation of the density states , the weight function xmath1 is an analog of the probability - distribution function in the phase space . for some class of states , this function is identical to the probability - distribution function like in classical statistical mechanics . in xcite , the tomographic - probability representation of quantum states , where the quantum state is associated with the so - called symplectic tomogram , was introduced . the tomogram is a fair probability distribution containing the same information on quantum state that the density operator does ( or such its characteristics as the wigner or husimi kano functions ) . the aim of this work ... ... ... Gold the quasidistributions corresponding to the diagonal representation of quantum states are discussed within the framework of operator - symbol construction . the tomographic - probability distribution describing the quantum state in the probability representation of quantum mechanics is reviewed . the connection of the diagonal and probability representations is discussed . the superposition rule is considered in terms of the density - operator symbols . the separability and entanglement properties of multipartite quantum systems are formulated as the properties of the density - operator symbols of the system states . Model the diagonal and tomographic representations of quantum states are considered . the quantum states are represented in terms of the weighted sum of coherent - state xmath0 projectors . in the tomographic - probability representation , the quantum state is associated with the so - called symplectic tomogram . the tomographic - probability distribution is a fair probability distribution containing the same information on quantum state that the density operator does ( or such its characteristics as the wigner or husimi kano functions ) . the aim of this work is to \ufb01nd the explicit formulae realizing the connection of the diagonal and tomographic probability representations . the functions like the wigner function , husimi kano function and tomographic - probability - distribution function are considered as symbols of the density operators of a corresponding star - product scheme . the superposition rule given in terms of tomograms and in terms of weight functions of the diagonal representation where explicit kernels of the corresponding star - products are employed to obtain the addition rules for the tomograms and weight functions are considered . we discuss also the formulation of the separability and entanglement properties of composed system in the tomographic probability and diagonal representations . ROUGE2-F1 26.52 Document (ID #32) deep neural networks ( dnns ) have been receiving ubiquitous success in wide applications , ranging from computer vision xcite , to speech recognition xcite , natural language processing xcite , and domain adaptation xcite . as the sizes of data mount up , people usually have to increase the number of parameters in dnns so as to absorb the vast volume of supervision . high performance computing techniques are investigated to speed up dnn training , concerning optimization algorithms , parallel synchronisations on clusters w / o gpus , and stochastic binarization / ternarization , etc xcite . on the other hand the memory and energy consumption is usually , if not always , constrained in industrial applications xcite . for instance , for commercial search engines ( e.g. , google and baidu ) and recommendation systems ( e.g. , net\ufb02ix and youtube ) , the ratio between the increased model size and the improved performance should be considered given limited online resources . compressing the model size becomes more important for applications on mobile and embedded devices xcite . having dnns running on mobile apps owns many great features such as better privacy , less network bandwidth and real time processing . however , the energy consumption of battery - constrained mobile devices is usually dominated by memory access , which would be greatly saved if a dnn model can \ufb01t in on - chip storage rather than dram storage ( c.f . xcite for details ) . a recent trend of studies are thus motivated to focus on compressing the size of dnns while mostly keeping their predictive performance xcite . with different intuitions , there are mainly two types of dnn compression methods , which could be used in conjunction for better parameter savings . the ... ... ... Gold as the complexity of deep neural networks ( dnns ) trend to grow to absorb the increasing sizes of data , memory and energy consumption has been receiving more and more attentions for industrial applications , especially on mobile devices . this paper presents a novel structure based on functional hashing to compress dnns , namely funhashnn . for each entry in a deep net , funhashnn uses multiple low - cost hash functions to fetch values in the compression space , and then employs a small reconstruction network to recover that entry . the reconstruction network is plugged into the whole network and trained jointly . funhashnn includes the recently proposed hashednets xcite as a degenerated case , and bene\ufb01ts from larger value capacity and less reconstruction loss . we further discuss extensions with dual space hashing and multi - hops . on several benchmark datasets , funhashnn demonstrates high compression ratios with little loss on prediction accuracy . Model deep neural networks ( dnns ) have been receiving ubiquitous success in wide applications , ranging from computer vision to speech recognition . as the sizes of data mount up , people usually have to increase the number of parameters in dnns so as to absorb the vast volume of supervision . compressing the model size becomes more important for applications on mobile and embedded devices . with different intuitions , there are mainly two types of dnn compression methods , which could be used in conjunction for better parameter savings . the \ufb01rst type tries to revise the training target into more informative supervision using dark knowledge . the second type observes the redundancy existence in network weights , and exploits techniques to constrain or reduce the number of free - parameters in dnns during learning . in applications , we observe hashednets compresses model sizes greatly at marginal loss of accuracy for some situations , whereas also signi\ufb01cantly loses accuracy for others . after revisiting its mechanism , we conjecture this instability comes from at least three factors . \ufb01rst , hashing and training are disjoint in a two - phase manner , i.e. , once inappropriate",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 45,
                    "type": "Table"
                }
            },
            {
                "content": "Table I.27: Generated summaries by PEGASUSLARGE (HugeNews) on arXiv sampled by ROUGEL-F1.\narXiv Document (ID #248) stripped supernovae ( sne ) and long - duration gamma - ray bursts ( long grbs ) are nature s most powerful explosions from massive stars . they energize and enrich the interstellar medium , and , like beacons , they are visible over large cosmological distances . however , the mass and metallicity range of their progenitors is not known , nor the detailed physics of the explosion ( see reviews by xcite and xcite ) . stripped - envelope sne ( i.e , sne of types iib , ib , and ic , e.g. , xcite ) are core - collapse events whose massive progenitors have been stripped of progressively larger amounts of their outermost h and he envelopes ( \ufb01g . [ \ufb01g1 ] ) . in particular , broad - lined sne ic ( sne ic - bl ) are sne ic whose line widths approach 20,000xmath030,000 xmath1 around maximum light ( see below ) and whose optical spectra show no trace of h and he . for the last 15 years , the exciting connection between long grbs and sne ic - bl , the only type of sne observed accompanying long grbs ( for reviews , see xcite ) , and the existence of many more sne ic - bl without grbs raises the question of what distinguishes sn - grb progenitors from those of ordinary sne ic - bl without grbs . viewing angle effects are probably not the reason why those sne ic - bl did not show an accompanied grbs xcite and based the same radio upper - limits , only xmath2 1% of sne ib / c appear to be accompanied by grbs xcite . one promising line of attack is to investigate what sets apart sne ib ... ... ... Gold while the connection between long gamma - ray bursts ( grbs ) and type ib / c supernovae ( sne ib / c ) from stripped stars has been well - established , one key outstanding question is what conditions and factors lead to each kind of explosion in massive stripped stars . one promising line of attack is to investigate what sets apart sne ib / c * with * grbs from those * without * grbs . here , i brie\ufb02y present two observational studies that probe the sn properties and the environmental metallicities of sne ib / c ( speci\ufb01cally broad - lined sne ic ) with and without grbs . i present an analysis of expansion velocities based on published spectra and on the homogeneous spectroscopic cfa data set of over 70 sne of types iib , ib , ic and ic - bl , which triples the world supply of well - observed stripped sne . moreover , i demonstrate that a meta - analysis of the three published sn ib / c metallicity data sets , when including only values at the sn positions to probe natal oxygen abundances , indicates at very high signi\ufb01cance that indeed sne ic erupt Model stripped - envelope supernovae ( sne ) and long - duration gamma - ray bursts ( long grbs ) are nature s most powerful explosions from massive stars . however , the mass and metallicity range of their progenitors is not known , nor the physics of the explosion . in particular , broad - lined sne ic ( sne ic - bl ) are sne ic whose line widths approach 20,000xmath030,000 xmath1 around maximum light and whose optical spectra show no trace of h and he . for the last 15 years , the exciting connection between long grbs and sne ic - bl , the only type of sne observed accompanying long grbs , raises the question of what distinguishes sn - grb progenitors from those of ordinary sne ic without grbs . one promising line of attack is to investigate what sets apart sne ib / c * with * grbs from those * without * grbs to elucidate the conditions and progenitors of these two types of explosions . ROUGEL-F1 22.50 Document (ID #270) the alice experiment is dedicated to the study of the properties of qcd matter created in nucleus - nucleus collisions at lhc energies xcite . the inner tracking system in the alice apparatus is made of position sensitive detectors which have to handle several thousands tracks per unit of rapidity . the two innermost layers at 3.9 xmath10 and 7.6 xmath10 radii , respectively , constitute the silicon pixel detector ( spd ) . the spatial precision and hit ef\ufb01ciency of the spd are key parameters since they determine the alice capability of detecting particles with open heavy - \ufb02avour xcite . + the basic detector unit of the alice spd is the ladder , a two - dimensional silicon matrix of pxmath11n reverse biased diodes of dimensions 50 x 425 xmath12 , \ufb02ip - chip bonded to \ufb01ve read - out chips . each diode is connected to a cell of the front - end read - out asic via a pb - sn solder bump of 25 xmath13 diameter . the detector contains nearly 10xmath14 active cells in total . the read - out is binary . to reduce the material budget , the sensor thickness is limited to 200 xmath13 and the read - out chip wafers are thinned down to 150 xmath13 . further details can be found in xcite . + early prototypes of the alice spd elements , in the form of single - chip assemblies , were tested in high energy proton / pion beams at the cern sps in 2002 and 2003 . these assemblies were made with sensors of 200 xmath13 and 300 xmath13 thicknesses , while the read - out chips ( unthinned ) were 725 xmath13 thick . those beam tests were primarily aimed at evaluating the performance of ... ... ... Gold the two innermost layers of the alice inner tracking system are instrumented with silicon pixel detectors . single chip assembly prototypes of the alice pixels have been tested in high energy particle beams at the cern sps . detection ef\ufb01ciency and spatial precision have been studied as a function of the threshold and the track incidence angle . the experimental method , data analysis and main results are presented . d. eliaxmath0 , g. anellixmath1 , f. antinorixmath2 , a. badalxmath3 , g.e . brunoxmath4 , m. burnsxmath1 , i.a . calixmath5 , m. campbellxmath1 , m. casellexmath4 , s. ceresaxmath1 , p. chochulaxmath1 , m. cinauseroxmath6 , j. conradxmath1 , r. dimaxmath2 , d. fabrisxmath2 , r.a . \ufb01nixmath4 , e. \ufb01orettox Model the spatial precision and hit ef\ufb01ciency of the alice silicon pixel detector ( spd ) are key parameters for the detection of heavy - \ufb02avour in nucleus - nucleus collisions at lhc energies . the performance of the detector under test , in the form of single - chip assemblies , together with a detailed cluster",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 46,
                    "type": "Table"
                }
            },
            {
                "content": "Table I.28: Generated summaries by PEGASUSLARGE (HugeNews) on PubMed sampled by ROUGE1-F1.\nPubMed Document (ID #80) aging is a gradual process , proportional to time , that causes structural and functional changes due to internal degeneration . aging can be divided into intrinsic aging , which is natural with the progression of time , and environmental aging caused by the external environment . uv - induced photoaging is usually observed on skin that is exposed to the sun . its clinical characteristics include earlier observation than intrinsic aging , deep and wide wrinkles , inconsistent pigmentation and solar lentigo on the exposed skin surface2 . along with recent socio - \ufb01nancial improvements , improvements in medical technology , hygiene and nutrition have increased life expectancy . as the elderly population increases , many treatment methods to reduce skin aging and improve wrinkles and pores are currently being researched4 , 5 . however , ablative laser treatment causes a \ufb01nancial burden and side effects , such as erythema , edema , and hyperpigmentation . in addition , the damage on the epidermis caused by ablative laser treatment delays the time between returning to daily activities6,7,8 . recently , non - ablative skin rejuvenation , which improves skin aging and photoaging without damaging the epidermis , has been the preferred treatment9 . previous studies suggest that radiofrequency ( rf ) effectively reduces abdominal obesity by transferring heat to the dermis , increases blood volume in the capillaries and hence increases blood \ufb02ow , induces collagen formation , and increases the thickness of the epidermis10,11,12 . electroacupunture ( ea ) has been reported to reduce in\ufb02ammation , induce healing of damaged tissues , reduce pain , and improve facial rejuvenation13,14,15 . low - level laser therapy ( lllt ) has been used in various research studies and has been reported to have anti - in\ufb02ammatory effects , reduce pain ... ... ... Gold [ purpose ] the purpose of this study was to investigate the effects of radiofrequency ( rf ) , electroacupuncture ( ea ) , and low - level laser therapy ( lllt ) , which are used in physical therapies , on facial wrinkles and moisture . [ subjects and methods ] a total of 30 female participants aged 3055 years participated in this study and the results will be used as a reference for further development of skin physical therapy . thirty adult females were assigned to an lllt ( n=10 ) , ea ( n=10 ) , or rf group ( n=10 ) . the intervention was performed in two 15-minute sessions per week for six weeks . subjects skin tone and pigmentation were observed before and after the intervention . [ results ] treatment of the under eye area showed that wrinkles were signi\ufb01cantly decreased on both sides after rf , ea , and lllt . treatment of the eye rims indicated that wrinkles signi\ufb01cantly decreased on the right side after rf , ea , and lllt . [ conclusion ] the application of lllt , ea , and rf had positive effects on wrinkle and moisture content of adult women s faces . Model [ purpose ] the purpose of this study was to investigate the effects of radiofrequency ( rf ) , electroacupuncture ( ea ) , and low - level laser therapy ( lllt ) on wrinkles and moisture content . [ subjects and methods ] the subjects of this study were 30 adult women aged between 30 and 55 years who presented at m dermatology hospital in yongin , gyeonggi - do , south korea and who voluntarily agreed to participate after listening to an explanation of the purpose of the study . the subjects were assigned to rf , ( n=10 ) , ea ( n=10 ) , or lllt ( n=10 ) groups . the subjects received rf , ea , and lllt treatments for six weeks . [ results ] the rf , ea , and lllt groups showed signi\ufb01cant differences in wrinkles and moisture content . [ conclusion ] the results of this study suggest that rf , ea , and lllt may be effective for facial rejuvenation . ROUGE1-F1 62.50 Document (ID #124) acute appendicitis is considered as the most frequent cause of acute abdomen in young patients with a slight predominance in males . although its clinical diagnosis is simple , 20 - 30% of appendicitis cases are reported to be gangrenous or perforated because of delayed diagnosis , which can lead to increased morbidity and mortality . additionally , up to 10% of patients especially older ones , immunocompromised patients , and pregnant women are diagnosed with atypical presentations such as appendiceal mass which requires long hospital admissions , intravenous antibiotics , and often needs percutaneous drainage . using colonoscopy for the diagnosis and treatment of asymptomatic acute appendicitis and appendiceal abscess are rare . in unusual appendicitis cases , especially in the presence of abdominal comorbidities , colonoscopy is used ; although colonoscopy is not considered as the \ufb01rst line intervention because of complications risk such as bleeding or perforation . furthermore non - invasive tests such as ultrasonography and computed tomography with more sensitivity can be used . we report an atypical case of appendiceal abscess , which was diagnosed during colonoscopy . a 30-year - old man referred for evaluation to our center with epigastric pain from three days prior to his evaluation . the pain was nonspeci\ufb01c and colicky distributed in periumblical region , did not radiate to any site and was unrelated to bowel movements . he had experienced also nausea , anorexia and intermittent vomiting . there was a history of marijuana and alcohol usage from several years ago . upon physical examination , he was ill with restlessness . other laboratory values such as hemoglobin , platelets , serum amylase , erythrocyte sedimentation rate , aspartate aminotransferase , alanine aminotransferase , alkaline phosphatase , and electrolytes were normal . his workup on admission including ... ... ... Gold although complicated acute appendicitis is occasionally observed in general practice , the diagnosis of appendiceal abscess by colonoscopy is unusual . we report a case of appendiceal abscess with spontaneous drainage into the lumen of colon diagnosed during colonoscopy and resolved by the time of surgery . Model acute appendicitis is considered as the most frequent cause of acute abdomen in young patients with a slight predominance in males . although its clinical diagnosis is simple , 20 - 30% of appendicitis cases are reported to be gangrenous or perforated because of delayed diagnosis , which can lead to increased morbidity and mortality . additionally , up to 10% of patients especially older ones , immunocompromised patients , and pregnant women are diagnosed with atypical presentations such as appendiceal mass which requires long hospital admissions , intravenous antibiotics , and often needs percutaneous drainage . using colonoscopy for the diagnosis and treatment of asymptomatic acute appendicitis and appendiceal abscess are rare . in unusual appendicitis cases , especially in the presence of abdominal comorbidities , colonoscopy is used ; although colonoscopy is not considered as the \ufb01rst line intervention because of complications risk such as bleeding or perforation . we report an atypical case of appendiceal abscess , which was diagnosed during colonoscopy .",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 47,
                    "type": "Table"
                }
            },
            {
                "content": "Document (ID #299) a positive family history of prostate cancer ( pca ) is an established risk factor for pca . \ufb01rst - degree relatives of affected men have a 23 fold increased risk of pca . when 3 or more \ufb01rst - degree relatives are affected ( or at least 2 \ufb01rst - degree relatives before the age of 55 years ) , the family is considered a hereditary prostate cancer ( hpc ) family according to the so - called johns hopkins or carter criteria . yet , only a few very rare high - penetrance gene mutations have been identi\ufb01ed to cause hpc . in recent years , genome - wide association studies have added approximately 40 low - penetrance genetic polymorphisms that are associated with an increased risk of pca [ 4 , 5 ] . several polymorphisms have also been identi\ufb01ed that are associated with serum prostate - speci\ufb01c antigen ( psa ) , the most commonly used marker for early detection of pca [ 6 , 7 ] . an ongoing matter of debate is whether psa testing should be used for population - wide screening . in a population - based setting , the european randomized study of screening for prostate cancer ( erspc ) showed a decrease in pca mortality of 31% in the screening arm after correction for non - attendance and contamination . by contrast , the prostate , lung , colorectal and ovarian cancer screening trial ( plco ) , found no effect of psa screening on mortality , but suffered from methodological problems which severely hamper interpretation of the results . previous studies into the effectiveness of psa sceening in men with an increased risk due to family history have yielded largely inconsistent results . these range from a marked bene\ufb01t ... ... ... Gold a family history of prostate cancer ( pca ) is an established risk factor for pca . in case of a positive family history , the balance between positive and adverse effects of prostate - speci\ufb01c antigen ( psa ) testing might be different from the general population , for which the european randomized study of screening for prostate cancer ( erspc ) showed a bene\ufb01cial effect on mortality . this , however , went at the cost of considerable overtreatment . this study assessed dutch physicians knowledge of heredity and pca and their post - erspc attitude towards pca testing , including consideration of family history . in january 2010 , all dutch urologists and clinical geneticists ( cgs ) and 300 general practitioners ( gps ) were invited by email to complete an anonymous online survey , which contained questions about hereditary pca and their attitudes towards pca case - \ufb01nding and screening . 109 urologists ( 31% ) , 69 gps ( 23% ) and 46 cgs ( 31% ) completed the survey . cgs had the most accurate knowledge of hereditary pca . all but 1 cg mentioned at least one inherited trait with pca , compared to only 25% of urologists and 9% of gps Model purposea positive family history of prostate cancer ( pca ) is an established risk factor for pca . to guide the public and physicians in translating the results of the european randomized study of screening for prostate cancer ( erspc ) and the prostate , lung , colorectal and ovarian cancer screening trial ( plco ) into clinical practice , the dutch association of urology ( nvu ) and the dutch college of general practitioners ( nhg ) concurrently released a policy statement in march 2009 . to date , in absence of of\ufb01cial internationally accepted guidelines , the advice is to attempt to distinguish genetic predisposition - based families with multiple pca cases from ascertainment - based multiple - case families and offer pca testing only to the former . this study assessed the knowledge of dutch urologists , general practitioners ( gps ) and clinical geneticists ( cgs ) about hpc and pca as a phenotype in hereditary syndromes and their post - erspc attitude towards pca testing and the role of family history in clinical decision - making about pca testing.methodsonline surveys were sent to all dutch urologists ( n = 351 ) , gps ( n = 69 ) and cgs ( n = 50 ROUGE2-F1 24.86 Document (ID #72) this retrospective study was approved by the institutional review board of our hospital ( 2014 - 09 - 037 ) . between november 2011 and october 2014 , 245 patients in the icu underwent bedside pdt with a ciaglia blue rhino percutaneous tracheostomy set ( cook medical , bloomington , in , usa ) due to prolonged endotracheal tube insertion . pdt was not performed in patients aged under 18 years , those with any pulsation palpated over the tracheostomy site , patients with a history of surgery or radiotherapy in the cervical region , and those with coagulopathy ( increased prothrombin time , inr \u00bf 2 ) . the \ufb01rst four procedures were performed by two physicians and one nurse using the standard technique , with one physician operating the bronchoscope while the other performed pdt . the next 55 procedures were performed by one physician and one nurse , with the physician using the bronchoscope only to con\ufb01rm and evaluate the proper depth for the endotracheal tube before performing pdt using the simpli\ufb01ed method described below . the remaining 186 procedures were performed using the simpli\ufb01ed technique after repositioning the endotracheal tube at the predetermined depth ( 17 cm for males and 15 cm for females at the incisor ) . all pdts were performed by a single physician and one nurse , both of whom fully understood the entire procedure . the nurse administered sedatives and analgesics , as well as repositioning the endotracheal tube . except for patients with limited neck extension , a small pillow was placed under the patient \u2019s shoulders to slightly extend the neck . transverse markings were made at the cricothyroid membrane and 2.0 cm toward the sternal notch ( \ufb01g . was also made in the midline of the trachea ... ... ... Gold when \ufb01rst introduced , percutaneous dilatational tracheostomy ( pdt ) was performed using a bronchoscope . the bronchoscope itself , however , has several disadvantages , including interruption of ventilation . modi\ufb01cations of the standard pdt technique have suggested that pdt can be safely performed without a bronchoscope . however , the safety of these modi\ufb01cations is unclear and many modi\ufb01cations have yet to be widely adopted . this study retrospectively evaluated the safety of a simpli\ufb01ed pdt technique using the cook ciaglia blue rhino in 186 patients . after con\ufb01rming the underlying structures by palpation , pdt was performed without a bronchoscope or blunt dissection , 2.0 cm below the cricothyroid membrane . ultrasonography was used only when palpation was dif\ufb01cult . except for bleeding in three patients with coagulopathy , none of the patients experienced any early or late complications , and there were no deaths related to pdt . these \ufb01ndings con\ufb01rm that pdt can be safely performed using a simpli\ufb01ed technique , which may be useful in situations with limited equipment and medical personnel . Model backgroundpercutaneous tracheostomy ( pdt ) is one of the most common tracheostomy procedures performed in the intensive care unit ( icu ) . although the use of a \ufb01beroptic bronchoscope ( fob ) during pdt has been recommended for precise positioning and to avoid complications , bronchoscopy itself can cause complications . this study assessed the safety of a simpli\ufb01ed pdt technique using the ciaglia blue rhino , by repositioning the existing endotracheal tube at a pre - determined depth and determining proper pdt site by super\ufb01cial palpation.methodsbetween november 2011 and october 2014 , 245 patients in the icu underwent bedside pdt with a ciaglia blue rhino percutaneous tracheostomy set due to prolonged endotracheal tube insertion . the \ufb01rst four procedures were performed by two physicians and one nurse using the standard technique , with one physician operating the bronchoscope while the other performed pdt . the next 55 procedures were performed by one physician and one nurse , with the physician using the bronchoscope only to con\ufb01rm and evaluate the proper depth for the endotracheal tube before performing pdt using the simpli\ufb01ed method described below . the remaining 186 procedures were performed using the simpli\ufb01ed technique after repositioning the endo",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 48,
                    "type": "Table"
                }
            },
            {
                "content": "tic brackets . stainless steel alloy contains 8%-12% nickel , 17%-22% chromium and other elements such as copper , iron molybdenum , manganese , silicon and sulfur[3 - 5 ] in the oral environment , orthodontic brackets are subjected to mechanical and chemical damaging which results in suscep- tibility to corrosion . corrosion leads to loss of substance from the material , change in its structural characteristics , or loss of structural integrity . due to the electrolytic capabilities of saliva various types of brackets are commercially available and each demonstrates a unique pattern of corrosion . in soldered brackets , this corrosion is due to the presence of dissimilar metals ( i.e. the silver solder and the stainless steel ) , a phenomenon termed galvanic corrosion . metal injection molding ( mim ) brackets are manufactured as a single unit and therefore do not demonstrate galvanic corrosion . corrosion can have detrimental effects on the surface of stainless steel brackets due to the continuous loss of metal ions . corrosion can increase the surface roughness of the bracket which leads to elevated friction forces between the bracket and the archwire . this increase in friction results in unfavorable distribution of forces and reduces the effectiveness of archwire guided orthodontic tooth movement.[7 - 8 ] moreover , by means of increased stress , the friction would further accelerate the corrosion process . the release of metal ions following the corrosion of brackets has concerned clinicians and has instigated research in this \ufb01eld . among these metal ions , furthermore , direct and prolonged contact of orthodontic appliances and the resulting corrosion products have been shown to cause local pain and ... ... ... Gold statement of the problem : stainless steel brackets release metallic ions following the process of corrosion in the oral environment . these released ions have potential adverse effects on health , friction between wire and bracket , staining , strength of brackets . choosing a bracket with favorable corrosive properties ; therefore , should be a goal of every practitioner . purpose : the goal of this study is to compare the amount of corrosion among \ufb01ve different brands of brackets using inductively coupled plasma ( icp ) mass spectrometry . materials and method : \ufb01ve different brands of brackets ( dentaurum , 3 m , ortho organizer , cobas and o.r.g ) were chosen and ten brackets were selected from each brand . a piece of stainless steel wire was ligated to each bracket . the bracket - archwire complex was then immersed in arti\ufb01cial saliva . subsequently , the samples were analyzed using an icp device and the levels of iron , chromium , nickel , and manganese ions were measured . results : the \ufb01ndings of this study demonstrated that iron was released the most from the tested brackets , followed by nickel . we also found that the cobas bracket had the most ion release among the tested brackets ( p \u00a1 0.05 ) Model abstractobjectivesthe release of metal ions following the corrosion of brackets has concerned clinicians and has instigated research in this \ufb01eld . the present study aimed to investigate \ufb01ve different brands of stainless steel brackets and compare their tendency towards corrosion by measuring ion release in an in - vitro setting.material and methodsten central incisor brackets were selected from each brand . in order to simulate conventional orthodontic treatment , 8 mm of 0.016 stainless steel archwire was tied in each bracket using 10 mm of 0.25 mm ligature wire ( dentaurum , germany ) . once the brackets were prepared , they were placed in poly - ethylene capped vials containing 10 ml of arti\ufb01cial saliva at a ph of 7.2 . the vials were incubated at 37c for 6 weeks and then they were subjected to thermocycling with 500 temperature cycles from 5c to 55c to simulate the effect of temperature changes in the oral cavity . the brackets were immersed in each bath for 30 seconds with 2 seconds at air temperature in - between the immersions . the details of the brackets selected for the study after thermal - cycling the solutions from the vials were analyzed to determine the amount of nickel , chromium , manganese , and iron using an inductively coupled plasma spectrometer ( icp ) ROUGEL-F1 23.41 Document (ID #114) chronic pain affects up to 20% of the population in developed nations.14 this represents a profound impact on individuals and their families alongside the sizeable burden on employers , health care systems , and society in general.3 when chronic pain occurs , it has the potential to become disease itself , and subsequently , chronic pain has emerged as a distinct phenomenon.5 management of chronic pain varies greatly between nations and even within nations . literature supports a multidisciplinary approach as the standard of care , although various health care systems may not always support this concept consistently.2 the current standard of care for chronic , noncancer pain typically includes many disciplines with the clinician developing an individualized treatment plan with the options of utilizing surgical interventions , pharmacology , and psychological and physical therapies . opioid analgesics are often prescribed , despite the lack of clinical evidence supporting their long - term use in the management of chronic pain.6 however , for many patients , this multidisciplinary approach is inadequate or ineffectual or is accompanied by the burden of side effects that are unacceptable and debilitating . only at this late stage , the \ufb01eld of neuromodulation for the treatment of pain has developed rapidly since the seminal paper on the electrical inhibition of pain by the stimulation of the dorsal column almost 50 years ago.7 the original term of dorsal column stimulation has evolved to become known as spinal cord stimulation ( scs).8 scs has been particularly effective as an adjunct in treating mixed neuropathic / nociceptive and neuropathic / radicular pain conditions such as failed back surgery syndrome ( fbss ) and complex regional pain syndrome ( crps ) . neuromodulation therapies offer a treatment option that has minimal side effects and that is relatively safe and ... ... ... Gold spinal cord stimulation ( scs ) applications and technologies are fast advancing . new scs technologies are being used increasingly in the clinical environment , but often there is a lag period between the clinical application and the publishing of high - quality evidence on safety and ef\ufb01cacy . recent developments will undoubtedly expand the applicability of scs , allowing more effective and individualized treatment for patients , and may have the potential to salvage patients who have previously failed neuromodulation . already , high - level evidence exists for the safety , ef\ufb01cacy , and cost - effectiveness ( level i ii ) of traditional scs therapies in the treatment of chronic refractory low back with predominant limb pain ( regardless of surgical history ) . more than half of all patients with chronic painful conditions experience sustained and signi\ufb01cant levels of pain reduction following scs treatment . although only limited evidence exists for burst stimulation , there is now level i evidence for both dorsal root ganglion scs and high - frequency scs that demonstrates compelling results compared with traditional therapies . the body of evidence built on traditional scs research may be redundant , with newer iterations of scs therapies such as dorsal root ganglion scs , high - frequency scs , and burst scs . a number of variables Model chronic pain affects up to 20% of the population in developed nations . the current standard of care for chronic , noncancer pain typically includes many disciplines with the clinician developing an individualized treatment plan with the options of utilizing surgical interventions , pharmacology , and psychological and physical therapies . however , for many patients , this multidisciplinary approach is inadequate or ineffectual or is accompanied by the burden of side effects that are unacceptable and debilitating . only at this late stage , the \ufb01eld of neuromodulation for the treatment of pain has developed rapidly since the seminal paper on the electrical inhibition of pain by the stimulation of the dorsal column almost 50 years ago . spinal cord stimulation ( scs ) has been particularly effective as an adjunct in treating mixed neuropathic / nociceptive and neuropathic / radicular pain",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 49,
                    "type": "Table"
                }
            },
            {
                "content": "Document (ID #260) Subject: Access Card Information Dear Colleagues, We know you have many questions about your new bene\ufb01ts plan, payroll issues and other changes that will occur over the next few weeks. Human Resources and Communications are preparing a thorough Q&A, which we will send to you tomorrow and will regularly update as more information is available. Meanwhile, please read the announcement below from Security about how you can obtain a new UBS Warburg access card that will allow you to enter our new of\ufb01ce space after Friday. I am con\ufb01dent that you share my enthusiasm about our new opportunity. We look forward to working along side each of you in the coming days. David Oxley Human Resources Gold Access Card Information Model Access Card Information ROUGE1-F1 100.00 Document (ID #33) The funerals of Andy, Darren and Paul, who tragically lost their lives in last Wednesday\u2019s explosion at Teesside Power Station, will be held this week at the following times: Andy Sherwood 2.00pm, Wednesday, August 15 St James Church, Hartlepool Darren Higgins 9.15 am, Thursday, August 16 St. Andrews Church, Teesville, Middlesbrough Paul Surtees 1.30 pm, Thursday, August 16 Easington Church of the Ascension, Easington Colliery We know that everyone at Enron will want to remember the colleagues we have lost at this dif\ufb01cult time, and our thoughts and prayers are with their families and friends. We are considering how best to commemorate the lives of Andy, Darren and Paul, and will be discussing our ideas with their families, in due course. Graeme White is recovering in Middlesbrough General Hospital where his condition remains stable and we are continuing to provide as much support as possible for Graeme and his family. The plant remains off-line and Enron continues to co-operate fully with the Health & Safety Executive\u2019s investigation into the cause of the explosion. John Sherriff and Michael Brown Gold TEESSIDE POWER STATION \u2013 UPDATE Model Teesside Update ROUGE1-F1 66.67 Document (ID #230) Wayne, I have modi\ufb01ed the above contract as we discussed. The \u201dCustomer\u2019s Agent\u201d language is contained in Appendix \u201d1\u201d. Please have the contract signed by JER Partners, a J.E. Robert Company and fax it back to me. I will then have it signed by Houston Pipe Line Company. I have left the attachment at the end as originally drafted until I am able to talk to Gary Lamphier. If it is truly correct that there are no minimum quantity obligations under the deal, then Gary may have to reprice the $0.09 discount for the 1000/d. I will leave that up to Gary and you to resolve. Gold JER / BRE Austin Tech, L.P. Model JER Partners, L.L.C. ROUGE1-F1 36.36 Document (ID #231) Steve: I just spoke with Bill Bradford and I understand that Bill and Mark Tarkington discussed the use of \u201dSpeci\ufb01ed Entity\u201d and agreed, subject to DB committee approval, to eliminate the use of Speci\ufb01ed Entity in the ENA and af\ufb01liated masters with DB (i.e., \u201dSpeci\ufb01ed Entity: none\u201d). Once you have con\ufb01rmed this with Mark, please prepare a \ufb01nal blacklined draft for our review and then execution copies of the ISDA Master Agreement. In connection with the \ufb01nal draft, Tana Jones will fax to you minor format comments. [She will also fax to you the electronic trading agreements and password applications for London and U.S. based trading.] The following ENA af\ufb01liates will also need to execute ISDA Master Agreements with DB: (1) ECT Investments, Inc. (Delaware corporation) (2) Enron Capital & Trade Resources International Corp. (Delaware corporation) (3) EnronCredit.com Limited (U.K. corporation) Gold Enron North America Corp. (\u201dENA\u201d)/Deutsche Bank (\u201dDB\u201d) ISDA Master Agreement Model Deutsche Bank ROUGE1-F1 30.77 Document (ID #162) Ms. Peppers, I am very sorry, but I must once again reschedule the appointment that you have with Mr. Lay. He will be out of town the entire week of September 26. The \ufb01rst available time to reschedule is October 19, 2000. Would an afternoon appointment work for you - about 2:00 p.m.? I look forward to hearing from you. Rosalee Gold Reschedule appointment Model Mr. Lay\u2019s schedule ROUGE1-F1 0.00 Document (ID #108) Due to a variety of circumstances we did not migrate employees to the UBSWE environment tonight (Monday, February 4, 2002). We will commu- nicate further details of the migration plan on Tuesday, February 5, 2002. Thank you for your cooperation in advance, please contact the Resolution Center at x 3-1411 should you have any questions. UBSW Energy Global Infrastructure Gold DRAFT: INFORMATION, Please Read: UBSWE Migration postponed Model migrated employees",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 50,
                    "type": "Table"
                }
            },
            {
                "content": "Table I.32: Generated summaries by PEGASUSLARGE (HugeNews) on AESLC sampled by ROUGE2-F1.\nAESLC Document (ID #292) Just a reminder - We have added a new pub code to EnPower. The code is for the Dow Jones COB Sunday Index. Please advise that the Portland West Desk has one deal in the system at this time against that index, and more may follow. Thanks for your help with entering these prices. Kate Gold New Pub Code Model New pub code for EnPower ROUGE2-F1 66.67 Document (ID #100) El Paso recently \ufb01led their proposal to reallocate receipt point capacity. Comments are due on Thursday, May 17, 2001. We would like to meet with you to make sure our comments address the commercial concerns regarding the reallocation plan. In that regard, please plan to attend a meeting on Thursday, May 3 at 3:45 in 3269. Becky will provide for your review a summary of El Paso\u2019s proposal. Please let me know if you have any questions. Thanks Gold El Paso\u2019s Proposal to Address the Allocation of Receipt Point Capacity Model Comments on El Paso\u2019s Reallocation Proposal ROUGE2-F1 23.53 Document (ID #241) Media reports out of France indicate that explosions have just taken place at two chemical plants in the southern city of Tououse. The main explosion reportedly took place at a plant that manufactures fuel for Arianespace, the European Space Agency\u2019s commercial arm. Explosions have also been reported at a plant owned by Groupe SNPE, which manufactures propellants and explosives. The incidents are reported to have involved deaths at the scene. We have yet to acquire independent con\ufb01rmation of the explosions, or whether terrorism is suspected. We are in touch with military and intelligence sources, and will provide updates. Gold **** Uncon\ufb01rmed Explosions at French Chemical Plants Model Recent explosions in France ROUGE2-F1 0.00 Document (ID #189) Nothing from Ben yet. If I don\u2019t hear from him in the next couple of hours, then I think we should be ready with an extension letter, giving them a couple of days extra for free. Ben indicated to me last week that he would give them some time once they asked. MK called with questions on the assignment - nothing serious. Kay Gold NW Model SMUD ROUGE2-F1 0.00 Document (ID #104) Hey Chris, I was wondering if there was a fax number at the base so that my apartment complex can send you some info. regarding getting your name taken off of our lease. If there is, just shoot me off an email with the number and I\u2019ll pass it along to them. Hope things are going well and that you\u2019re not too homesick with Thanksgiving and all. I have a feeling I will be talking to you guys tomorrow along with both of our parents and extended family members. Have a great holiday and root on the Horns ( I think this is going to be a tough one). Love, Gold Apartment Lease Model Hello! ROUGE2-F1 0.00 Document (ID #37) Kal/Susan - Attached is an updated version of the disclosure consent agreement for EOL information. Please take a look at the attached and let me know your thoughts. Mark Gold EOL Disclosure Consent Model EOL Consent",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 51,
                    "type": "Table"
                }
            },
            {
                "content": "Table I.33: Generated summaries by PEGASUSLARGE (HugeNews) on AESLC sampled by ROUGEL-F1.\nAESLC Document (ID #192) I need a copy of your insurance before the \ufb02ight on 11/25. Our records show your insurance expired on 10/4/2001 I have to have your insurance on \ufb01le before your \ufb02ight. thank you Gold insurance Model Insurance ROUGEL-F1 100.00 Document (ID #106) As discussed during our recent demonstration of our new portfolio tracking system (PortRAC), we have completed a security system that will provide a \u201d\ufb01rewall\u201d controlling access to each business unit\u2019s data. Our intent is to open up the system to designated members of each business unit or others as approved by the COO or Managing Director of that group. In order to complete the initial setup, we need to receive a listing of approved users for your business unit and their login Id\u2019s. If you prefer to delegate security approval authority to someone else on your staff, please let us know and we will establish procedures with them. If additional demonstrations of PortRAC are necessary for your group, please let us know. Thank you for your assistance. Rick C. Gold PortRAC Security Model PortRAC Security System ROUGEL-F1 80.00 Document (ID #254) Mike, Here\u2019s a revised version of the option agreement. I changed the shipping dates, quantity (10), Outside Exercise Date, and the amount of the option payment. I did not change the prices, since I had not discussed this with Ben, and couldn\u2019t get the numbers to work. Have there been some changes to the data sheets? It appears to me that there might be a version which re\ufb02ects the inclusion of the low sound option. In the last draft we did not include the low sound option because the impact on the load losses had not been agreed, so the prices did not re\ufb02ect the option. Please call or email me if you have any questions or comments. Kay Gold Option Model Rev option agreement ROUGEL-F1 50.00 Document (ID #267) Susan: I\u2019m not sure that what I told you to do in the CSA Annex with respect to Custodians and their quali\ufb01cations work so for now let\u2019s keep it out and go back to our old language. Sara and Mark: Shari had a counterparty point out to her that in our CSA Annex language where we talk about the circumstances under which a party can \u201dhold\u201d collateral, all of our triggers are based on something happening to our counterparty and nothing is said about what would happen if a Custodian holds the colateral and fails to be creditworthy. Do you have any thoughts on this? At \ufb01rst I thought that we should require the Custodian to be a Quali\ufb01ed institution but I\u2019m not sure that that is the right approach. Let me know what you think. Carol Gold Custodian Language Model Custodians ROUGEL-F1 0.00 Document (ID #162) Ms. Peppers, I am very sorry, but I must once again reschedule the appointment that you have with Mr. Lay. He will be out of town the entire week of September 26. The \ufb01rst available time to reschedule is October 19, 2000. Would an afternoon appointment work for you - about 2:00 p.m.? I look forward to hearing from you. Rosalee Gold Reschedule appointment Model Mr. Lay\u2019s schedule ROUGEL-F1 0.00 Document (ID #22) the tax issue that we discussed at the rmt meeting has been cleared. now we need to see if the $250 million is somehow at the rmt level or would be trapped in some way in a rmt \ufb01ling. i keep thinking that i will have time to track down with weil and has not happened. can you track this down and then let the group know. add stuart to this list. can you do? ? Gold rmt Model ISDA",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 52,
                    "type": "Table"
                }
            },
            {
                "content": "Document (ID #193) SECTION 1. SHORT TITLE. This Act may be cited as the Veterans Homebuyer Accessibility Act of 2015\u201d. SEC. 2. VETERAN FIRST-TIME HOMEBUYER TAX CREDIT. (a) In General.\u2013Section 36(a) of the Internal Revenue Code of 1986 is amended to read as follows: (a) Allowance of Credit.\u2013In the case of an eligible veteran who purchases a principal residence in the United States during the taxable year, there shall be allowed as a credit against the tax imposed by this subtitle for such taxable year an amount equal to 10 percent of the purchase price of the residence.\u201d. (b) Additional Limitation for Adaptive Housing Improvements.\u2013 Section 36(b)(1) of such Code is amended by redesignating subparagraphs (B), (C), and (D) as subparagraphs (C), (D), and (E), respectively, and by inserting after subparagraph (A) the following new subparagraph: (B) Special rule for adaptive housing improvements.\u2013In the case of a principal residence with special \ufb01xtures or movable facilities made necessary by the nature of the disability of the veteran, if such \ufb01xtures and facilities are\u2013 (i) provided to the veteran pursuant to specially adapted housing assistance under chapter 17 or 21 of title 38, United States Code, or (ii) similar to such \ufb01xtures and facilities that would be provided to the veteran if the veteran received such assistance, then subparagraph (A) shall be increased by the lesser of $8,000 or the portion of the purchase price of the principal residence attributable such \ufb01xtures or movable facilities.\u201d. (c) Eligible Veteran.\u2013 (1) In general.\u2013Section 36(c)(1) of such Code is amended by striking First-time homebuyer.\u2013The term \ufb01rst time homebuyer\u2019 means any individual\u201d and inserting Eligible veteran.\u2013The term eligible veteran\u2019 means any individual who is a veteran (as de\ufb01ned in section 101(2) ... ... ... Gold Veterans Homebuyer Accessibility Act of 2015 This bill amends the Internal Revenue Code, with respect to the tax credit for \ufb01rst-time homebuyers, to: (1)&nbsp;allow a veteran of the Armed Forces a tax credit for 10% of the purchase price of a principal residence purchased prior to January 1, 2018; (2)&nbsp;allow an additional credit for the cost, not exceeding $8,000 in a taxable year, of installing special \ufb01xtures or movable facilities in a residence to accommodate a disability of the veteran; and (3) require a recapture of credit amounts if the veteran sells such residence within 36 months after purchasing it. Model Veterans Homebuyer Accessibility Act of 2015 Amends the Internal Revenue Code, with respect to the tax credit for \ufb01rst-time homebuyers, to allow veterans of the Armed Forces a tax credit for 10% of the purchase price of a principal residence purchased prior to January 1, 2017. Allows an additional credit for the cost of installing special \ufb01xtures or movable facilities in a residence to accommodate a disability of the veteran. Requires a recapture of credit amounts if the veterans sells such residence within 36 months after purchasing it. ROUGE1-F1 86.15 Document (ID #122) SECTION 1. SHORT TITLE. This Act may be cited as the Patients\u2019 Formulary Rights Act of 1999\u201d. SEC. 2. PATIENT PROTECTIONS AGAINST ABUSE OF FORMULARIES FOR PRESCRIPTION DRUGS. (a) Group Health Plans.\u2013 (1) Public health service act amendments.\u2013(A) Subpart 2 of part A of title XXVII of the Public Health Service Act is amended by adding at the end the following new section: SEC. 2707. STANDARDS RELATING TO USE OF FORMULARIES AND THERAPEUTIC SUBSTITUTION. (a) Requirements on Use of Formularies.\u2013 (1) In general.\u2013A group health plan, and a health insurance issuer offering group health insurance coverage, shall not use a formulary unless the plan or issuer\u2013 (A) noti\ufb01es participants, bene\ufb01ciaries, and enrollees, prior to initial enrollment or coverage, and makes available at any time to health care professionals who prescribe prescription drugs under the plan or coverage of the information described in paragraph (2); (B) noti\ufb01es participants, bene\ufb01ciaries, enrollees, and health care providers who prescribe covered prescription drugs under the plan or coverage on a routine and annual basis of any changes in (including deletions from) the formulary; and (C) in the case of a participant, bene\ufb01ciary, or enrollee who is provided coverage for a prescription drug at the time the drug is removed from the formulary, to permit the participant, bene\ufb01ciary, or enrollee to continue to have the drug prescribed for treatment of the same condition for which it was previously prescribed. (2) Information to be disclosed.\u2013The information described in this paragraph is as follows (with respect to prescription drug coverage under a group health plan or health insurance coverage): (A) Extent of therapeutic substitution.\u2013What constitutes the practice or therapeutic substitution that may be effected under the plan or coverage. ... ... ... Gold Includes within required formulary information: (1) the extent of therapeutic substitution; (2) a complete list of all drugs included in the formulary; (3) information on access to nonformulary drugs; (4) whether or not, and to what extent, coverage will exist for nonformulary drugs; (5) copayments or other cost-sharing for formulary drugs; and (6) limits on coverage payments for outpatient formulary and nonformulary drugs. Amends title XVIII (Medicare) of the Social Security Act to require its authorized health insurance issuers and any Medicare+Choice organization to comply with such noti\ufb01cation requirements in the same manner that such requirements apply to health insurance coverage or issuers in connection with a group plan. Requires the coordination of noti\ufb01cation and formulary requirements through an interagency memorandum of understanding among the Secretaries of Labor, the Treasury, and Health and Human Services. Model Patients\u2019 Formulary Rights Act of 1999 - Amends the Public Health Service Act, the Employee Retirement Income Security Act of 1974, and the Internal Revenue Code to prohibit a group health plan and a health insurance issuer offering group coverage from using a formulary unless the plan or issuer: (1) noti\ufb01es participants, bene\ufb01ciaries, and enrollees, prior to initial enrollment or coverage, and makes available at any time to health care professionals who prescribe prescription drugs; (2) noti\ufb01es participants, bene\ufb01ciaries, enrollees, and health care providers who prescribe covered prescription drugs on a routine and annual basis of any changes in the formulary; and (3) in the case of a participant, bene\ufb01ciary, or enrollee who is provided coverage for a prescription drug at the time the drug is removed from the formulary, permits the participant, bene\ufb01ciary, or enrollee to continue to have the drug prescribed for treatment of the same condition for which it was previously prescribed. Requires such information to be disclosed to participants, bene\ufb01ciaries, enrollees, and health care providers. Prohibits a plan or issuer from using a formulary unless the plan or issuer: (1) noti\ufb01es participants, bene\ufb01ciaries, and enrollees, prior to initial enrollment or coverage, and makes available at any time to health care professionals who prescribe prescription drugs; (2) makes available at any time to health care professionals",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 53,
                    "type": "Table"
                }
            },
            {
                "content": "title 17, United States Code, is amended\u2013 (1) by redesignating clauses (iii), (iv), and (v) as clauses (iv), (v), and (vi), respectively; and (2) by inserting after clause (ii) the following: (iii) Noncommercial educational broadcast stations.\u2013 (I) Secondary transmissions within state-wide network.\u2013In the case of a State-wide network of noncommercial educational broadcast stations, the statutory license provided for in subparagraph (A) shall apply to the secondary transmission of any noncommercial educational broadcast station in that State-wide network to any subscriber in any county within that State that is located outside that station\u2019s local market and is not served by a noncommercial educational television broadcast station that is located within that State. (II) State-wide network de\ufb01ned.\u2013In this clause, the term State-wide network of noncommercial educational broadcast stations\u2019 is a network of three or more noncommercial educational broadcast stations that are licensed to a single State, political, educational, or special purpose subdivision of a State, or a single public agency.\u201d. (b) Royalty Fee.\u2013Section 119(b)(1)(B) of title 17, United States Code, is amended by inserting or paragraph (2)(C)(iii)\u201d after subscriber under paragraph (3)\u201d. SEC. 3. AMENDMENTS TO THE COMMUNICATIONS ACT OF 1934. (a) In General.\u2013Section 325(b)(2) of the Communications Act of 1934 (47 U.S.C. 325 (b)(2)) is amended\u2013 (1) by striking or\u201d at the end of subparagraph (D); (2) by striking the period at the end of subparagraph (E) and inserting ; or\u201d; and (3) by inserting after subparagraph (E) the following new subparagraph: (F) to retransmission of ... ... ... Gold Statewide PTV Access Act of 2009 - Permits a satellite carrier to retransmit the signals of a state-wide network of noncommercial educational broadcast stations to any subscriber in any county within such state that is: (1) outside that station\u2019s local market; and (2) not served by an in-state noncommercial educational television broadcast station. Amends the Communications Act of 1934 to permit a satellite carrier to retransmit the signals of a television broadcast station outside the station\u2019s local market directly to its subscribers if: (1) such station is a part of a network of three or more noncommercial educational broadcast stations that are licensed to a single state, political, educational, or special purpose subdivision of a state, or to a single public agency; and (2) the carrier retransmits such signal only to subscribers in a county within that state that is outside the station\u2019s local market and is not served by another in-state noncommercial educational broadcast station. Model Statewide PTV Access Act of 2009 - Amends federal copyright law to provide that the statutory license provided for the secondary transmission of any noncommercial educational broadcast station in a state-wide network of such stations shall apply to the secondary transmission of any such station in any county within that state that is located outside that station\u2019s local market and is not served by a noncommercial educational television broadcast station located within that state. De\ufb01nes \u201dstate-wide network of noncommercial educational broadcast stations\u201d as a network of three or more noncommercial educational broadcast stations that are licensed to a single state, political, educational, or special purpose subdivision of a state, or a single public agency. Requires a satellite carrier that retransmits the signal of a television broadcast station outside the station\u2019s local market by a satellite carrier directly to its subscribers, if: (1) the station is part of a network of three or more noncommercial educational broadcast stations that are licensed to a single state, political, educational, or special purpose subdivision of a state, or a single public agency; and (2) the satellite carrier retransmits the secondary signal of such eligible station only to its subscribers located in any county within that state that is located outside that station\u2019s local market and not served by another noncommercial educational broadcast station located ROUGE2-F1 56.40 Document (ID #210) SECTION 1. SHORT TITLE. This Act may be cited as the Medicare Common Access Card Act of 2017\u201d. SEC. 2. MEDICARE SMART CARD PILOT PROGRAM. Part E of title XVIII of the Social Security Act is amended by inserting after section 1866E the following new section: SEC. 1866F. SMART CARD PILOT PROGRAM. (a) Implementation.\u2013 (1) In general.\u2013Not later than 36 months after the date of the enactment of this section, the Secretary shall establish a pilot program (in this section referred to as the pilot program\u2019) to demonstrate the feasibility of using smart card technology under this title. (2) Smart card technology de\ufb01ned.\u2013In this section, the term smart card technology\u2019 means the following: (A) Bene\ufb01ciary smart card.\u2013A machine readable, fraud- and tamper-resistant card (in this section referred to as a smart card\u2019) that includes an embedded integrated circuit chip with a secure micro- controller that enables the veri\ufb01cation and secure, electronic authentication of the identity of a Medicare bene\ufb01ciary at the point of service through a combination of the smart card and a personal identi\ufb01cation number known by or associated with such bene\ufb01ciary. (B) Card reader technology.\u2013Information technology that enables a supplier and provider to authenticate the identity of a Medicare bene\ufb01ciary through presentation of such a smart card and such components, with such authentication to be re\ufb02ected through the use of a modi\ufb01er or in another appropriate manner, as determined by the Secretary, in the claims adjudication process. (3) Program design elements.\u2013The pilot program shall be conducted for a period of 3 years consistent with the following: (A) Selection of area.\u2013In consultation with the Inspector General of the Department of Health and Human Services, the ... ... ... Gold Medicare Common Access Card Act of 2017 This bill requires the Centers for Medicare &amp; Medicaid Services (CMS) to establish a three- year pilot program, in at least three&nbsp;geographic areas,&nbsp;to demonstrate the feasibility of using smart-card technology to authenticate the identity of a Medicare bene\ufb01ciary at points of service. The CMS shall select supplier and provider types that will be required to participate in the pilot program as a condition of Medicare payment, but&nbsp;must exempt from participation a supplier or provider that: (1) does not have access to card-reader technology, (2) does not have suf\ufb01cient internet access, or (3) has a low volume of Medicare claims. The CMS shall select a private contractor to implement and operate the pilot program. The CMS must submit speci\ufb01ed reports to Congress on program design, implementation, and performance. For purposes of conducting the pilot program, the CMS shall&nbsp;provide for the&nbsp;transfer of $150 million from the Supplemental Medical Insurance Trust Fund to the CMS Program Management Account. Model Medicare Common Access Card Act of 2017 This bill requires the Centers for Medicare &amp; Medicaid Services (CMS) to establish a pilot program that demonstrates the feasibility of using smart card technology to address Medicare fraud. Under the program, smart card technology must be issued free-of-charge to selected Medicare bene\ufb01ciaries, suppliers, and providers; such technology must support the secure, electronic authentication of",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 54,
                    "type": "Table"
                }
            },
            {
                "content": "OF CARRYOVER BASIS. (a) In General.\u2013The following provisions of the Economic Growth and Tax Relief Reconciliation Act of 2001, and the amendments made by such provisions, are hereby repealed: (1) Subtitles A and E of title V. (2) Subsection (d), and so much of subsection (f)(3) as relates to subsection (d), of section 511. (3) Paragraph (2) of subsection (b), and paragraph (2) of subsection (e), of section 521. The Internal Revenue Code of 1986 shall be applied as if such provisions and amendments had never been enacted. (b) Sunset Not To Apply.\u2013 (1) Subsection (a) of section 901 of the Economic Growth and Tax Relief Reconciliation Act of 2001 is amended by striking this Act\u201d and all that follows and inserting this Act (other than title V) shall not apply to taxable, plan, or limitation years beginning after December 31, 2010.\u201d. (2) Subsection (b) of such section 901 is amended by striking , estates, gifts, and transfers\u201d. SEC. 3. UNIFIED CREDIT AGAINST THE ESTATE TAX. (a) In General.\u2013 Subsection (c) of section 2010 of the Internal Revenue Code of 1986 (relating to applicable credit amount) is amended by striking all that follows the applicable exclusion amount\u201d and inserting . For purposes of the preceding sentence, the applicable exclusion amount is $2,000,000.\u201d. (b) In\ufb02ation Adjustment.\u2013Subsection (c) of section 2010 of such Code, as amended by subsection (a), is amended\u2013 (1) by striking For purposes of this section,\u201d and inserting the following: (1) In general.\u2013For purposes of this section,\u201d, and (2) by adding at the end the following new paragraph: ... ... ... Gold Sensible Estate Tax Act of 2008 - Repeals provisions of the Economic Growth and Tax Reconciliation Act of 2001 relating to the estate and gift tax. Amends the Internal Revenue Code to: (1) allow an estate tax exclusion of $2 million adjusted for in\ufb02ation in calendar years after 2008; (2) revise the estate tax rates for larger estates; (3) restore the estate tax credit for state estate, inheritance, legacy, or succession taxes; (4) restore the uni\ufb01ed credit against the gift tax; and (5) allow a surviving spouse an increase in the uni\ufb01ed estate tax credit by the amount of any unused credit of a deceased spouse. Model Sensible Estate Tax Act of 2008 - Repeals provisions of the Economic Growth and Tax Relief Reconciliation Act of 2001 (EGTRRA) eliminating the tax on estates and generation-skipping transfers and the step-up in basis provisions for property acquired from a decedent for estates of decedents dying after 2008. Declares that the sunset provision (general terminating date of December 10, 2010) of EGTRRA shall not apply to title V of such Act (Estate, Gift, and Generation-Skipping Transfer Tax Provisions). Amends the Internal Revenue Code to: (1) restore the uni\ufb01ed credit against the estate tax; (2) provide for an in\ufb02ation adjustment to the estate tax exclusion amount after 2008; (3) establish maximum estate tax rates of over $5 million; and (4) restore the credit for state estate, inheritance, legacy, or succession taxes after 2008. ROUGEL-F1 41.84 Document (ID #216) SECTION 1. SHORT TITLE; TABLE OF CONTENTS. (a) Short Title.\u2013This Act may be cited as the Meeting Our Responsibility to Medicare Bene\ufb01ciaries Act of 2005\u201d. (b) Table of Contents.\u2013The table of contents of this Act is as follows: Sec. 1. Short title; table of contents. TITLE I\u2013ELIMINATING SPECIAL INTEREST PREFERENCES Sec. 101. Negotiating fair prices for medicare prescription drugs. Sec. 102. Elimination of MA Regional Plan Stabilization Fund (Slush Fund). Sec. 103. Application of risk adjustment re\ufb02ecting characteristics for the entire medicare population in payments to Medicare Advantage organizations. TITLE II\u2013IMPROVING THE MEDICARE PROGRAM FOR BENEFICIARIES Sec. 201. Eliminating coverage gap. Sec. 202. Requiring two prescription drug plans to avoid Federal fallback. Sec. 203. Waiver of part D late enrollment penalty for transition period. Sec. 204. Improving the transition of full-bene\ufb01t dual eligible individuals to coverage under the medicare drug bene\ufb01t. Sec. 205. Part B premium reduction. Sec. 206. Study and report on providing incentives to preserve retiree coverage. Sec. 207. Promoting transparency in employer subsidy payments. TITLE I\u2013ELIMINATING SPECIAL INTEREST PREFERENCES SEC. 101. NEGOTIATING FAIR PRICES FOR MEDICARE PRESCRIPTION DRUGS. (a) In General.\u2013Section 1860D-11 of the Social Security Act (42 U.S.C. 1395w-111) is amended by striking subsection (i) (relating to noninterference) and by inserting the following new subsection: (i) Authority To Negotiate Prices With Manufacturers.\u2013 (1) In general.\u2013The Secretary shall have authority similar to that of other Federal entities that purchase prescription drugs in bulk to negotiate contracts with manufacturers of covered part D drugs, consistent with the requirements and in furtherance of the goals of providing quality care and containing costs under this part. (2) Required use of authority.\u2013 (A) Fallback plans.\u2013The Secretary shall exercise the authority described in paragraph (1) with respect to covered part D drugs ... ... ... Gold Meeting Our Responsibility to Medicare Bene\ufb01ciaries Act of 2005 - Amends title XVIII (Medicare) of the Social Security Act (SSA) with respect to: (1) negotiating fair prices for Medicare prescription drugs; (2) elimination of the MA Regional Plan Stabilization Fund; (3) application of risk adjustment re\ufb02ecting characteristics for the entire Medicare population in payments to Medicare advantage organizations; (4) modi\ufb01cation of the annual out-of-pocket threshold with respect to prescription drug bene\ufb01ts; (5) requiring two prescription drug plans to avoid Federal fallback; (6) waiver of the part D (Voluntary Prescription Drug Bene\ufb01t Program); (7) transition of full-bene\ufb01t dual eligible individuals to coverage under the Medicare drug bene\ufb01t; and (8) Medicare part B (Supplementary Medical Insurance) premium reduction. Directs the Secretary to study and report to Congress on providing incentives to preserve retiree coverage. Amends SSA title XVIII to direct the Secretary to make certain information regarding the sponsor of a quali\ufb01ed prescription drug plan receiving a subsidy under the prescription drug program available to the public through the Internet website of the Centers for Medicare & Medicaid Services. Model Meeting Our Responsibility to Medicare Bene\ufb01ciaries Act of 2005 - Amends title XVIII (Medicare) of the Social Security Act to repeal the prohi- bition against interference by the Secretary of Health and Human Services with the negotiations between drug manufacturers and pharmacies and prescription drug plan sponsors. Grants the Secretary authority to negotiate contracts with manufacturers of covered Medicare part D (Voluntary Prescription Drug Bene\ufb01t Program) drugs, consistent with requirements and in furtherance of the goals of providing quality care and containing costs under Medicare part D. Requires the Secretary to exercise such authority with respect to covered part D drugs offered under fallback prescription drug plans, if the Secretary determines that the negotiated prices are not fair and affordable prices compared to prices obtained by other Federal",
                "metadata": {
                    "filename": "1912.08777.pdf",
                    "page_number": 55,
                    "type": "Table"
                }
            }
        ]
    },
    "1911.10390.pdf": {
        "normalized_output": [
            {
                "Task": "Summarization",
                "Dataset": "Gigaword",
                "Metric": "ROGUE-1",
                "Result": "39.19"
            },
            {
                "Task": "Summarization",
                "Dataset": "Gigaword",
                "Metric": "ROGUE-2",
                "Result": "20.38"
            },
            {
                "Task": "Summarization",
                "Dataset": "Gigaword",
                "Metric": "ROGUE-L",
                "Result": "36.69"
            },
            {
                "Task": "Summarization",
                "Dataset": "Gigaword",
                "Metric": "BERTScore",
                "Result": "61.46"
            },
            {
                "Task": "Summarization",
                "Dataset": "Newsroom",
                "Metric": "ROGUE-1</s>",
                "Result": "45.93"
            },
            {
                "Task": "Summarization",
                "Dataset": "Newsroom",
                "Metric": "ROGUE-2</s>",
                "Result": "24.14"
            },
            {
                "Task": "Summarization",
                "Dataset": "Newsroom",
                "Metric": "ROGUE-L</s>",
                "Result": "42.51"
            },
            {
                "Task": "Summarization",
                "Dataset": "Newsroom",
                "Metric": "BERTScore",
                "Result": "66.20"
            }
        ],
        "source_documents": [
            {
                "content": "the point: Summarization with pointer-generator networks. In Proc. of ACL. Song, K.; Zhao, L.; and Liu, F. 2018. Structure-infused copy mech- anisms for abstractive summarization. In Proc. of COLING. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention is all you need. In Proc. of NIPS. Wang, L.; Raghavan, H.; Castelli, V.; Florian, R.; and Cardie, C. 2013. A sentence compression based framework to query-focused multi-document summarization. In ACL. Wang, K.; Quan, X.; and Wang, R. 2019. BiSET: bi-directional selective encoding with template for abstractive summarization. In Proc. of ACL. Weber, N.; Shekhar, L.; Balasubramanian, N.; and Cho, K. 2018. Controlling decoding for more abstractive summaries with copy- based networks. https://arxiv.org/abs/1803.07038. Yang, Y.; Huang, L.; and Ma, M. 2018. Breaking the beam search curse: A study of (re-)scoring methods and stopping criteria for neural machine translation. In EMNLP. Zhang, T.; Kishore, V.; Wu, F.; Weinberger, K. Q.; and Artzi, Y. 2019. BERTScore: Evaluating text generation with BERT. https://arxiv.org/abs/1904.09675. In",
                "metadata": {
                    "page_number": 8,
                    "filename": "1911.10390.pdf",
                    "type": "Text",
                    "start_index": 5382
                }
            },
            {
                "content": "and Smith, N. A. 2015. Toward abstractive summarization using semantic represen- tations. In Proc. of NAACL. Martins, A. F. T., and Smith, N. A. 2009. Summarization with a joint model for sentence extraction and compression. In Workshop on Integer Linear Programming for Natural Language Processing. Nallapati, R.; Zhou, B.; dos Santos, C.; Gulcehre, C.; and Xiang, B. 2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. In Proc. of SIGNLL. Nenkova, A., and McKeown, K. 2011. Automatic summarization. Foundations and Trends in Information Retrieval. Ng, J.-P., and Abrecht, V. 2015. Better summarization evaluation with word embeddings for ROUGE. In EMNLP. Ouyang, J.; Chang, S.; and McKeown, K. 2017. Crowd-sourced iterative annotation for narrative summarization corpora. In EACL. Over, P., and Yen, J. 2004. An introduction to DUC-2004. NIST. Parker, R. 2011. English Gigaword \ufb01fth edition LDC2011T07. Philadelphia: Linguistic Data Consortium. Pasunuru, R., and Bansal, M. 2018. Multi-reward reinforced sum- marization with saliency and entailment. In NAACL. Paulus, R.; Xiong, C.; and Socher, R. 2017. A deep reinforced model for abstractive summarization. In Proc. of EMNLP. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. In Proc. of NAACL. Qazvinian, V.; Radev, D. R.; Mohammad, S. M.; Dorr, B.; Zajic, D.; Whidby, M.; and Moon, T. 2013. Generating extractive sum- maries of scienti\ufb01c paradigms. JAIR. Reiter, E. 2018. A structured review of the validity of BLEU. Computational Linguistics 44(3):393\u2013401. Rush, A. M.; Chopra, S.; and Weston, J. 2015. A neural attention model for sentence summarization. In EMNLP. See, A.; Liu, P. J.; and Manning, C. D. 2017. Get to the point: Summarization with pointer-generator networks. In Proc. of ACL. Song, K.; Zhao, L.; and Liu, F. 2018. Structure-infused copy mech- anisms for abstractive summarization. In Proc. of COLING.",
                "metadata": {
                    "type": "Text",
                    "page_number": 8,
                    "start_index": 3587,
                    "filename": "1911.10390.pdf"
                }
            },
            {
                "content": "Carletta, J., and et al. 2005. The AMI meeting corpus. In MLMI. Celikyilmaz, A.; Bosselut, A.; He, X.; and Choi, Y. 2018. Deep communicating agents for abstractive summarization. In NAACL. Chen, Y.-C., and Bansal, M. 2018. Fast abstractive summarization with reinforce-selected sentence rewriting. In ACL. de Castilho, R. E.; Dore, G.; Margoni, T.; Labropoulou, P.; and Gurevych, I. 2019. A legal perspective on training models for natural language processing. In Proc. of LREC. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proc. of NAACL. Dong, L.; Yang, N.; Wang, W.; Wei, F.; Liu, X.; Wang, Y.; Gao, J.; Zhou, M.; and Hon, H.-W. 2019. Uni\ufb01ed language model pre-training for natural language understanding and generation. https://arxiv.org/abs/1905.03197. Durrett, G.; Berg-Kirkpatrick, T.; and Klein, D. 2016. Learning- based single-document summarization with compression and anaphoricity constraints. In Proc. of ACL. Fan, A.; Grangier, D.; and Auli, M. 2018. Controllable abstractive summarization. In the 2nd Workshop on NMT and Generation. Filippova, K.; Alfonseca, E.; Colmenares, C.; Kaiser, L.; and Vinyals, O. 2015. Sentence compression by deletion with lstms. In Proc. of EMNLP. Gehrmann, S.; Deng, Y.; and Rush, A. M. 2018. Bottom-up ab- stractive summarization. In Proc. of EMNLP. Grusky, M.; Naaman, M.; and Artzi, Y. 2018. NEWSROOM: A dataset of 1.3 million summaries with diverse extractive strategies. In Proc. of NAACL. Gulcehre, C.; Ahn, S.; Nallapati, R.; Zhou, B.; and Bengio, Y. 2016. Pointing the unknown words. In Proc. of ACL. Guo, H.; Pasunuru, R.; and Bansal, M. 2018. Soft, layer-speci\ufb01c multi-task summarization with entailment and question generation. In Proc. of ACL. Hardy, H., and Vlachos, A. 2018. Guided neural language gener- ation for abstractive summarization using abstract meaning repre- sentation. In Proc. of EMNLP. Hermann, K. M.; Kocisky, T.;",
                "metadata": {
                    "type": "Text",
                    "filename": "1911.10390.pdf",
                    "page_number": 8,
                    "start_index": 0
                }
            },
            {
                "content": "of ACL. Hardy, H., and Vlachos, A. 2018. Guided neural language gener- ation for abstractive summarization using abstract meaning repre- sentation. In Proc. of EMNLP. Hermann, K. M.; Kocisky, T.; Grefenstette, E.; Espeholt, L.; Kay, W.; Suleyman, M.; and Blunsom, P. 2015. Teaching machines to read and comprehend. In Proc. of NIPS. Jing, H., and McKeown, K. 1999. The decomposition of human- written summary sentences. In Proc. of SIGIR. Khandelwal, U.; Clark, K.; Jurafsky, D.; and Kaiser, L. 2019. Sam- ple ef\ufb01cient text summarization using a single pre-trained trans- former. https://arxiv.org/abs/1905.08836. Knowles, R., and Koehn, P. 2018. Context and copying in neural machine translation. In Proc. of EMNLP. Kryscinski, W.; Paulus, R.; Xiong, C.; and Socher, R. 2018. Im- proving abstraction in text summarization. In EMNLP. Lebanoff, L.; Song, K.; and Liu, F. 2018. Adapting the neural encoder-decoder framework from single to multi-document sum- marization. In EMNLP. Li, C.; Liu, F.; Weng, F.; and Liu, Y. 2013. Document summariza- tion via guided sentence compression. In EMNLP. Li, P.; Lam, W.; Bing, L.; and Wang, Z. 2017. Deep recurrent generative decoder for abstractive text summarization. In EMNLP. Liao, K.; Lebanoff, L.; and Liu, F. 2018. Abstract meaning repre- sentation for multi-document summarization. In COLING. Lin, C.-Y. 2004. ROUGE: a package for automatic evaluation of summaries. In Wksp. on Text Summarization Branches Out. Liu, Y., and Lapata, M. 2019. Hierarchical transformers for multi- document summarization. In ACL. Liu, F., and Liu, Y. 2013. Towards abstractive speech summariza- tion: Exploring unsupervised and supervised approaches for spo- ken utterance compression. IEEE Trans. ASLP 21(7):1469\u20131480. Liu, F.; Flanigan, J.; Thomson, S.; Sadeh, N.; and Smith, N. A. 2015. Toward abstractive summarization using semantic represen- tations. In Proc. of NAACL. Martins, A. F. T., and Smith, N. A. 2009. Summarization with a joint model for sentence",
                "metadata": {
                    "page_number": 8,
                    "type": "Text",
                    "filename": "1911.10390.pdf",
                    "start_index": 1794
                }
            },
            {
                "content": "Table 3: The copy rate of various summarization models. We define copy rate as the percentage of summary n-grams appearing in the source text, where n=1/2/3/4 as well as an average of them. We experiment with selecting varying amounts of seen summary tokens (@), unseen summary tokens (\u00a9), and source tokens (@) for training. A circle corresponds to about 5 million tokens for GIGAWORD and 385k tokens for NEWSROOM, which are used to compute the loss term.\nGIGAWORD NEWSROOM Training Loss 1-gram 2-gram 3-gram 4-gram | Average | R-2 | l-gram 2-gram 3-gram 4-gram | Average | R-2 a @\u20ac000 98.90 55.92 33.85 20.32 52.25 | 14.51 | 99.19 65.28 45.25 31.16 60.22 | 21.51 b. C6000 86.74 46.14 27.15 16.14 44.05 | 19.37] 92.32 57.60 38.14 25.11 53.29 | 23.93 c. @@6@C0O 80.96 40.58 23.08 13.15 39.44 | 20.00] 87.80 52.67 33.84 21.17 48.87 | 23.90 d. @@CO 73.98 34.89 19.19 10.55 34.65 | 19.20] 82.23 46.55 28.54 17.37 43.67 | 22.97 ec. C0000 98.57 56.33 35.10 = 21.72 52.93 | 15.21] 98.71 64.35 44.61 30.69 59.59 | 21.81 f. @@@O@COOO 86.29 45.91 27.07 16.06 43.83 | 19.55] 91.52 56.36 36.93 24.12 52.23 | 24.14 \u00a3 S@@OQOOOO| 8056 4032 22.66 12.87 39.10 | 20.37] 87.59 52.25 33.50 21.43 48.69 | 24.04 h @@CCOOO 74.22 35.09 19.13 10.49 34.73 | 19.39] 82.41 47.16 29.16 17.92 44.16 | 23.10",
                "metadata": {
                    "filename": "1911.10390.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "System R-1 R-2 R-L BERT-S lvt5k-1sent 35.30 16.64 32.62 \u2013 Multi-Task w/ Entailment 32.75 15.35 30.82 \u2013 SEASS 36.15 17.54 33.63 \u2013 DRGD EntailGen+QuesGen PG Networks Struct+2Way+Relation R3Sum BiSET 36.27 35.98 34.19 35.61 36.36 38.45 17.57 17.76 16.92 18.02 18.23 19.53 33.62 33.63 31.81 33.45 33.85 36.04 \u2013 \u2013 58.32 58.84 56.74 57.10 Best-\ufb01rst Search Beam Search Beam+LengthNorm 39.07 38.87 39.10 20.28 20.37 20.25 36.49 36.52 36.55 61.27 61.47 61.41 Beam+BPNorm (c=0.55) Beam+SBWR (r=0.25) 39.19 39.08 20.38 20.47 36.69 36.68 61.46 61.51",
                "metadata": {
                    "filename": "1911.10390.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5: Summarization results on the Newsroom test set. The top four systems are trained on Newsroom training data, whereas the bottom two systems are trained on Gigaword.\nSystem R-1 R-2 R-L BERT-S m PG Networks o Struct+2Way+Rel. o r s Ours (pure-ext) w e N Ours (best-abs) 39.86 40.54 43.21 45.93 19.51 20.44 21.81 24.14 36.61 37.40 40.05 42.51 62.01 62.05 63.68 66.20 a Ours (pure-ext) g 39.44 17.32 36.10 61.00 i G Ours (best-abs) 40.89 19.11 37.60 62.74",
                "metadata": {
                    "filename": "1911.10390.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 6: Human assessment of informativeness, grammati- cality, truthfulness, and best-worst scaling.\nSystem Inform. Gramm. Truthful. Bst-Wst Human 2.801 2.831 2.778 -0.001 PG Networks 2.768 2.697 2.678 -0.058 R3Sum 2.748 2.680 2.709 -0.009 BiSET 2.740 2.634 2.738 -0.006 Ours (pure-ext) 2.846 2.817 2.785 0.032 Ours (best-abs) 2.843 2.855 2.865 0.042",
                "metadata": {
                    "filename": "1911.10390.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            }
        ]
    },
    "1806.03489.pdf": {
        "normalized_output": [
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2003 - English",
                "Metric": "F1</s>",
                "Result": "94.80 (\u00b1 0.10)"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "Ontonotes v5 - English",
                "Metric": "F1</s>",
                "Result": "86.44 (\u00b1 0.14)"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2003 - English</s>",
                "Metric": "F-Score (F-S)",
                "Result": "91.73"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "Ontonotes v5 - English",
                "Metric": "F1</s>",
                "Result": "87.95"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "Ontonotes v5 - English",
                "Metric": "F1",
                "Result": "86.33"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "Ontonotes v5 - English",
                "Metric": "F1</s>",
                "Result": "90.46"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "Ontonotes v5 - English",
                "Metric": "F1</s>",
                "Result": "85.91"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "Ontonotes v5 - English",
                "Metric": "F1</s>",
                "Result": "89.75"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "Ontonotes v5 - English",
                "Metric": "F-Score (F-S)",
                "Result": "75.41"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "Ontonotes v5 - English",
                "Metric": "F1",
                "Result": "80.39"
            }
        ],
        "source_documents": [
            {
                "content": "0.002), and optimization algorithms and \ufb01xed the other hyper-parameters. We implemented our system using the Tensor\ufb02ow (Abadi et al., 2016) library, and ran our models on a GeForce GTX TITAN Xp GPU. Training requires about 2.5 hours for CONLL and 8 hours for ONTONOTES. 5.3 Results on the Development Set Table 3 shows the development set performance of our \ufb01nal models on each dataset compared to the work of Chiu and Nichols (2016). The authors use an architecture similar to ours, but use a binary gazetteer feature set, while we use our LS representation. Since our systems involve random initialization, we report the mean as well as the standard deviation over \ufb01ve runs. The improvements yielded by our model on the CONLL dataset are signi\ufb01cant although modest, while those observed on ONTONOTES are more substantial. We also observe a lower variance of our system over the 5 runs. 5.4 Results on CONLL Table 4 reports our model\u2019s performance3on the CONLL test set, as well as the performance of systems previously tested on this test set (the \ufb01gures are those published by the authors). Because of the small size of the training set, some authors (Chiu and Nichols, 2016; Yang et al., 2017; Peters et al., 2017; Peters et al., 2018) incorporated the development set as a part of training data after tuning the hyper-parameters. Consequently, their results are not directly comparable, so we do not report them. Table 4: F1 scores on the CONLL test set. The \ufb01rst four systems are feature-based, the others are neuronal. The feature con\ufb01guration of each system is encoded with: LEX which stands for LEXical feature, GAZ for GAZetteers, CAP for CAPitalization, EMB for pre-trained EMBeddings, CHE for CHaracter Embeddings, LME for Language Model Embeddings, and LS for the proposed LS feature representation. + indicates that the model uses the feature set. First, we observe that our model signi\ufb01cantly outperforms models that use extensive sets of hand- crafted features (Ratinov and Roth,",
                "metadata": {
                    "type": "Text",
                    "start_index": 0,
                    "filename": "1806.03489.pdf",
                    "page_number": 7
                }
            },
            {
                "content": "noAlphaNum. We de\ufb01ne a random lookup table for these features, and learn its parameters during training. 4.2.4 LS Vectors Contrarily to previous features, lexical vectors are computed of\ufb02ine and are not adjusted during training. We found useful in practice to apply a MinMax scaler in the range [\u22121,+1] to each LS vector we computed; thus, [..,0.095,..,0.20,..,0.76,..] becomes [..,\u22121,..,\u22120.67,..,1,..]. 5 Experiments 5.1 Data and Evaluation We consider two well-established NER benchmarks: CONLL-2003 and ONTONOTES 5.0. Table 2 provides an overview of the two datasets. As we can see, ONTONOTES is much larger. For both datasets, we convert the IOB encoding to BILOU, since Ratinov and Roth (2009) found the latter to perform better. In keeping with others, we report mention-level F1 score using the conlleval script2. The CONLL-2003 NER dataset (Tjong Kim Sang and De Meulder, 2003) is a well known collection of Reuters newswire articles that contains a large portion of sports news. It is annotated with four entity types: Person (PER), Location (LOC), Organization (ORG) and Miscellaneous (MISC). The four entity types are fairly evenly distributed, and the train/dev/test datasets present a similar type distribution. Test Table 2: Statistics of the CONLL-2003 and ONTONOTES 5.0 datasets. #tok stands for the number of tokens, and #ent indicates the number of named-entities gold annotated. The ONTONOTES 5.0 dataset (Hovy et al., 2006; Pradhan et al., 2013) includes texts from \ufb01ve different genres: broadcast conversation (200k), broadcast news (200k), magazine (120k), newswire (625k), and web data (300k). This dataset is annotated with 18 entity types, and is much larger than CONLL. Follow- ing previous researches (Chiu and Nichols, 2016; Strubell et al., 2017), we use the of\ufb01cial train/dev/test split of the CoNLL-2012 shared task (Pradhan et al., 2012). Also, we exclude (both during training and testing) the New Testaments portion as it does not contain gold NE annotations. 5.2",
                "metadata": {
                    "filename": "1806.03489.pdf",
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 0
                }
            },
            {
                "content": "suffers from the lack of manually annotated training data. Acknowledgements This work has been partly funded by the TRIBE Natural Sciences and Engineering Research Council of Canada CREATE program and Nuance Foundation. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research. We thank the anonymous reviewers for their insightful comments. References",
                "metadata": {
                    "filename": "1806.03489.pdf",
                    "type": "Text",
                    "page_number": 10,
                    "start_index": 1802
                }
            },
            {
                "content": "Matthew D Zeiler. 2012. ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.",
                "metadata": {
                    "start_index": 0,
                    "filename": "1806.03489.pdf",
                    "type": "Text",
                    "page_number": 12
                }
            },
            {
                "content": "RALI-DIRO RALI-DIRO Universit\u00b4e de Montr\u00b4eal Universit\u00b4e de Montr\u00b4eal Montr\u00b4eal, Canada Montr\u00b4eal, Canada",
                "metadata": {
                    "filename": "1806.03489.pdf",
                    "page_number": 1,
                    "type": "Table"
                }
            },
            {
                "content": "Table 1: Topmost similar entity types to a few single-word mentions (left table) and non-entity words (right table).\nWord Entity Type Sim Word Entity Type Sim /building/hotel 0.58 /location 0.47 hilton /building/restaurant 0.46 located /location/city 0.44 /person/actor 0.37 /building 0.40 gpx2 /biology /product/software 0.69 0.56 directed /person/director /art/film 0.60 0.55 jrun /product/software /product/weapon 0.64 0.23 in /date /location/city 0.58 0.54 dammstadt /location/city /location/railway 0.45 0.44 won /award 0.53 /event/sports event 0.53",
                "metadata": {
                    "filename": "1806.03489.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Dataset Train Dev CONLL-2003 #tok 204,567 51,578 #ent 23,499 5,942 ONTONOTES 5.0 #tok 1,088,503 147,724 152,728 #ent 81,828 11,066",
                "metadata": {
                    "filename": "1806.03489.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3 shows the development set performance of our \ufb01nal models on each dataset compared to the work of Chiu and Nichols (2016). The authors use an architecture similar to ours, but use a binary gazetteer feature set, while we use our LS representation. Since our systems involve random initialization, we report the mean as well as the standard deviation over \ufb01ve runs. The improvements yielded by our model on the CONLL dataset are signi\ufb01cant although modest, while those observed on ONTONOTES are more substantial. We also observe a lower variance of our system over the 5 runs.\nCONLL ONTONOTES (Chiu and Nichols, 2016) 94.03 (\u00b1 0.23) 84.57 (\u00b1 0.27) Our model 94.80 (\u00b1 0.10) 86.44 (\u00b1 0.14)",
                "metadata": {
                    "filename": "1806.03489.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4 reports our model\u2019s performance3on the CONLL test set, as well as the performance of systems previously tested on this test set (the \ufb01gures are those published by the authors). Because of the small size of the training set, some authors (Chiu and Nichols, 2016; Yang et al., 2017; Peters et al., 2017; Peters et al., 2018) incorporated the development set as a part of training data after tuning the hyper-parameters. Consequently, their results are not directly comparable, so we do not report them.\nModel LEX GAZ CAP EMB CHE LME LS F1 (Finkel et al., 2005) + + + \u2022 \u2022 \u2022 \u2022 86.86 (Ratinov and Roth, 2009) + + + \u2022 \u2022 \u2022 \u2022 90.88 (Lin and Wu, 2009) + + + \u2022 \u2022 \u2022 \u2022 90.90 (Luo et al., 2015) + + + \u2022 \u2022 \u2022 \u2022 91.20 (Collobert et al., 2011) \u2022 + + + \u2022 \u2022 \u2022 89.56 (Huang et al., 2015) \u2022 \u2022 + + + \u2022 \u2022 90.10 (Lample et al., 2016) \u2022 \u2022 + + + \u2022 \u2022 90.94 (Ma and Hovy, 2016) \u2022 \u2022 + + + \u2022 \u2022 91.21 (Shen et al., 2017) \u2022 \u2022 + + \u2022 \u2022 \u2022 90.89 (Strubell et al., 2017) \u2022 \u2022 + + \u2022 \u2022 \u2022 90.54 (Tran et al., 2017) \u2022 \u2022 + + + + \u2022 91.69 (Liu et al., 2017) \u2022 \u2022 + + + + \u2022 91.71 This work \u2022 + + + + \u2022 + 91.73",
                "metadata": {
                    "filename": "1806.03489.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5 reports the F1 score of our system compared to the performance reported by others on the ONTONOTES test set. To the best of our knowledge, we surpass previously reported F1 scores on this dataset. In particular, our system signi\ufb01cantly outperforms the Bi-LSTM-CNN-CRF models of (Chiu and Nichols, 2016) and (Strubell et al., 2017) by an absolute gain of 1.68 and 0.96 points respectively. Less surprisingly, it surpasses systems with hand-crafted features, including Ratinov and Roth (2009) that use gazetteers, and the system of Durrett and Klein (2014) which uses coreference annotation in ONTONOTES to jointly model NER, entity linking, and coreference resolution tasks.\nModel LEX GAZ CAP EMB CHE LME LS F1 (Finkel and Manning, 2009) + + + \u2022 \u2022 \u2022 \u2022 82.42 (Ratinov and Roth, 2009) + + + \u2022 \u2022 \u2022 \u2022 84.88 (Passos et al., 2014) + + + \u2022 \u2022 \u2022 \u2022 82.24 (Durrett and Klein, 2014) + + + \u2022 \u2022 \u2022 \u2022 84.04 (Chiu and Nichols, 2016) \u2022 + + + + \u2022 \u2022 86.28 (Shen et al., 2017) \u2022 \u2022 + + + \u2022 \u2022 86.52 (Strubell et al., 2017) \u2022 \u2022 + + + \u2022 \u2022 86.99 This work \u2022 + + + + \u2022 + 87.95",
                "metadata": {
                    "filename": "1806.03489.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 6: Per-genre F1 scores on ONTONOTES (numbers taken from Chiu and Nichols (2016)). BC = broadcast conversation, BN = broadcast news, MZ = magazine, NW = newswire, TC = telephone conversation, WB = blogs and newsgroups.\nModel BC BN MZ NW TC WB (Finkel and Manning, 2009) 78.66 87.29 82.45 85.50 67.27 72.56 (Durrett and Klein, 2014) 78.88 87.39 82.46 87.60 72.68 76.17 (Chiu and Nichols, 2016) 85.23 89.93 84.45 88.39 72.39 78.38 This work 86.33 90.46 85.91 89.75 75.41 80.39",
                "metadata": {
                    "filename": "1806.03489.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 7: F1 scores of differently trained systems on CONLL and ONTONOTES 5.0 datasets. Capitaliza- tion (Section 4.2.3) and character features (Section 4.2.2) are used by default by all models.\nModel CONLL ONTONOTES SSKIP 90.52 (\u00b1 0.18) 86.57 (\u00b1 0.10) LS 89.94 (\u00b1 0.16) 85.92 (\u00b1 0.12) all 91.73 (\u00b1 0.10) 87.95 (\u00b1 0.13)",
                "metadata": {
                    "filename": "1806.03489.pdf",
                    "page_number": 9,
                    "type": "Table"
                }
            }
        ]
    },
    "1603.01354.pdf": {
        "normalized_output": [
            {
                "Task": "Part-of-Speech (POS) Tagging",
                "Dataset": "Penn Treebank (PTB)",
                "Metric": "Accuracy",
                "Result": "97.55"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2003 - English</s>",
                "Metric": "F1",
                "Result": "91.21"
            }
        ],
        "source_documents": [
            {
                "content": "samples from [- Weight Matrices and Bias Vectors. Matrix pa- rameters are randomly initialized with uniform 6 /_6 r+c? + r+e are the number of of rows and columns in the structure (Glorot and Bengio, 2010). Bias vec- tors are initialized to zero, except the bias by for the forget gate in LSTM , which is initialized to 1.0 (Jozefowicz et al., 2015). ], where r and c 3.2 Optimization Algorithm Parameter optimization is performed with mini- batch stochastic gradient descent (SGD) with batch size 10 and momentum 0.9. We choose an initial learning rate of \u03b70 (\u03b70 = 0.01 for POS tag- ging, and 0.015 for NER, see Section 3.3.), and the learning rate is updated on each epoch of training as \u03b7t = \u03b70/(1+\u03c1t), with decay rate \u03c1 = 0.05 and t is the number of epoch completed. To reduce the effects of \u201cgradient exploding\u201d, we use a gradient clipping of 5.0 (Pascanu et al., 2012). We explored other more sophisticated optimization algorithms such as AdaDelta (Zeiler, 2012), Adam (Kingma and Ba, 2014) or RMSProp (Dauphin et al., 2015), but none of them meaningfully improve upon SGD with momentum and gradient clipping in our pre- liminary experiments. Early Stopping. We use early stopping (Giles, 2001; Graves et al., 2013) based on performance on validation sets. The \u201cbest\u201d parameters appear at around 50 epochs, according to our experiments. 2http://ronan.collobert.com/senna/",
                "metadata": {
                    "start_index": 1795,
                    "page_number": 4,
                    "type": "Text",
                    "filename": "1603.01354.pdf"
                }
            },
            {
                "content": "splits \u2014 section 0\u201318 as training data, section 19\u2013 21 as development data and section 22\u201324 as test data (Manning, 2011; S\u00f8gaard, 2011). NER. For NER, We perform experiments on the English data from CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003). This data set contains four different types of named entities: PERSON, LOCATION, ORGA- NIZATION, and MISC. We use the BIOES tag- ging scheme instead of standard BIO2, as pre- vious studies have reported meaningful improve- ment with this scheme (Ratinov and Roth, 2009; Dai et al., 2015; Lample et al., 2016). The corpora statistics are shown in Table 2. We did not perform any pre-processing for data sets, leaving our system truly end-to-end. 4.2 Main Results We \ufb01rst run experiments to dissect the effective- ness of each component (layer) of our neural net- work architecture by ablation studies. We com- pare the performance with three baseline systems \u2014 BRNN, the bi-direction RNN; BLSTM, the bi- direction LSTM, and BLSTM-CNNs, the combi- nation of BLSTM with CNN to model character- level information. All these models are run using Stanford\u2019s GloVe 100 dimensional word embed- dings and the same hyper-parameters as shown in Table 1. According to the results shown in Ta- ble 3, BLSTM obtains better performance than BRNN on all evaluation metrics of both the two tasks. BLSTM-CNN models signi\ufb01cantly outper- form the BLSTM model, showing that character- level representations are important for linguistic sequence labeling tasks. This is consistent with",
                "metadata": {
                    "type": "Text",
                    "filename": "1603.01354.pdf",
                    "start_index": 1801,
                    "page_number": 5
                }
            },
            {
                "content": "Fine Tuning. For each of the embeddings, we \ufb01ne-tune initial embeddings, modifying them dur- ing gradient updates of the neural network model by back-propagating gradients. The effectiveness of this method has been previously explored in se- quential and structured prediction problems (Col- lobert et al., 2011; Peng and Dredze, 2015). Dropout Training. To mitigate over\ufb01tting, we ap- ply the dropout method (Srivastava et al., 2014) to regularize our model. As shown in Figure 1 and 3, we apply dropout on character embeddings before inputting to CNN, and on both the input and out- put vectors of BLSTM. We \ufb01x dropout rate at 0.5 for all dropout layers through all the experiments. We obtain signi\ufb01cant improvements on model per- formance after using dropout (see Section 4.5). 3.3 Tuning Hyper-Parameters Table 1 summarizes the chosen hyper-parameters for all experiments. We tune the hyper-parameters on the development sets by random search. Due to time constrains it is infeasible to do a ran- dom search across the full hyper-parameter space. Thus, for the tasks of POS tagging and NER we try to share as many hyper-parameters as possible. Note that the \ufb01nal hyper-parameters for these two tasks are almost the same, except the initial learn- ing rate. We set the state size of LSTM to 200. Tuning this parameter did not signi\ufb01cantly impact the performance of our model. For CNN, we use 30 \ufb01lters with window length 3. 4 Experiments 4.1 Data Sets As mentioned before, we evaluate our neural net- work model on two sequence labeling tasks: POS tagging and NER. POS Tagging. For English POS tagging, we use the Wall Street Journal (WSJ) portion of Penn Treebank (PTB) (Marcus et al., 1993), which con- tains 45 different POS tags. In order to com- pare with previous work, we adopt the standard splits \u2014 section 0\u201318 as training data, section 19\u2013 21 as development data and section 22\u201324 as test data (Manning, 2011; S\u00f8gaard, 2011). NER. For NER, We perform experiments on the English data from",
                "metadata": {
                    "start_index": 0,
                    "filename": "1603.01354.pdf",
                    "type": "Text",
                    "page_number": 5
                }
            },
            {
                "content": "Figure 3: The main architecture of our neural network. The character representation for each word is computed by the CNN in Figure 1. Then the character representation vector is concatenated with the word embedding before feeding into the BLSTM network. Dashed arrows indicate dropout layers applied on both the input and output vectors of BLSTM. improve the performance of our model (see Sec- tion 4.5 for details). 3 Network Training In this section, we provide details about training the neural network. We implement the neural net- work using the Theano library (Bergstra et al., 2010). The computations for a single model are run on a GeForce GTX TITAN X GPU. Using the settings discussed in this section, the model train- ing requires about 12 hours for POS tagging and 8 hours for NER. 3.1 Parameter Initialization Word Embeddings. We use Stanford\u2019s pub- licly available GloVe 100-dimensional embed- dings1 trained on 6 billion words from Wikipedia and web text (Pennington et al., 2014) 1http://nlp.stanford.edu/projects/ glove/ We also run experiments on two other sets of published embeddings, namely Senna 50- dimensional embeddings?| trained on Wikipedia and Reuters RCV-1 corpus (Collobert et al., 2017), and Google\u2019s Word2Vec 300-dimensional embed- dings} trained on 100 billion words from Google News (Mikolov et al., 2013). To test the effec- tiveness of pretrained word embeddings, we ex- perimented with randomly initialized embeddings with 100 dimensions, where embeddings are uni- formly sampled from range [\u2014 Baty al where dim is the dimension of embeddings fet al., 2015). The performance of different word embeddings is discussed in Section |4.4] Character Embeddings. Character embed- dings are initialized with uniform samples from [\u2212 dim,+ dim], where we set dim = 30. samples from [- Weight Matrices and Bias Vectors. Matrix pa- rameters are randomly initialized with uniform 6 /_6 r+c? + r+e are the number of of rows and columns in the structure (Glorot and Bengio,",
                "metadata": {
                    "start_index": 0,
                    "type": "Text",
                    "page_number": 4,
                    "filename": "1603.01354.pdf"
                }
            },
            {
                "content": "Table 1: Hyper-parameters for all experiments.\nLayer Hyper-parameter POS NER CNN window size number of \ufb01lters 3 30 3 30 state size 200 200 LSTM initial state 0.0 0.0 peepholes no no Dropout dropout rate 0.5 0.5 batch size 10 10 initial learning rate 0.01 0.015 decay rate 0.05 0.05 gradient clipping 5.0 5.0",
                "metadata": {
                    "filename": "1603.01354.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Corpora statistics. SENT and TOKEN refer to the number of sentences and tokens in each data set.\nDataset WSJ CoNLL2003 Train SENT 38,219 TOKEN 912,344 14,987 204,567 Dev SENT 5,527 TOKEN 131,768 3,466 51,578 Test SENT 5,462 TOKEN 129,654 3,684 46,666",
                "metadata": {
                    "filename": "1603.01354.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Performance of our model on both the development and test sets of the two tasks, together with three baseline systems.\nPOS NER Dev Test Dev Test Model Acc. Acc. Prec. Recall F1 Prec. Recall F1 BRNN 96.56 96.76 92.04 89.13 90.56 87.05 83.88 85.44 BLSTM 96.88 96.93 92.31 90.85 91.57 87.77 86.23 87.00 BLSTM-CNN 97.34 97.33 92.52 93.64 93.07 88.53 90.21 89.36 BRNN-CNN-CRF 97.46 97.55 94.85 94.63 94.74 91.35 91.06 91.21",
                "metadata": {
                    "filename": "1603.01354.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: POS tagging accuracy of our model on test data from WSJ proportion of PTB, together with top-performance systems. The neural net- work based models are marked with \u2021.\nModel Acc. Gim\u00b4enez and M`arquez (2004) 97.16 Toutanova et al. (2003) 97.27 Manning (2011) 97.28 Collobert et al. (2011)\u2021 97.29 Santos and Zadrozny (2014)\u2021 97.32 Shen et al. (2007) 97.33 Sun (2014) 97.36 S\u00f8gaard (2011) 97.50 This paper 97.55",
                "metadata": {
                    "filename": "1603.01354.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4 illustrates the results of our model for POS tagging, together with seven previous top- performance systems for comparison. Our model signi\ufb01cantly outperform Senna (Collobert et al., 2011), which is a feed-forward neural network model using capitalization and discrete suf\ufb01x fea- tures, and data pre-processing. Moreover, our model achieves 0.23% improvements on accu- racy over the \u201cCharWNN\u201d (Santos and Zadrozny, 2014), which is a neural network model based on Senna and also uses CNNs to model character- level representations. This demonstrates the effec- tiveness of BLSTM for modeling sequential data\nModel F1 Chieu and Ng (2002) 88.31 Florian et al. (2003) 88.76 Ando and Zhang (2005) 89.31 Collobert et al. (2011)\u2021 89.59 Huang et al. (2015)\u2021 90.10 Chiu and Nichols (2015)\u2021 90.77 Ratinov and Roth (2009) 90.80 Lin and Wu (2009) 90.90 Passos et al. (2014) 90.90 Lample et al. (2016)\u2021 Luo et al. (2015) This paper 90.94 91.20 91.21",
                "metadata": {
                    "filename": "1603.01354.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5 shows the F1 scores of previous models for NER on the test data set from CoNLL-2003 shared task. For the purpose of comparison, we list their results together with ours. Similar to the observations of POS tagging, our model achieves signi\ufb01cant improvements over Senna and the other three neural models, namely the LSTM-CRF pro- posed by Huang et al. (2015), LSTM-CNNs pro-\nEmbedding Dimension POS NER Random 100 97.13 80.76 Senna 50 97.44 90.28 Word2Vec 300 97.40 84.91 GloVe 100 97.55 91.21",
                "metadata": {
                    "filename": "1603.01354.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 7: Results with and without dropout on two tasks (accuracy for POS tagging and F1 for NER).\nPOS NER Train Dev Test Train Dev Test No 98.46 97.06 97.11 99.97 93.51 89.25 Yes 97.86 97.46 97.55 99.63 94.74 91.21",
                "metadata": {
                    "filename": "1603.01354.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 8: Statistics of the partition on each corpus. It lists the number of tokens of each subset for POS tagging and the number of entities for NER.\nPOS NER Dev Test Dev Test IV 127,247 125,826 4,616 3,773 OOTV 2,960 2,412 1,087 1,597 OOEV 659 588 44 8 OOBV 902 828 195 270",
                "metadata": {
                    "filename": "1603.01354.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 7 compares the results with and without dropout layers for each data set. All other hyper- parameters remain the same as in Table 1. We observe a essential improvement for both the two tasks. It demonstrates the effectiveness of dropout in reducing over\ufb01tting.\nPOS Dev Test IV OOTV OOEV OOBV IV OOTV OOEV OOBV LSTM-CNN 97.57 93.75 90.29 80.27 97.55 93.45 90.14 80.07 LSTM-CNN-CRF 97.68 93.65 91.05 82.71 97.77 93.16 90.65 82.49 NER Dev Test IV OOTV OOEV OOBV IV OOTV OOEV OOBV LSTM-CNN 94.83 87.28 96.55 82.90 90.07 89.45 100.00 78.44 LSTM-CNN-CRF 96.49 88.63 97.67 86.91 92.14 90.73 100.00 80.60",
                "metadata": {
                    "filename": "1603.01354.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            }
        ]
    },
    "1804.09849.pdf": {
        "normalized_output": [
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201914 EN-FR</s>",
                "Metric": "BLEU",
                "Result": "41.00"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201914 EN-DE</s>",
                "Metric": "BLEU",
                "Result": "28.49"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201914 EN-FR</s>",
                "Metric": "BLEU",
                "Result": "41.67"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201914 EN-DE",
                "Metric": "BLEU",
                "Result": "28.84"
            }
        ],
        "source_documents": [
            {
                "content": "different architectures to other tasks? And what are the characteristic errors that each architecture makes, e.g., linguistic plausibility? Acknowledgments We would like to thank the entire Google Brain Team and Google Translate Team for their foun- dational contributions to this project. We would also like to thank Noam Shazeer, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, and the entire Tensor2Tensor development team for their useful inputs and discussions. References",
                "metadata": {
                    "start_index": 0,
                    "type": "Text",
                    "filename": "1804.09849.pdf",
                    "page_number": 9
                }
            },
            {
                "content": "speed and superior model quality. To further stabilize training, we also use adap- tive gradient clipping. We discard a training step completely if an anomaly in the gradient norm value is detected, which is usually an indication of an imminent gradient explosion. More specif- ically, we keep track of a moving average and a moving standard deviation of the log of the gradi- ent norm values, and we abort a step if the norm of the gradient exceeds four standard deviations of the moving average. 4.2 Model Analysis and Comparison In this section, we compare the results of RNMT+ with ConvS2S and Transformer. All models were trained with synchronous training. RNMT+ and ConvS2S were trained with 32 NVIDIA P100 GPUs while the Transformer Base and Big models were trained using 16 GPUs. For RNMT+, we use sentence-level cross- entropy loss. Each training batch contained 4096 sentence pairs (4096 source sequences and 4096 target sequences). For ConvS2S and Transformer models, we use token-level cross-entropy loss. Each training batch contained 65536 source to- kens and 65536 target tokens. For the GNMT baselines on both tasks, we cite the largest BLEU score reported in (Wu et al., 2016) without rein- forcement learning. Table 1 shows our results on the WMT\u201914 En\u2192Fr task. Both the Transformer Big model and RNMT+ outperform GNMT and ConvS2S by about 2 BLEU points. RNMT+ is slightly better than the Transformer Big model in terms of its mean BLEU score. RNMT+ also yields a much lower standard deviation, and hence we observed much less \ufb02uctuation in the training curve. takes approximately 3 days for the Transformer It",
                "metadata": {
                    "page_number": 5,
                    "type": "Text",
                    "filename": "1804.09849.pdf",
                    "start_index": 1804
                }
            },
            {
                "content": "and how much can be attributed to the associated training and inference techniques. In some cases, these new techniques may be broadly applicable to other architectures and thus constitute a major, though implicit, contribution of an architecture pa- per. Clearly, they need to be considered in order to ensure a fair comparison across different model architectures. In this paper, we therefore take a step back and look at which techniques and methods contribute signi\ufb01cantly to the success of recent architectures, namely ConvS2S and Transformer, and explore applying these methods to other architectures, in- cluding RNMT models. In doing so, we come up with an enhanced version of RNMT, referred to as RNMT+, that signi\ufb01cantly outperforms all in- dividual architectures in our setup. We further in- troduce new architectures built with different com- ponents borrowed from RNMT+, ConvS2S and Transformer. In order to ensure a fair setting for comparison, all architectures were implemented in the same framework, use the same pre-processed data and apply no further post-processing as this may confound bare model performance. Our contributions are three-fold: We quickly note two prior works that pro- vided empirical solutions to the dif\ufb01culty of train- ing NMT architectures (speci\ufb01cally RNMT). In (Britz et al., 2017) the authors systematically ex- plore which elements of NMT architectures have a signi\ufb01cant impact on translation quality. (Denkowski and Neubig, 2017) the authors recom- mend three speci\ufb01c techniques for strengthening NMT systems and empirically demonstrated how incorporating those techniques improves the relia- bility of the experimental results. In 2 Background In this section, we brie\ufb02y discuss the commmonly used NMT architectures. 2.1 RNN-based NMT Models - RNMT RNMT models are composed of an encoder RNN and a decoder RNN, coupled with an attention network. The encoder summarizes the input se- quence into a set of vectors while the decoder con- ditions on the",
                "metadata": {
                    "page_number": 2,
                    "start_index": 0,
                    "filename": "1804.09849.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "A Supplemental Material A.1 ConvS2S For the WMT\u201914 En\u2192De task, both the encoder and decoder have 15 layers, with 512 hidden units in the \ufb01rst ten layers, 768 units in the subsequent three layers and 2048 units in the \ufb01nal two lay- ers. The \ufb01rst 13 layers use kernel width 3 and the \ufb01nal two layers use kernel width 1. For the WMT\u201914 En\u2192Fr task, both the encoder and de- coder have 14 layers, with 512 hidden units in the \ufb01rst \ufb01ve layers, 768 units in the subsequent four layers, 1024 units in the next three layers, 2048 units and 4096 units in the \ufb01nal two layers. The \ufb01rst 12 layers use kernel width 3 and the \ufb01nal two layers use kernel width 1. We train the ConvS2S models with synchronous training using 32 GPUs. A.2 Transformer Both the encoder and the decoder have 6 Trans- former layers. Transformer base model has model dimension 512, hidden dimension 2048 and 8 at- tention heads. The Transformer Big model uses model dimension 1024, hidden dimension 8192 and 16 attention heads. We group the dropout in Transformer models into four types: input dropout - dropout applied to the sum of token embeddings and position encodings, residual dropout - dropout applied to the output of each sublayer before added to the sublayer input, relu dropout - dropout applied to the inner layer output after ReLU ac- tivation in each feed-forward sub-layer, attention dropout - dropout applied to attention weight in each attention sub-layer. All Transformer models use the following learning rate schedule: where t is the current step, p is the number of warmup steps, dmodel is the model dimension and r0 is a constant to adjust the magnitude of the learning rate. On WMT\u201914 En\u2192De, the Transformer Base model employs all four types of dropout with dropout probs = 0.1. We use r0 = 2.0 and p = 8000 in the learning rate schedule. For the Trans- former Big model, only residual dropout and input dropout are applied, both with dropout probs = 0.3. r0 = 3.0 and p = 40000 are used in the learn- ing rate",
                "metadata": {
                    "page_number": 11,
                    "filename": "1804.09849.pdf",
                    "start_index": 0,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results on WMT14 En\u2192Fr. The num- bers before and after \u2019\u00b1\u2019 are the mean and stan- dard deviation of test BLEU score over an evalua- tion window.\nModel Test BLEU Epochs Training Time GNMT 38.95 - - ConvS2S 7 39.49 \u00b1 0.11 62.2 438h Trans. Base 39.43 \u00b1 0.17 20.7 90h Trans. Big 8 40.73 \u00b1 0.19 8.3 120h RNMT+ 41.00 \u00b1 0.05 8.5 120h",
                "metadata": {
                    "filename": "1804.09849.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Results on WMT14 En\u2192De.\nModel Test BLEU Epochs Training Time GNMT 24.67 - - ConvS2S 25.01 \u00b10.17 38 20h Trans. Base 27.26 \u00b1 0.15 38 17h Trans. Big 27.94 \u00b1 0.18 26.9 48h RNMT+ 28.49 \u00b1 0.05 24.6 40h",
                "metadata": {
                    "filename": "1804.09849.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Performance comparison. Examples/s are normalized by the number of GPUs used in the training job. FLOPs are computed assuming that source and target sequence length are both 50.\nModel Examples/s FLOPs Params ConvS2S 80 15.7B 263.4M Trans. Base 160 6.2B 93.3M Trans. Big 50 31.2B 375.4M RNMT+ 30 28.1B 378.9M",
                "metadata": {
                    "filename": "1804.09849.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: Ablation results of RNMT+ and the Transformer Big model on WMT\u201914 En \u2192 Fr. We report average BLEU scores on the test set. An as- terisk \u2019*\u2019 indicates an unstable training run (train- ing halts due to non-\ufb01nite elements).\nModel RNMT+ Trans. Big Baseline 41.00 40.73 - Label Smoothing 40.33 40.49 - Multi-head Attention 40.44 39.83 - Layer Norm. * * - Sync. Training 39.68 *",
                "metadata": {
                    "filename": "1804.09849.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5: Results for encoder-decoder hybrids.\nEncoder Decoder En\u2192Fr Test BLEU Trans. Big Trans. Big 40.73 \u00b1 0.19 RNMT+ RNMT+ 41.00 \u00b1 0.05 Trans. Big RNMT+ 41.12 \u00b1 0.16 RNMT+ Trans. Big 39.92 \u00b1 0.21",
                "metadata": {
                    "filename": "1804.09849.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 6: Results for hybrids with cascaded en- coder and multi-column encoder.\nModel En\u2192Fr BLEU En\u2192De BLEU Trans. Big 40.73 \u00b1 0.19 27.94 \u00b1 0.18 RNMT+ 41.00 \u00b1 0.05 28.49 \u00b1 0.05 Cascaded 41.67 \u00b1 0.11 28.62 \u00b1 0.06 MultiCol 41.66 \u00b1 0.11 28.84 \u00b1 0.06",
                "metadata": {
                    "filename": "1804.09849.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            }
        ]
    },
    "D18-1105.pdf": {
        "normalized_output": [
            {
                "Task": "Dialogue Act Classification",
                "Dataset": "Switchboard Dialog Act Corpus (SWDA)",
                "Metric": "Accuracy",
                "Result": "83.1"
            },
            {
                "Task": "Dialogue Act Classification",
                "Dataset": "ICSI Meeting Recorder Dialog Act Corpus (MRDA)",
                "Metric": "Accuracy",
                "Result": "86.7"
            }
        ],
        "source_documents": [
            {
                "content": "Tasks, pages 143\u2013148, Taipei, Taiwan. Asian Feder- ation of Natural Language Processing.",
                "metadata": {
                    "type": "Text",
                    "filename": "D18-1105.pdf",
                    "page_number": 7,
                    "start_index": 0
                }
            },
            {
                "content": "References Automatic Speech Recognition and Understanding, pages 88\u201395.",
                "metadata": {
                    "filename": "D18-1105.pdf",
                    "page_number": 6,
                    "start_index": 0,
                    "type": "Text"
                }
            },
            {
                "content": "We conduct our experimental evaluation on two dialog act benchmark datasets. Table 1: Dialog Act Datasets Statistics Table 1 summarizes dataset statistics. We use the train, validation and test splits as de\ufb01ned in (Lee and Dernoncourt, 2016; Ortega and Vu, 2017). 3.2 Experimental Setup We setup our experimental evaluation, as follows: given a classi\ufb01cation task and a dataset, we gen- erate an on-device model. The size of the model can be con\ufb01gured (by adjusting the projection ma- trix P) to \ufb01t in the memory footprint of the de- vice, i.e. a phone has more memory compared to a smart watch. For each classi\ufb01cation task, we re- port Accuracy on the test set. 3.3 Hyperparameter and Training For both datasets we used the following: 2- layer SGNN (PT=80,d=14 \u00d7 FullyConnected256 \u00d7 FullyConnected256), mini-batch size of 100, dropout rate of 0.25, learning rate was initialized to 0.025 with cosine annealing decay (Loshchilov and Hutter, 2016). Unlike prior approaches (Lee and Dernoncourt, 2016; Ortega and Vu, 2017) that rely on pre-trained word embeddings, we learn the projection weights on the \ufb02y during training, i.e word embeddings (or vocabularies) do not need to be stored. Instead, features are computed on the \ufb02y and are dynamically compressed via the pro- jection matrices into projection vectors. These val- ues were chosen via a grid search on development sets, we do not perform any other dataset-speci\ufb01c tuning. Training is performed through stochastic gradient descent over shuf\ufb02ed mini-batches with Nesterov momentum optimizer (Sutskever et al., 2013), run for 1M steps. 4 Results Tables 2 and 3 show results on the SwDA and MRDA dialog act datasets. Overall, our SGNN model consistently outperforms the baselines and prior state-of-the-art deep learning models.",
                "metadata": {
                    "page_number": 4,
                    "start_index": 1811,
                    "type": "Text",
                    "filename": "D18-1105.pdf"
                }
            },
            {
                "content": "4.1 Baselines We compare our model against a majority class baseline and Naive Bayes classi\ufb01er (Lee and Der- noncourt, 2016). Our model signi\ufb01cantly outper- forms both baselines by 12 to 35% absolute. 4.2 Comparison against State-of-art Methods We also compare our performance against prior work using HMMs (Stolcke et al., 2000) and re- cent deep learning methods like CNN (Lee and Dernoncourt, 2016), RNN (Khanpour et al., 2016) and RNN with gated attention (Tran et al., 2017). To the best of our knowledge, (Lee and Der- noncourt, 2016; Ortega and Vu, 2017; Tran et al., 2017) are the latest approaches in dialog act clas- si\ufb01cation, which also reported on the same data splits. Therefore, we compare our research against these works. According to (Ortega and Vu, 2017), prior work by (Ji and Bilmes, 2006) achieved promising results on the MRDA dataset, but since the evaluation was conducted on a different data split, it is hard to compare them directly. For both SwDA and MRDA datasets, our SGNNs obtains the best result of 83.1 and 86.7 ac- curacy outperforming prior state-of-the-art work. This is very impressive given that we work with very small memory footprint and we do not rely on pre-trained word embeddings. Our study also shows that the proposed method is very effective for such natural language tasks compared to more complex neural network architectures such as deep CNN (Lee and Dernoncourt, 2016) and RNN vari- ants (Khanpour et al., 2016; Ortega and Vu, 2017). We believe that the compression techniques like locality sensitive projections jointly coupled with non-linear functions are effective at capturing low- dimensional semantic text representations that are useful for text classi\ufb01cation applications. 4.3 Discussion on Model Size and Inference LSTMs have millions of parameters, while our on-device architecture has just 300K parameters (order of magnitude lower). Most deep learning methods also use large vocabulary size of 10K or higher. Each word embedding is",
                "metadata": {
                    "type": "Text",
                    "page_number": 5,
                    "filename": "D18-1105.pdf",
                    "start_index": 0
                }
            },
            {
                "content": "Table 1: Dialog Act Datasets Statistics\nDatasets Class Vocab. Train Validation Test SwDA 42 20K 193K 23K 5K MRDA 5 12K 78K 16K 15K",
                "metadata": {
                    "filename": "D18-1105.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Method Acc. Majority Class (baseline) (Ortega and Vu, 2017) 33.7 Naive Bayes (baseline) (Khanpour et al., 2016) 47.3 HMM (Stolcke et al., 2000) 71.0 DRLM-conditional training (Ji and Bilmes, 2006) 77.0 DRLM-joint training (Ji and Bilmes, 2006) 74.0 LSTM (Lee and Dernoncourt, 2016) 69.9 CNN (Lee and Dernoncourt, 2016) 73.1 Gated-Attention&HMM (Tran et al., 2017) 74.2 RNN+Attention (Ortega and Vu, 2017) 73.8 RNN (Khanpour et al., 2016) 80.1 SGNN: Self-Governing Neural Network (ours) 83.1",
                "metadata": {
                    "filename": "D18-1105.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: SwDA Dataset Results\nMethod Acc. Majority Class (baseline)(Ortega and Vu, 2017) 59.1 Naive Bayes (baseline) (Khanpour et al., 2016) 74.6 Graphical Model (Ji and Bilmes, 2006) 81.3 CNN (Lee and Dernoncourt, 2016) 84.6 RNN+Attention(Ortega and Vu, 2017) 84.3 RNN (Khanpour et al., 2016) 86.8 SGNN: Self-Governing Neural Network (ours) 86.7",
                "metadata": {
                    "filename": "D18-1105.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1809.08370.pdf": {
        "normalized_output": [
            {
                "Task": "Combinatory Categorial Grammar (CCG) Supertagging",
                "Dataset": "CCGBank",
                "Metric": "Accuracy",
                "Result": "96.05"
            },
            {
                "Task": "Text Chunking",
                "Dataset": "CoNLL-2000",
                "Metric": "F1",
                "Result": "96.98"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2003 - English",
                "Metric": "F1",
                "Result": "92.61"
            },
            {
                "Task": "Named Entity Recognition (NER)</s>",
                "Dataset": "Ontonotes v5 - English",
                "Metric": "F1",
                "Result": "88.81"
            },
            {
                "Task": "Part-of-Speech (POS) Tagging",
                "Dataset": "Penn Treebank (PTB)",
                "Metric": "Accuracy",
                "Result": "97.74"
            },
            {
                "Task": "Dependency Parsing",
                "Dataset": "Penn Treebank (PTB)",
                "Metric": "Unlabeled Attachment Score",
                "Result": "96.61"
            },
            {
                "Task": "Dependency Parsing",
                "Dataset": "Penn Treebank (PTB)",
                "Metric": "Labeled Attachment Score",
                "Result": "95.02"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "IWSLT\u201915 EN-VI</s>",
                "Metric": "BLEU",
                "Result": "29.58"
            }
        ],
        "source_documents": [
            {
                "content": "Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger model has four times as many hidden units as the others, making it similar in size to the models when ELMo is included. For dependency parsing, we omit results from Choe and Charniak (2016), Kuncoro et al. (2017), and Liu and Zhang (2017) because these train constituency parsers and convert the system outputs to dependency parses. They produce higher scores, but have access to more information during training and do not apply to datasets without constituency annotations. * denotes semi-supervised and \u2020 denotes multi-task. Parameter Word Embeddings Initializiation Character Embedding Size Character CNN Filter Widths Character CNN Num Filters Encoder LSTM sizes Encoder LSTM sizes, \u201cLarge\u201d model LSTM projection layer size Hidden layer sizes Dropout EMA coef\ufb01cient 0.5/(1 + 0.005t0.5) (t is number of SGD updates so far) Learning rate Momentum Batch size Table 6: Hyperparameters for the model.",
                "metadata": {
                    "start_index": 0,
                    "filename": "1809.08370.pdf",
                    "page_number": 16,
                    "type": "Text"
                }
            },
            {
                "content": "several strong baselines on seven tasks: Combinatory Categorial Grammar (CCG) Su- pertagging: We use data from CCGBank (Hock- enmaier and Steedman, 2007). Text Chunking: We use the CoNLL-2000 data (Tjong Kim Sang and Buchholz, 2000). Named Entity Recognition (NER): We use the CoNLL-2003 data (Tjong Kim Sang and De Meul- der, 2003). Fine-Grained NER (FGN): We use the OntoNotes (Hovy et al., 2006) dataset. Part-of-Speech (POS) Tagging: We use the Wall Street Journal portion of the Penn Treebank (Mar- cus et al., 1993). Dependency Parsing: We use the Penn Treebank converted to Stanford Dependencies version 3.3.0. Machine Translation: We use the English- Vietnamese translation dataset from IWSLT 2015 (Cettolo et al., 2015). We report (tokenized) BLEU scores on the tst2013 test set. We use the 1 Billion Word Language Model Benchmark (Chelba et al., 2014) as a pool of un- labeled sentences for semi-supervised learning. 4.1 Model Details and Baselines We apply dropout during training, but not when running the primary prediction module to pro- duce soft targets on unlabeled examples. In ad- dition to the auxiliary prediction modules listed in Section 3, we \ufb01nd it slightly improves re- sults to add another one that sees the whole in- put rather than a subset (but unlike the primary prediction module, does have dropout applied to its representations). Unless indicated otherwise, our models have LSTMs with 1024-sized hidden states and 512-sized projection layers. See the ap- pendix for full training details and hyperparame- ters. We compare CVT with the following other semi-supervised learning algorithms: Word Dropout. In this method, we only train the primary prediction module. When acting as a teacher it is run as normal, but when acting as a student, we randomly replace some of the input words with a REMOVED token. This is similar to CVT in that it exposes the model to a restricted view of the input. However, it is less data ef\ufb01- cient. By carefully designing the auxiliary",
                "metadata": {
                    "page_number": 5,
                    "filename": "1809.08370.pdf",
                    "start_index": 1800,
                    "type": "Text"
                }
            },
            {
                "content": "A Detailed Results We provide a more detailed version of the test set results in the paper, adding two decimals of pre- cision, standard deviations of the 5 runs for each model, and more prior work, in Table 5.",
                "metadata": {
                    "type": "Text",
                    "start_index": 0,
                    "page_number": 12,
                    "filename": "1809.08370.pdf"
                }
            },
            {
                "content": "multi-task CVT with and without producing all-tasks-labeled examples. Model Generalization. In order to evaluate how our models generalize to the dev set from the train set, we plot the dev vs. train accuracy for our dif- ferent methods as they learn (see Figure 4). Both CVT and multi-task learning improve model gen- eralization: for the same train accuracy, the mod- els get better dev accuracy than purely supervised learning. Interestingly, CVT continues to improve in dev set accuracy while close to 100% train ac- curacy for CCG, Chunking, and NER, perhaps be- cause the model is still learning from unlabeled data even when it has completely \ufb01t to the train set. We also show results for a smaller multi-task + CVT model. Although it generalizes at least as well as the larger one, it halts making progress on the train set earlier. This suggests it is important to use suf\ufb01ciently large neural networks for multi- task learning: otherwise the model does not have the capacity to \ufb01t to all the training data. Auxiliary Prediction Module Ablation. We brie\ufb02y explore which auxiliary prediction modules are more important for the sequence tagging tasks in Table 3. We \ufb01nd that both kinds of auxiliary prediction modules improve performance, but that the future and past modules improve results more than the forward and backward ones, perhaps be- cause they see a more restricted and challenging view of the input. Table 3: Ablation study on auxiliary prediction mod- ules for sequence tagging. Training Models on Small Datasets. We ex- plore how CVT scales with dataset size by vary- ing the amount of training data the model has ac- cess to. Unsurprisingly, the improvement of CVT",
                "metadata": {
                    "filename": "1809.08370.pdf",
                    "type": "Text",
                    "start_index": 1798,
                    "page_number": 7
                }
            },
            {
                "content": "Auxiliary 1: They traveled to Auxiliary 2: They traveled to Washington Auxiliary 3: Washington by plane Auxiliary 4: by plane",
                "metadata": {
                    "filename": "1809.08370.pdf",
                    "page_number": 2,
                    "type": "Table"
                }
            },
            {
                "content": "Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around 0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with them included. The +Large model has four times as many hidden units as the others, making it similar in size to the models when ELMo is included. * denotes semi-supervised and \u2020 denotes multi-task.\nMethod CCG Chunk NER FGN POS Dep. Parse Translate Acc. F1 F1 F1 Acc. UAS LAS BLEU Shortcut LSTM (Wu et al., 2017) 95.1 97.53 ID-CNN-CRF (Strubell et al., 2017) 90.7 86.8 JMT\u2020 (Hashimoto et al., 2017) 95.8 97.55 94.7 92.9 TagLM* (Peters et al., 2017) 96.4 91.9 ELMo* (Peters et al., 2018) 92.2 Biaf\ufb01ne (Dozat and Manning, 2017) 95.7 94.1 Stack Pointer (Ma et al., 2018) 95.9 94.2 Stanford (Luong and Manning, 2015) 23.3 Google (Luong et al., 2017) 26.1 Supervised 94.9 95.1 91.2 87.5 97.60 95.1 93.3 28.9 Virtual Adversarial Training* 95.1 95.1 91.8 87.9 97.64 95.4 93.7 \u2013 Word Dropout* 95.2 95.8 92.1 88.1 97.66 95.6 93.8 29.3 ELMo (our implementation)* 95.8 96.5 92.2 88.5 97.72 96.2 94.4 29.3 ELMo + Multi-task*\u2020 95.9 96.8 92.3 88.4 97.79 96.4 94.8 \u2013 CVT* 95.7 96.6 92.3 88.7 97.70 95.9 94.1 29.6 CVT + Multi-task*\u2020 96.0 96.9 92.4 88.4 97.76 96.4 94.8 \u2013 CVT + Multi-task + Large*\u2020 96.1 97.0 92.6 88.8 97.74 96.6 95.0 \u2013",
                "metadata": {
                    "filename": "1809.08370.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Dev set performance of multi-task CVT with and without producing all-tasks-labeled examples.\nModel CCG Chnk NER FGN POS Dep. CVT-MT 95.7 97.4 96.0 86.7 97.74 94.4 w/out all-labeled 95.4 97.1 95.6 86.3 97.71 94.1",
                "metadata": {
                    "filename": "1809.08370.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Ablation study on auxiliary prediction mod- ules for sequence tagging.\nModel CCG Chnk NER FGN POS Supervised 94.8 95.5 95.0 86.0 97.59 CVT 95.6 97.0 95.9 87.3 97.66 no fwd/bwd \u20130.1 \u20130.2 \u20130.2 \u20130.1 \u20130.01 no future/past \u20130.3 \u20130.4 \u20130.4 \u20130.3 \u20130.04",
                "metadata": {
                    "filename": "1809.08370.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: Comparison of single-task models on the dev sets. \u201cCVT-MT frozen\u201d means we pretrain a CVT + multi-task model on \ufb01ve tasks, and then train only the prediction module for the sixth. \u201cELMo frozen\u201d means we train prediction modules (but no LSTMs) on top of ELMo embeddings.\nModel CCG Chnk NER FGN POS Dep. Supervised 94.8 95.6 95.0 86.0 97.59 92.9 CVT-MT frozen ELMo frozen 95.1 96.6 94.3 92.2 94.6 83.2 97.66 92.5 91.3 80.6 97.50 89.4",
                "metadata": {
                    "filename": "1809.08370.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger model has four times as many hidden units as the others, making it similar in size to the models when ELMo is included. For dependency parsing, we omit results from Choe and Charniak (2016), Kuncoro et al. (2017), and Liu and Zhang (2017) because these train constituency parsers and convert the system outputs to dependency parses. They produce higher scores, but have access to more information during training and do not apply to datasets without constituency annotations. * denotes semi-supervised and \u2020 denotes multi-task.\nMethod CCG Acc. Chunking F1 NER F1 FGN F1 POS Acc. Dependency Parsing UAS LAS Translation BLEU LSTM-CNN-CRF (Ma and Hovy, 2016) 91.21 97.55 LSTM-CNN (Chiu and Nichols, 2016) 91.62 \u00b1 0.33 86.28 \u00b1 0.26 ID-CNN-CRF (Strubell et al., 2017) 90.65 \u00b1 0.15 86.84 \u00b1 0.19 Tri-Trained LSTM (Lewis et al., 2016) 94.7 Shortcut LSTM (Wu et al., 2017) 95.08 97.53 JMT* (Hashimoto et al., 2017) 95.77 97.55 94.67 92.90 LM-LSTM-CNN-CRF (Liu et al., 2017) 95.96 \u00b1 0.08 91.71 \u00b1 0.10 97.53 \u00b1 0.03 TagLM\u2020 (Peters et al., 2017) 96.37 \u00b1 0.05 91.93 \u00b1 0.19 ELMo\u2020 (Peters et al., 2018) 92.22 \u00b1 0.10 NPM (Ma and Hovy, 2017) 94.9 93.0 Deep Biaf\ufb01ne (Dozat and Manning, 2017) 95.74 94.08 Stack Pointer (Ma et al., 2018) 95.87 94.19 Stanford (Luong and Manning, 2015) 23.3 Google (Luong et al., 2017) 26.1 Supervised 94.94 \u00b1 0.02 95.10 \u00b1 0.06 91.16 \u00b1 0.09 87.48 \u00b1 0.08 97.60 \u00b1 0.02 95.08 \u00b1 0.03 93.27 \u00b1 0.03 28.88 \u00b1 0.12 Virtual Adversarial Training* 95.07 \u00b1 0.04 95.06 \u00b1 0.06 91.75 \u00b1 0.10 87.91 \u00b1 0.11 97.64 \u00b1 0.03 95.44 \u00b1 0.06 93.72 \u00b1 0.07 \u2013 Word Dropout* 95.20 \u00b1 0.04 95.79 \u00b1 0.08 92.14 \u00b1 0.11 88.06 \u00b1 0.09 97.66 \u00b1 0.01 95.56 \u00b1 0.05 93.80 \u00b1 0.08 29.33 \u00b1 0.10 ELMo* 95.79 \u00b1 0.04 96.50 \u00b1 0.03 92.24 \u00b1 0.09 88.49 \u00b1 0.12 97.72 \u00b1 0.01 96.22 \u00b1 0.05 94.44 \u00b1 0.06 29.34 \u00b1 0.11 ELMo + Multi-task*\u2020 95.91 \u00b1 0.05 96.83 \u00b1 0.03 92.32 \u00b1 0.12 88.37 \u00b1 0.16 97.79 \u00b1 0.03 96.40 \u00b1 0.04 94.79 \u00b1 0.05 \u2013 CVT* 95.65 \u00b1 0.04 96.58 \u00b1 0.04 92.34 \u00b1 0.06 88.68 \u00b1 0.14 97.70 \u00b1 0.03 95.86 \u00b1 0.03 94.06 \u00b1 0.02 29.58 \u00b1 0.07 CVT + Multi-Task*\u2020 95.97 \u00b1 0.04 96.85 \u00b1 0.05 92.42 \u00b1 0.08 88.42 \u00b1 0.13 97.76 \u00b1 0.02 96.44 \u00b1 0.04 94.83 \u00b1 0.06 \u2013 CVT + Multi-Task + Large*\u2020 96.05 \u00b1 0.03 96.98 \u00b1 0.05 92.61 \u00b1 0.09 88.81 \u00b1 0.09 97.74 \u00b1 0.02 96.61 \u00b1 0.04 95.02 \u00b1 0.04 \u2013",
                "metadata": {
                    "filename": "1809.08370.pdf",
                    "page_number": 16,
                    "type": "Table"
                }
            },
            {
                "content": "Value 300d GloVe 6B 50 [2, 3, 4] 300 (100 per \ufb01lter width) 1024 for the \ufb01rst layer, 512 for the second one 4096 for the \ufb01rst layer, 2048 for the second one 512 512 0.5 for labeled examples, 0.8 for unlabeled examples 0.998 0.9 64 sentences",
                "metadata": {
                    "filename": "1809.08370.pdf",
                    "page_number": 16,
                    "type": "Table"
                }
            },
            {
                "content": "Table 6: Hyperparameters for the model.\nMethod CIFAR-10 CIFAR-10+ 4000 labels GAN (Salimans et al., 2016) \u2013 18.63 \u00b1 2.32 Stochastic Transformations (Sajjadi et al., 2016) \u2013 11.29 \u00b1 0.24 \u03a0 model (Laine and Aila, 2017) 16.55 \u00b1 0.29 12.36 \u00b1 0.31 Temporal Ensemble (Laine and Aila, 2017) \u2013 12.16 \u00b1 0.24 Mean Teacher (Tarvainen and Valpola, 2017) \u2013 12.31 \u00b1 0.28 Complement GAN (Dai et al., 2017) 14.41 \u00b1 0.30 \u2013 VAT (Miyato et al., 2017b) 13.15 10.55 VAdD (Park et al., 2017) \u2013 11.68 \u00b1 0.19 VAdD + VAT (Park et al., 2017) \u2013 10.07 \u00b1 0.11 SNGT + \u03a0 model (Luong et al., 2017) 13.62 \u00b1 0.17 11.00 \u00b1 0.36 SNGT + VAT (Luong et al., 2017) 12.49 \u00b1 0.36 9.89 \u00b1 0.34 Consistency + WGAN (Wei et al., 2018) \u2013 9.98 \u00b1 0.21 Manifold Mixup (Verma et al., 2018) \u2013 10.26 \u00b1 0.32 Supervised 23.61 \u00b1 0.60 19.61 \u00b1 0.56 VAT (ours) 13.29 \u00b1 0.33 10.90 \u00b1 0.31 CVT, no input perturbation 14.63 \u00b1 0.20 12.44 \u00b1 0.27 CVT, random input perturbation 13.80 \u00b1 0.30 11.10 \u00b1 0.26 CVT, adversarial input perturbation 12.01 \u00b1 0.11 10.11 \u00b1 0.15",
                "metadata": {
                    "filename": "1809.08370.pdf",
                    "page_number": 17,
                    "type": "Table"
                }
            }
        ]
    },
    "C18-1139.pdf": {
        "normalized_output": [
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2003 - English",
                "Metric": "F-Score (F-S)",
                "Result": "93.09"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2003 - German</s>",
                "Metric": "F-Score (F-S)",
                "Result": "88.32"
            },
            {
                "Task": "Text Chunking",
                "Dataset": "CoNLL-2000</s>",
                "Metric": "F-Score (F-S)",
                "Result": "96.72"
            },
            {
                "Task": "Part-of-Speech (POS) Tagging",
                "Dataset": "Penn Treebank (PTB)",
                "Metric": "Accuracy",
                "Result": "97.85"
            }
        ],
        "source_documents": [
            {
                "content": "Table 2: Summary of evaluation results or all proposed setups and baselines. We also list the best published scores for each task for reference. We significantly outperform all previous works on NER, and slightly outperform the previous state-of-the-art in PoS tagging and chunking. 3.2 Model Training and Parameters Character-level language models. We train our LMs using SGD to perform truncated backpropagation through time (BPTT) with a window length of 250, a non-annealed learning rate of 20.0, a batch size of 100, clipping gradients at 0.25 and dropout probabilities of 0.25. We prepare the data by unking-the rarest 0.0001 percent of characters. We set the number of hidden states of the (one-layered) LSTM to 2048. We halt training by tracking the performance on the validation set, stopping when negligible gains were observed, or after 1 week, whichever came first. We train our English models on the 1-billion word corpus (Chelba et al., 2013) and our German models on a corpus of half a billion words aggregated from various open source corpora in the OPUS project\u201d. After training, we find the models trained for the full week achieve character level perplexity on the supplied test-set of 2.42 for English and 2.38 for German. Resources did not allow us to train for longer or on larger language model variants. We hypothesize that more resources and time would permit learning an even better model. Sequence tagging model. We train the sequence tagging model using vanilla SGD with no momentum, clipping gradients at 5, for 150 epochs. We employ a simple learning rate annealing method in which we halve the learning rate if training loss does not fall for 5 consecutive epochs. Following recommendations of Reimers et al. (2017)\u2019s in-depth analysis of hyperparameters in sequence labeling, we utilize varia- tional dropout, set the number of hidden states per-layer of the LSTM to 256, set the number of LSTM layers to 1, and perform model selection over the learning rate \u20ac",
                "metadata": {
                    "start_index": 0,
                    "type": "Text",
                    "page_number": 7,
                    "filename": "C18-1139.pdf"
                }
            },
            {
                "content": "Scott Menard. 2018. Applied logistic regression analysis, volume 106. SAGE publications.",
                "metadata": {
                    "page_number": 11,
                    "type": "Text",
                    "start_index": 0,
                    "filename": "C18-1139.pdf"
                }
            },
            {
                "content": "labeling, we utilize varia- tional dropout, set the number of hidden states per-layer of the LSTM to 256, set the number of LSTM layers to 1, and perform model selection over the learning rate \u20ac {0.01,0.05,0.1} and mini-batch size \u20ac {8, 16,32}, choosing the model with the best F-measure (for NER and chunking) or accuracy (for PoS) in the best epoch as judged by performance on the validation set. Following Peters et al. (2017), we then repeat the experiment for the model chosen 5 times with different random seeds, and train using both train and development set, reporting both average performance and standard deviation over these runs on the test set as final performance. Classic word embeddings. Following Reimers et al. (2017), we use GLOVE embeddings for English NER and KOMNIOS embeddings (Komninos and Manandhar, 2016) for PoS tagging and chunking. For German, we use the German FASTTEXT embeddings (Grave et al., 2018). In configurations that train character features, we apply a BiLSTM with 25 hidden states to each word separately and extract the final hidden output states (see Lample et al. (2016)). >We aggregate over the PARACRAWL, EUROPARL, OPENSUBTITLES2018, and WIKIPEDIA releases in OPUS for versatile text and to allow reproduction of our results.",
                "metadata": {
                    "start_index": 1798,
                    "page_number": 7,
                    "type": "Text",
                    "filename": "C18-1139.pdf"
                }
            },
            {
                "content": "to isolate the impact of our proposed embeddings vis-a-vis earlier approaches. The setups are as follows: Proposed approach. We evaluate our proposed contextual string embeddings in the following configu- rations:",
                "metadata": {
                    "start_index": 1797,
                    "type": "Text",
                    "page_number": 6,
                    "filename": "C18-1139.pdf"
                }
            },
            {
                "content": "Alan Akbik Duncan Blythe Roland Vollgraf Zalando Research Zalando Research Zalando Research MiihlenstraBe 25 MiihlenstraBe 25 MiihlenstraBe 25 10243 Berlin 10243 Berlin 10243 Berlin {firstname.lastname}@zalando.de",
                "metadata": {
                    "filename": "C18-1139.pdf",
                    "page_number": 1,
                    "type": "Table"
                }
            },
            {
                "content": "Table 1: Summary of evaluation results for best configuration of proposed architecture, and current best published results. The proposed approach significantly outperforms previous work on the CONLLO3 NER task for German and English and slightly outperforms previous works on CONLL2000 chunking and Penn treebank PoS tagging.\nTask PROPOSED Previous best NER English 93.09-0.12 92.22+0.1 (Peters et al., 2018) NER German 88.32\u00a30.2 78.76 (Lample et al., 2016) Chunking 96.72+0.05 96.37+0.05 (Peters et al., 2017) PoS tagging 97.8540.01 97.64 (Choi, 2016)",
                "metadata": {
                    "filename": "C18-1139.pdf",
                    "page_number": 3,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Summary of evaluation results or all proposed setups and baselines. We also list the best published scores for each task for reference. We significantly outperform all previous works on NER, and slightly outperform the previous state-of-the-art in PoS tagging and chunking.\nNER-English NER-German Chunking POS Approach F1-score F1-score F1-score Accuracy proposed PROPOSED 91.97+0.04 85.78 + 96.68+0.03 97.73\u00a30.02 PROPOSED.worp 93.07+\u00a30.10 88.20 4 96.70+0.04 97.82+\u00a30.02 PROPOSEDscuar 91.92+0.03 85.88 4 96.72+0.05 97.8+0.01 PROPOSEDsworp+cuar 93.090.12 88.32 + 96.71\u00a30.07 97.76+0.01 PROPOSED, ai 92.72+0.09 n/a 96.65+0.05 97.85+0.01 baselines HUANG 88.54+0.08 82.32 + 0.35 95.4+0.08 96.94+0.02 LAMPLE 89.3+0.23 83.78 + 0.39 95.34+0.06 97.02+0.03 PETERS 92.34+0.09 n/a 96.69+0.05 97.81+ 0.02 best published 92.22+0.10 78.76 96.3740.05 97.64 (Peters et al., 2018) (Lample et al., 2016) (Peters et al., 2017) (Choi, 2016) 91.93+0.19 77.20 95.96+0.08 97.55 (Peters et al., 2017) (Seyler et al., 2017) (Liu et al., 2017) (Ma and Hovy, 2016) 91.71+\u00a30.10 76.22 95.77 97.53\u00a30.03 (Liu et al., 2017) (Gillick et al., 2015) | (Hashimoto et al., 2016) (Liu et al., 2017) 91.21 75.72 95.56 97.30 (Ma and Hovy, 2016) (Qi et al., 2009) Sggaard et al. (2016) (Lample et al., 2016)",
                "metadata": {
                    "filename": "C18-1139.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Additional ablation experiment in which we evaluate our approach without BiLSTM and CRF. Here, we instead use simple linear map over the embeddings to determine their direct information content.\nNER-English NER-German Chunking POS Embedding + Architecture F1-score F1-score F1-score Accuracy PROPOSED,worp +BiLSTM-CRF 93.07 + 0.10 88.20 + 0.21 96.70+ 0.04 97.82 + 0.02 +Map-CRF 90.17 + 0.06 85.17+0.04 96.054 0.04 97.62+0.01 +Map 79.86 + 0.12 76.97\u00a30.16 90.55+0.05 97.35+0.01 PROPOSED +BiLSTM-CRF 91.97 + 0.04 78 \u00a30.18 96.68+0.03 97.73 + 0.02 +Map-CRF 88.62 + 0.15 95.96 \u00a30.05 97.53 + 0.02 +Map 81.42 + 0.16 90.50+ 0.06 97.26 + 0.01 CLASSIC WORD EMBEDDINGS +BiLSTM-CRF 88.54 + 0.08 95.40+0.08 96.94 + 0.02 +Map-CRF 66.53 + 0.03 91.26+0.04 94.06 + 0.02 +Map 48.79 + 0.27 65.0140.50 89.58 + 0.02",
                "metadata": {
                    "filename": "C18-1139.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: Examples of the word \u201cWashington\u201d in different contexts in the CONLLO3 data set, and nearest neighbors using cosine distance over our proposed embeddings. Since our approach produces different embeddings based on context, we retrieve different nearest neighbors for each mention of the same word.\nword context neighbors Washington (a) Washington to curb support for [..] (1) Washington would also take [..] action [..] (2) Russia to clamp down on barter deals [..] (3) Brazil to use hovercrafts for [..] Washington = (b) /..] Anthony Washington (U.S.) [..] (1) [..] Carla Sacramento ( Portugal ) [..] (2) [..] Charles Austin ( U.S. ) [..] (3) [..] Steve Backley ( Britain ) [..] Washington (c) [..] flown to Washington for [..] (1) [..] while visiting Washington to [..] (2) [..] journey to New York City and Washington [..] (14) [..] lives in Chicago [..] Washington (d) [..] when Washington came charging back [..] (A) [..] point for victory when Washington found [..] (4) [..] before England struck back with [..] (6) [..] before Ethiopia won the spot kick decider [..] Washington (e) [..] said Washington [..] (1) [..] subdue the never-say-die Washington [..] (4) [..] a private school in Washington [..] (9) [..] said Florida manager John Boles [..]",
                "metadata": {
                    "filename": "C18-1139.pdf",
                    "page_number": 9,
                    "type": "Table"
                }
            }
        ]
    },
    "2020.emnlp-main.152.pdf": {
        "normalized_output": [
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "ATIS",
                "Metric": "F-Score (F-S)",
                "Result": "96.22"
            },
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "ATIS",
                "Metric": "Accuracy",
                "Result": "97.11"
            },
            {
                "Task": "Linguistic Acceptability",
                "Dataset": "ATIS",
                "Metric": "Accuracy",
                "Result": "86.96"
            },
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "SNIPS",
                "Metric": "F-Score (F-S)",
                "Result": "93.72"
            },
            {
                "Task": "Intent Detection and Slot Filling",
                "Dataset": "SNIPS",
                "Metric": "Accuracy",
                "Result": "97.44"
            },
            {
                "Task": "Linguistic Acceptability",
                "Dataset": "SNIPS",
                "Metric": "Accuracy",
                "Result": "84.38"
            }
        ],
        "source_documents": [
            {
                "content": "References",
                "metadata": {
                    "filename": "2020.emnlp-main.152.pdf",
                    "page_number": 6,
                    "type": "Text",
                    "start_index": 0
                }
            },
            {
                "content": "works combine the autoregressive model and CRF to achieve the competitive performance, which therefore are set as our baseline methods. Slot \ufb01lling (SF) and intent detection (ID) play im- portant roles in spoken language understanding, especially for task-oriented dialogue system. For example, for an utterance like \u201cBuy an air ticket from Beijing to Seattle\u201d, intent detection works on sentence-level to indicate the task is about purchas- ing an air ticket, while the slot \ufb01lling focus on words-level to \ufb01gure out the departure and destina- tion of that ticket are \u201cBeijing\u201d and \u201cSeattle\u201d. In early studies, ID and SF were often modeled separately, where ID was modeled as a classi\ufb01ca- tion task, while SF was regarded as a sequence labeling task. Due to the correlation between these two tasks, training them jointly could enhance each other. Zhang and Wang (2016) propose a joint model using bidirectional gated recurrent unit to learn the representation at each time step. Mean- while, a max-pooling layer is employed to capture the global features of a sentence for intent classi- \ufb01cation. Liu and Lane (2016) cast the slot \ufb01lling task as a tag generation problem and introduce a However, for SF task, we argue that identifying token dependencies among slot chunk is enough, and it is unnecessary to model the entire sequence dependency in autoregressive fashion, which leads to redundant computation and inevitable high la- tency. In this study, we cast these two tasks jointly as a non-autoregressive tag generation problem to get rid of unnecessary temporal dependencies. Partic- ularly, a Transformer (Vaswani et al., 2017) based architecture is adopted here to learn the represen- tations of an utterance in both sentence and word level simultaneously (Sec.\u00a72.1). The slots and in- tent labels are predicted independently and simulta- neously, achieving better decoding ef\ufb01ciency. We further introduce a two-pass re\ufb01ne mechanism (in Sec.\u00a72.2) to model boundary prediction of each slots",
                "metadata": {
                    "start_index": 1812,
                    "page_number": 1,
                    "type": "Text",
                    "filename": "2020.emnlp-main.152.pdf"
                }
            },
            {
                "content": "slot boundaries. Speedup As each slot tagging result can be cal- culated in parallel with our approach, inference latency can be signi\ufb01cantly reduced. As shown in Table 2, on ATIS test set, our non-autoregressive model could achieve \u00d78.80 speedup compared with the existing state-of-the-art model (Haihong et al., 2019). And after introducing two-pass mech- anism (SlotRe\ufb01ne), our model still achieves com- petitive inference speedup (\u00d74.31). Our decoding Table 2: \u201cLatency\u201d is the average time to decode an ut- terance without minibatching. \u201cSpeedup\u201d is compared against existing SOTA model (Haihong et al., 2019). is conducted with a single Tesla P40 GPU. It is worth noting that for long sentences (Length\u226512), the speedup achieves \u00d713 (not reported in table). Two-Pass Mechanism v.s. CRF In SF task, CRF is usually used to learn the dependence of slot labels. Two most important dependence rules CRF learned can be summarized as tag O can only be fol- lowed by O or B and tag B-* can only be followed by same-type label I-* or O, which can be per- fectly addressed with our proposed two-pass mech- anism. Experiments about +CRF can be found in Table 1&2 (\u201cOur Joint Model +CRF\u201d), we can see that two-pass mechanism equipped SlotRe\ufb01ne out- performs +CRF by averagely +0.89, meanwhile preserving \u00d72.8 speedup, demonstrating that two- pass mechanism can be a better substitute for CRF in this task for better performance and ef\ufb01ciency. Remedy Uncoordinated Slots in Training We visualize the number decrease of uncoordinated slots of the training process on ATIS dataset. As depicted in Figure 3, uncoordinated errors of both \u201cOne-Pass\u201d and \u201cTwo-Pass\u201d models decrease with training goes. Notably, the uncoordinated slots number of Two-Pass model drops signi\ufb01cantly",
                "metadata": {
                    "page_number": 4,
                    "start_index": 1804,
                    "type": "Text",
                    "filename": "2020.emnlp-main.152.pdf"
                }
            },
            {
                "content": "introduce more iterations (e.g., 10 iters) to achieve competitive performance, which largely reduces the inference speed. 3 Experiment Datasets We choose two widely-used datasets: ATIS (Airline Travel Information Systems,Tur et al. (2010)) and Snips (collected by Snips personal voice assistant,Coucke et al. (2018)). Compared with ATIS, the Snips dataset is more complex due to its large vocabulary size, cross-domain intents and more out-of-vocabulary words. Metrics Three evaluation metrics are used in our experiments. F1-score and accuracy are applied for slot \ufb01lling and intent detection task, respectively. Besides, we use sentence accuracy to indicate pro- portion of utterance in the corpus whose slots and intent are both correctly-predicted. Setup All embeddings are initialized with xavier method (Glorot and Bengio, 2010). The batch size is set to 32 and learning rate is 0.001. we set num- ber of Transformer layers, attention heads and hid- den sizes to {2,8,64} and {4,16,96} for ATIS and Snips datasets. In addition, we report the results of previous studies (Hakkani-T\u00a8ur et al., 2016; Liu and Lane, 2016; Goo et al., 2018; Haihong et al., 2019; Qin et al., 2019) and conduct speed evalua- tion based on their open-source codes. Main Results Table 1 summarizes the model per- formance on ATIS and snips corpus. It can be seen that SlotRe\ufb01ne consistently outperforms other base- lines in all three metrics. Compared with our ba- sic non-autoregressive joint model in Section\u00a7 2.1, SlotRe\ufb01ne achieve +1.18 and +1.55 sentence-level accuracy improvements for ATIS and Snips, respec- tively. It is worthy noting that our SlotRe\ufb01ne sig- ni\ufb01cantly improves the slot \ufb01lling task (F1-score\u2191). we attribute the improvement to that our two-pass mechanism successfully makes the model learn better slot boundaries. Speedup As each slot tagging result can be cal- culated in parallel with our approach, inference latency can be signi\ufb01cantly reduced. As shown in Table 2, on ATIS test set, our",
                "metadata": {
                    "start_index": 0,
                    "page_number": 4,
                    "type": "Text",
                    "filename": "2020.emnlp-main.152.pdf"
                }
            },
            {
                "content": "=m I kn tT tT t tT tT tT Play The Rolling Stone's Love in Vain",
                "metadata": {
                    "filename": "2020.emnlp-main.152.pdf",
                    "page_number": 2,
                    "type": "Table"
                }
            },
            {
                "content": "Table 1: Performance comparison on ATIS and Snips datasets. \u201c\u2191\u201dindicates signi\ufb01cant difference (p < 0.05) with previous works. Model name written in bold refer to ours.\nModel Slot ATIS Dataset Intent Sent Slot Snips Dataset Intent Sent Joint Seq (Hakkani-T\u00a8ur et al., 2016) 94.30 92.60 80.70 87.30 96.90 73.20 Atten.-Based (Liu and Lane, 2016) 94.20 91.10 78.90 87.80 96.70 74.10 Sloted-Gated (Goo et al., 2018) 95.42 95.41 83.73 89.27 96.86 76.43 SF-ID (w/o CRF) (Haihong et al., 2019) 95.50 96.58 86.00 90.46 97.00 78.37 SF-ID (w/ CRF) (Haihong et al., 2019) 95.80 97.09 86.90 92.23 97.29 80.43 Stack-Propagation (Qin et al., 2019) 95.90 96.90 86.50 94.20 98.00 86.90 Our Joint Model (in Sec.\u00a72.1) 95.33 96.84 85.78 93.13 97.21 82.83 Our Joint Model +CRF 95.71 96.54 85.71 93.22 96.79 82.51 SlotRe\ufb01ne 96.22\u2191 97.11\u2191 86.96\u2191 93.72 97.44 84.38",
                "metadata": {
                    "filename": "2020.emnlp-main.152.pdf",
                    "page_number": 3,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: \u201cLatency\u201d is the average time to decode an ut- terance without minibatching. \u201cSpeedup\u201d is compared against existing SOTA model (Haihong et al., 2019).\nModel Latency Speedup Sloted-Gated 11.31ms 1.41\u00d7 SF-ID (with CRF) 13.03ms 1.22\u00d7 Stack-Propagation 15.94ms 1.00\u00d7 Our Joint Model 1.48ms 10.77\u00d7 Our Joint Model +CRF 8.32ms 1.92\u00d7 SlotRe\ufb01ne 3.02ms 4.31\u00d7",
                "metadata": {
                    "filename": "2020.emnlp-main.152.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Performance comparison between SlotRe\ufb01ne with GloVe initialzation and Bert based model on ATIS and Snips datasets.\nModel ATIS Dataset Slot Intent Sent Snips Dataset Slot Intent Sent Joint Model 95.33 96.84 85.78 93.31 97.21 82.83 Joint Model with CRF 95.71 96.54 85.71 93.22 96.79 82.51 SlotRe\ufb01ne 96.22 97.11 86.96 93.72 97.44 84.38 SlotRe\ufb01ne with GloVe 96.24 97.35 87.57 96.33 98.36 91.06 SlotRe\ufb01ne with BERT 96.16 97.74 88.64 97.05 99.04 92.96 previous work with pretraining BERT-Joint (Chen et al., 2019) 96.10 97.50 88.20 97.00 98.60 92.80 Stack-Propagation with BERT (Qin et al., 2019) 96.10 97.50 88.60 97.00 99.00 92.90",
                "metadata": {
                    "filename": "2020.emnlp-main.152.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1911.09483.pdf": {
        "normalized_output": [
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201914 EN-DE",
                "Metric": "BLEU",
                "Result": "29.9"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201914 EN-FR",
                "Metric": "BLEU",
                "Result": "43.5"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "IWSLT\u201914 EN-DE",
                "Metric": "BLEU",
                "Result": "36.3"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "IWSLT\u201915 EN-VI",
                "Metric": "BLEU",
                "Result": "31.3"
            }
        ],
        "source_documents": [
            {
                "content": "training hyper-parameters are tuned on the validation set. MUSE-large For training MUSE-large, following Ott et al. (2018), parameters are updated every 32 steps. We train the model for 80K updates with a batch size of 5120 for En-Fr, and train the model for 30K updates with a batch size of 3584 for En-De. The dropout rate is set to 0.1 for En-Fr and 0.3 for En-De. We borrow the setup of optimizer from Wu et al. (2019a) and use the cosine learning rate schedule with 10000 warmup steps. The max learning rate is set to 0.001 on En-De translation and 0.0007 on En-Fr translation. For checkpoint averaging, following Wu et al. (2019a), we tune the average checkpoints for En-De translation tasks. For En-Fr translation, we do not average checkpoint but use the \ufb01nal single checkpoint. MUSE-base We train and test MUSE-base on two small datasets, IWSLT 2014 De-En translation and IWSLT2015 En-Vi translation. Following Vaswani et al. (2017), we use Adam optimizer with a learning rate of 0.001. We use the warmup mechanism and invert the learning rate decay with warmup updates of 4K. For the De-En dataset, we train the model for 20K steps with a batch size of 4K. The parameters are updated every 4 steps. The dropout rate is set to 0.4. For the En-Vi dataset, we train the model for 10K steps with a batch size of 4K. The parameters are also updated every 4 steps. The dropout rate is set to 0.3. We save checkpoints every epoch and average the last 10 checkpoints for inference. During inference, we adopt beam search with a beam size of 5 for De-En, En-Fr and En-Vi trans- lation tasks. The length penalty is set to 0.8 for En-Fr according to the validation results, 1 for the two small datasets following the default setting of Ott et al. (2019). We do not tune beam width and length penalty but use the setting reported in Vaswani et al. (2017). The BLEU1 metric is adopted to evaluate the model performance during evaluation.",
                "metadata": {
                    "page_number": 5,
                    "type": "Text",
                    "start_index": 1799,
                    "filename": "1911.09483.pdf"
                }
            },
            {
                "content": "be seen as a token feature extractor. where W1, b1, W2, and b2 are projection parameters. 3 EXPERIMENT We evaluate MUSE on four machine translation tasks. This section describes the datasets, experi- mental settings, detailed results, and analysis. WMT14 En-Fr and En-De datasets The WMT 2014 English-French translation dataset, consisting of 36M sentence pairs, is adopted as a big dataset to test our model. We use the standard split of de- velopment set and test set. We use newstest2014 as the test set and use newstest2012 +newstest2013 as the development set. Following Gehring et al. (2017), we also adopt a joint source and target BPE factorization with the vocabulary size of 40K. For medium dataset, we borrow the setup of Vaswani et al. (2017) and adopt the WMT 2014 English-German translation dataset which consists of 4.5M sentence pairs, the BPE vocabulary size is set to 32K. The test and validation datasets we used are the same as Vaswani et al. (2017). IWSLT De-En and En-Vi datasets Besides, we perform experiments on two small IWSLT datasets to test the small version of MUSE with other comparable models. The IWSLT 2014 German-English",
                "metadata": {
                    "start_index": 1798,
                    "type": "Text",
                    "filename": "1911.09483.pdf",
                    "page_number": 4
                }
            },
            {
                "content": "translation dataset consists of 160k sentence pairs. We also adopt a joint source and target BPE factorization with the vocabulary size of 32K. The IWSLT 2015 English-Vietnamese translation dataset consists of 133K training sentence pairs. For the En-Vi task, we build a dictionary including all source and target tokens. The vocabulary size for English is 17.2K, and the vocabulary size for the Vietnamese is 6.8K. For fair comparisons, we only compare models reported with the comparable model size and the same training data. We do not compare Wu et al. (2019b) because it is an ensemble method. We build MUSE-base and MUSE-large with the parameter size comparable to Transformer-base and Transformer-large. We adopt multi-head attention (Vaswani et al., 2017) as implementation of self- attention in MUSE module. The number of attention head is set to 4 for MUSE-base and 16 for MUSE-large. We also add the network architecture built by MUSE-simple in the similar way into the comparison. MUSE consists of 12 residual blocks for encoder and 12 residual blocks for decoder, the dimen- sion is set to 384 for MUSE-base and 768 for MUSE-large. The hidden dimension of non linear transformation is set to 768 for MUSE-base and 3072 for MUSE-large. The MUSE-large is trained on 4 Titan RTX GPUs while the MUSE-base is trained on a single NVIDIA RTX 2080Ti GPU. The batch size is calculated at the token level, which is called dynamic batching (Vaswani et al., 2017). We adopt dynamic convolution as the variant of depth-wise separa- ble convolution. We tune the kernel size on the validation set. For convolution with a single kernel, we use the kernel size of 7 for all layers. In case of dynamic selected kernels, the kernel size is 3 for small kernels and 15 for large kernels for all layers. The training hyper-parameters are tuned on the validation set. MUSE-large For training MUSE-large, following Ott et al. (2018), parameters are updated every 32 steps. We train the model for 80K updates",
                "metadata": {
                    "page_number": 5,
                    "start_index": 0,
                    "filename": "1911.09483.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "language processing tasks. Recent studies explore convolutional neural networks (CNN) (Gehring et al., 2017) or self-attention (Vaswani et al., 2017) to support high-parallel sequence modeling and does not require auto-regressive structure during encoding, thus bringing large ef\ufb01ciency improve- ments. They are strong at capturing local or global dependencies. There are several studies on combining self-attention and convolution. However, they do not surpass both convectional and self-attention mechanisms. Sukhbaatar et al. (2019b) propose to augment convolution with self attention by directly concentrating them in computer vision tasks. However, as demonstrated in Table 3 there method does not work for sequence to sequence learning task. Since state-of-the-art models on question answering tasks still consist on self-attention and do no adopt ideas in QAnet (Yu et al., 2018). Both self-attention (Ott et al., 2018) and convolution (Wu et al., 2019a) outperforms Evolved transformer by near 2 BLEU scores on En-Fr translation. It seems that learning global and local context through stacking self-attention and convolution layers does not beat either self-attention or convolution models. In contrast, the proposed parallel multi- scale attention outperforms previous convolution or self-attention based models on main translation tasks, showing its effectiveness for sequence to sequence learning. 5 CONCLUSION AND FUTURE WORK Although the self-attention mechanism has been prevalent in sequence modeling, we \ufb01nd that at- tention suffers from dispersed weights especially for long sequences, resulting from the insuf\ufb01cient local information. To address this problem, we present Parallel Multi-scale Attention (MUSE) and MUSE-simple. MUSE-simple introduces the idea of parallel multi-scale attention into sequence to sequence learn- ing. And MUSE fuses self-attention, convolution, and point-wise transformation together to explic- itly learn global, local and token level sequence",
                "metadata": {
                    "page_number": 9,
                    "start_index": 0,
                    "filename": "1911.09483.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: MUSE-large outperforms all previous models under the standard training and evaluation setting on WMT14 En-De and WMT14 En-Fr datasets.\nModel En-De En-Fr ConvSeq2seq (Gehring et al., 2017) 25.2 40.5 SliceNet (Kaiser et al., 2017) 26.1 - Transformer (Vaswani et al., 2017) 28.4 41.0 Weighted Transformer (Ahmed et al., 2017) 28.9 41.4 Layer-wise Coordination (He et al., 2018) 29.1 - Transformer (relative position) (Shaw et al., 2018) 29.2 41.5 Transformer (Ott et al., 2018) 29.3 43.2 Evolved Transformer (So et al., 2019) 29.8 41.3 DynamicConv (Wu et al., 2019a) 29.7 43.2 Local Joint Self-attention (Fonollosa et al., 2019) 29.7 43.3 MUSE-simple 29.8 43.2 MUSE 29.9 43.5",
                "metadata": {
                    "filename": "1911.09483.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: MUSE-base outperforms previous state-of-the-art models on IWSLT De-En translation datasets and outperforms previous models without BPE processing on IWSLT En-Vi.\nModel En-Vi De-En NBMT (Huang et al., 2017) 28.1 30.1 SACT (Lin et al., 2018) 29.1 - NP2MT (Feng et al., 2018) 30.6 31.7 Fixup (Zhang et al., 2019) - 34.5 DynamicConv (Wu et al., 2019a) - 35.2 Macaron (Lu et al., 2019) - 35.4 Local Joint Self-attention (Fonollosa et al., 2019) - 35.7 MUSE-simple 30.7 35.8 MUSE 31.3 36.3",
                "metadata": {
                    "filename": "1911.09483.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Comparisons between MUSE and its variants on the IWSLT 2015 De-En translation task.\nModel BLEU MUSE 36.3 MUSE-simple (without DepthConv) 35.8 substitute DepthConv with Convolution (k=3) 35.2 substitute DepthConv with Convolution (k=5) 35.0 substitute DepthConv with Convolution (k=7) 34.5 DepthConv without shared projection 34.9 DepthConv single kernel (k=3) 36.2 DepthConv single kernel (k=7) 36.2 DepthConv single kernel (k=15) 36.0 DepthConv single kernel (k=31) 35.8 DepthConv single kernel (grow kernels among layers:3,7,15,31) 35.9 DepthConv dynamically selected kernel (k=3,15) 36.3",
                "metadata": {
                    "filename": "1911.09483.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: The comparison between the inference speed of MUSE and Transformer.\nModel Inference Speed (tokens/s) Transformer 132 MUSE 173 Acceleration 31%",
                "metadata": {
                    "filename": "1911.09483.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5: Case study on the De-En dataset. The red bolded words denote the wrong translation and blue bolded words denote the correct translation. In case 1, transformer fails to capture the relationship between some words and their neighbors, such as \u201cright\u201d and \u201cclap\u201d. In case 2, the cause adverbial clause is correctly translated by MUSE while transformer misses the word \u201cwhy\u201d and fails to translate it.\nSource wenn sie denken, dass die auf der linken seite jazz ist und die, auf der rechten seite swing ist, dann klatschen sie bitte. Target if you think the one on the left is jazz and the one on the right is swing, clap your hands. Transformer if you think it\u2019s jazz on the left, and those on the right side of the swing are clapping, please. MUSE if you think the one on the left is jazz, and the one on the right is swing, please clap. Case 2 Source und deswegen haben wir uns entschlossen in berlin eine halle zu bauen, in der wir sozusagen die elektrischen verhltnisse der insel im mastab eins zu drei ganz genau abbilden knnen. Target and that\u2019s why we decided to build a hall in berlin, where we could precisely reconstruct, so to speak, the electrical ratio of the island on a one to three scale. Transformer and so in berlin, we decided to build a hall where we could sort of map the electrical proportions of the island at scale one to three very precisely. MUSE and that\u2019s why we decided to build a hall in berlin, where we can sort of map the electric relationship of the island at the scale one to three very precisely.",
                "metadata": {
                    "filename": "1911.09483.pdf",
                    "page_number": 12,
                    "type": "Table"
                }
            }
        ]
    },
    "1703.06345.pdf": {
        "normalized_output": [
            {
                "Task": "Part-of-Speech (POS) Tagging",
                "Dataset": "Penn Treebank (PTB)",
                "Metric": "Accuracy",
                "Result": "95.41"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2003 - English",
                "Metric": "F1",
                "Result": "91.26"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2002- Dutch",
                "Metric": "F1",
                "Result": "85.77"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2002 - Spanish",
                "Metric": "F1</s>",
                "Result": "85.19"
            },
            {
                "Task": "Text Chunking",
                "Dataset": "CoNLL-2000</s>",
                "Metric": "F1</s>",
                "Result": "97.55"
            }
        ],
        "source_documents": [
            {
                "content": "t, with the training instances being Xs and Xt. Let Ws and Wt denote the set of model parameters for the source and target tasks respectively. The model parameters are divided into two sets, task speci\ufb01c parameters and shared parameters, i.e.,",
                "metadata": {
                    "page_number": 4,
                    "filename": "1703.06345.pdf",
                    "type": "Text",
                    "start_index": 3600
                }
            },
            {
                "content": "Published as a conference paper at ICLR 2017 (b) Transfer from CoNLL 2003 NER to (c) Transfer from Spanish NER to Genia. Genia. (d) Transfer from PTB to Twitter POS tag- ging. tag-_(e) (f) Transfer from CoNLL 2003 NER to PTB POS tagging. (g) Transfer from PTB POS tagging to CoNLL 2000 chunking. (h) Transfer from PTB POS tagging to CoNLL 2003 NER. (i) Transfer from CoNLL 2003 English (j) Transfer from Spanish NER to CONLL NER to Spanish NER. 2003 English NER. The statistics of the datasets are described in Table 1. We construct the POS tagging dataset with the instructions described in Toutanova et al. (2003). Note that as a standard practice, the POS tags are extracted from the parsed trees. For the CoNLL 2003 English NER dataset, we follow previous works (Collobert et al., 2011) to append one-hot gazetteer features to the input of the CRF layer for fair comparison. Since there is no standard training/dev/test data split for the Genia and Twitter corpora, we randomly sample 10% for test, 10% for development, and 80% for training. We follow previous work (Barrett & Weber-Jahnke, 2014) to map Genia POS tags to PTB POS tags.",
                "metadata": {
                    "filename": "1703.06345.pdf",
                    "page_number": 6,
                    "type": "Text",
                    "start_index": 0
                }
            },
            {
                "content": "examine the effects of different transfer learning architectures. Now we compare our approach with state-of-the-art systems on these datasets. We use publicly available pretrained word embeddings as initialization. On the English datasets, fol- lowing previous works that are based on neural networks (Collobert et al., 2011; Huang et al., 2015; Chiu & Nichols, 2015; Ma & Hovy, 2016), we experiment with both the 50-dimensional SENNA embeddings (Collobert et al., 2011) and the 100-dimensional GloVe embeddings (Pennington et al., 2014) and use the development set to choose the embeddings for different tasks and settings. For Spanish and Dutch, we use the 64-dimensional Polyglot embeddings (Al-Rfou et al., 2013). We set the hidden state dimensions to be 300 for the word-level GRU. The initial learning rate for AdaGrad is \ufb01xed at 0.01. We use the development set to tune the other hyperparameters of our model. Our results are reported in Table 3. Since there are no standard data splits on the Genia and Twit- ter corpora, we do not include these datasets into our comparison. The results for CoNLL 2000 chunking, CoNLL 2003 NER, and PTB POS tagging are obtained by transfer learning between the three tasks, i.e., transferring from two tasks to the other. The results for Spanish and Dutch NER are obtained with transfer learning between the NER datasets in three languages (English, Spanish, and Dutch). From Table 3, we can draw two conclusions. First, our transfer learning approach achieves new state-of-the-art results on all the considered benchmark datasets except PTB POS tag- ging, which indicates that transfer learning can still improve the performance even on datasets with",
                "metadata": {
                    "type": "Text",
                    "page_number": 8,
                    "start_index": 1799,
                    "filename": "1703.06345.pdf"
                }
            },
            {
                "content": "Table 2: Improvements with transfer learning under multiple low-resource settings (%). \u201cDom\u201d, \u201capp\u201d, and \u201cling\u201d denote cross-domain, cross-application, and cross-lingual transfer settings respectively. The numbers following the slashes are labeling rates (chosen such that the number of labeled examples are of the same scale). We evaluate our transfer learning approach on the above datasets. We \ufb01x the hyperparameters for all the results reported in this section: we set the character embedding dimension at 25, the word embedding dimension at 50 for English and 64 for Spanish, the dimension of hidden states of the character-level GRUs at 80, the dimension of hidden states of the word-level GRUs at 300, and the initial learning rate at 0.01. Except for the Twitter datasets, these datasets are fairly large. To simulate a low-resource setting, we also use random subsets of the data. We vary the labeling rate of the target task at 0.001, 0.01, 0.1 and 1.0. Given a labeling rate r, we randomly sample a ratio r of the sentences from the training set and discard the rest of the training data\u2014e.g., a labeling rate of 0.001 results in around 900 training tokens on PTB POS tagging (Cf. Table 1). The results on transfer learning are plotted in Figure 2, where we compare the results with and without transfer learning under various labeling rates. The numbers in the y-axes are accuracies for POS tagging, and chunk-level F1 scores for chunking and NER. The numbers are shown in Table 2. We can see that our transfer learning approach consistently improved over the non-transfer results. We also observe that the improvement by transfer learning is more substantial when the labeling rate is lower. For cross-domain transfer, we obtained substantial improvement on the Genia and Twitter corpora by transferring the knowledge from PTB POS tagging and CoNLL 2003 NER. For example, as shown in Figure 2(a), we can obtain an tagging accuracy of 83%+ with zero labels and 92% with only 0.001 labels",
                "metadata": {
                    "filename": "1703.06345.pdf",
                    "start_index": 0,
                    "page_number": 7,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Dataset statistics.\nBenchmark Task Language # Training Tokens # Dev Tokens # Test Tokens PTB 2003 POS Tagging English 912,344 131,768 129,654 CoNLL 2000 Chunking English 211,727 - 47,377 CoNLL 2003 NER English 204,567 51,578 46,666 CoNLL 2002 NER Dutch 202,931 37,761 68,994 CoNLL 2002 NER Spanish 207,484 51,645 52,098 Genia POS Tagging English 400,658 50,525 49,761 Twitter POS Tagging English 12,196 1,362 1,627 Twitter NER English 36,936 4,612 4,921",
                "metadata": {
                    "filename": "1703.06345.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Improvements with transfer learning under multiple low-resource settings (%). \u201cDom\u201d, \u201capp\u201d, and \u201cling\u201d denote cross-domain, cross-application, and cross-lingual transfer settings respectively. The numbers following the slashes are labeling rates (chosen such that the number of labeled examples are of the same scale).\nSource Target Model Setting Transfer No Transfer Delta PTB Twitter/0.1 T-A dom 83.65 74.80 8.85 CoNLL03 Twitter/0.1 T-A dom 43.24 34.65 8.59 PTB CoNLL03/0.01 T-B app 74.92 68.64 6.28 PTB CoNLL00/0.01 T-B app 86.73 83.49 3.24 CoNLL03 PTB/0.001 T-B app 87.47 84.16 3.31 Spanish CoNLL03/0.01 T-C ling 72.61 68.64 3.97 CoNLL03 Spanish/0.01 T-C ling 60.43 59.84 0.59 PTB Genia/0.001 T-A dom 92.62 83.26 9.36 CoNLL03 Genia/0.001 T-B dom&app 87.47 83.26 4.21 Spanish Genia/0.001 T-C dom&app&ling 84.39 83.26 1.13 PTB Genia/0.001 T-B dom 89.77 83.26 6.51 PTB Genia/0.001 T-C dom 84.65 83.26 1.39",
                "metadata": {
                    "filename": "1703.06345.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Collobert et al. (2011) 94.32 89.59 \u2013 \u2013 97.29 Passos et al. (2014) \u2013 90.90 \u2013 \u2013 \u2013 Luo et al. (2015) \u2013 91.2 \u2013 \u2013 \u2013 Huang et al. (2015) 94.46 90.10 \u2013 \u2013 97.55 Gillick et al. (2015) \u2013 86.50 82.95 82.84 \u2013 Ling et al. (2015) \u2013 \u2013 \u2013 \u2013 97.78 Lample et al. (2016) \u2013 90.94 85.75 81.74 \u2013 Ma & Hovy (2016) \u2013 91.21 \u2013 \u2013 97.55 Ours w/o transfer 94.66 91.20 84.69 85.00 97.55 Ours w/ transfer 95.41 91.26 85.77 85.19 97.55",
                "metadata": {
                    "filename": "1703.06345.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            }
        ]
    },
    "1905.12598.pdf": {
        "normalized_output": [
            {
                "Task": "Word Sense Induction",
                "Dataset": "SemEval 2013 Task 13",
                "Metric": "Fuzzy normalized mutual information (FNMI)",
                "Result": "21.4"
            },
            {
                "Task": "Word Sense Induction",
                "Dataset": "SemEval 2013 Task 13",
                "Metric": "Fuzzy B-Cubed (FBC)",
                "Result": "64.0"
            },
            {
                "Task": "Word Sense Induction",
                "Dataset": "SemEval 2013 Task 13",
                "Metric": "AVG",
                "Result": "37.0"
            },
            {
                "Task": "Word Sense Induction",
                "Dataset": "SemEval 2010 Task 14",
                "Metric": "F-Score (F-S)</s>",
                "Result": "71.3"
            },
            {
                "Task": "Word Sense Induction",
                "Dataset": "SemEval 2010 Task 14",
                "Metric": "V-Measure (V-M)",
                "Result": "40.4"
            },
            {
                "Task": "Word Sense Induction",
                "Dataset": "SemEval 2010 Task 14",
                "Metric": "AVG",
                "Result": "53.6"
            }
        ],
        "source_documents": [
            {
                "content": "a target that our method scores high on (Table 3). We follow with wait(VERB), a target which our methods scores low on (Table 4). Finally, Table 5 demonstrates the error categories not present in the previous ones.",
                "metadata": {
                    "type": "Text",
                    "filename": "1905.12598.pdf",
                    "page_number": 6,
                    "start_index": 1795
                }
            },
            {
                "content": "where the \ufb01rst one prefers many small clusters, while the second one prefer fewer and larger clusters. Nei- ther of FNMI, FBC and AVG are suf\ufb01cient for in- dicating a good number of clusters.7 The metrics also do not penalize over- speci\ufb01cation of small senses: while FBC and F- S should discourage over-speci\ufb01cation, their mea- sures are proportional in instance pairs and would not punish small mass perturbations, even if those 7The story for the SemEval 2010 WSI task metrics is sim- ilar, with V-Measure favoring smaller clusters and F-S favor- ing larger ones.",
                "metadata": {
                    "page_number": 3,
                    "filename": "1905.12598.pdf",
                    "start_index": 3598,
                    "type": "Text"
                }
            },
            {
                "content": "A Handling of identi\ufb01ed failure modes Some of the failure modes mentioned in section 4 can be remedied by various means. LM and TEMPLATE cases are relatively rare and stand out when debugging the \ufb01nal solution. Their distinct distribution usually pushes them into clusters of their own, allowing identi\ufb01cation and possibly their removal before rerunning the procedure. Using a suitable LM for the target domain is im- portant, and indeed most LM failure we encoun- tered are due to transcribed spoken text. Fine- tuning BERT on the domain of interest could im- prove results. The MERGE and TOPIC cases deal with the discerning resolution of our method. An interest- ing direction for future work is \ufb01nding a way to collect additional target usages to better model the borderline cases. This also seems like a promising direction to take with SPLIT cases. The OTHER classi\ufb01ed senses are cases where our method completely fails. These include targets such as become(VERB) which are indeed hard to sense-induce without some mental process, specif- ically with substitutions alone. For example, the WordNet senses differentiate between \u201cbecome: a change in state\u201d and \u201cbecome: transform into something else\u201d, similarly to Spanish\u2019s ser/estar distinction. B Quality analysis examples The following provides examples of the different error categories we use, as well as demonstrates the senses that are induced by the method for some cases, and their descriptions according to the PMI method. Each table shows an induced sense, its high- ranking PMI words, and sentences associated to this sense. We additionally provide our assess- ment of that sense (OK, SPLIT, MERGE, TOPIC, TEMPLATE, LM, OTHER), as well as the gold- label WordNet sense for each sentence. We begin with the senses for meet(VERB), a target that our method scores high on (Table 3). We follow with wait(VERB), a target which our methods scores low on (Table 4). Finally, Table 5 demonstrates the error categories not present in the",
                "metadata": {
                    "start_index": 0,
                    "page_number": 6,
                    "filename": "1905.12598.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "The substitution-based method lends itself to in- trospection by considering the substitutes. We highlight the most prominent and informative word substitutes for each sense by computing the pointwise mutual information (PMI) between sub- 8For example, in the context of query-based search, a user may be satis\ufb01ed with the coarse grained distinction of \u201ddark(blackness) times\u201d and \u201ddark(sad) times\u201d. 9We take the most probable NOAD sense to each Word- Net one, according to the parallel corpus. This reduces the 399 senses SemEval to 205 (89 of which were not found in SemCor and left intact). Overall, 87% of the tokens were mapped to their coarse grained NOAD senses. stitute words and sense clusters. We then an- notate each sense with its top 10 most associ- ated substitutes (its signature). These sense sig- natures can be said to present the essence of what is captured by each sense cluster. As an example, one induced sense for the target meet(VERB)10 is characterized by the words \u201cconvene\u201d, \u201cgroup\u201d, \u201ccrowd\u201d, indicating the sense of a meeting that in- volves many participants. Interestingly, the Word- Net meet(VERB) entry does not make such a dis- tinction between meeting types by the number of their participants, highlighting a case were the un- supervised algorithm re\ufb01ned the human curated lexicon. Inspecting clusters and their signatures allows us to identify good and bad clusters, and identify failure modes in its process, as we do in the next section. 4 Detailed Error Analysis Armed with the cluster signatures, we turn to man- ually inspect all the produced sense clusters and their associated words. We identify the following characteristic failure modes: LM: errors of the underlying BERT LM;11 SPLIT: an additional cluster for an existing sense, for example the sense encouraging, close, per- sonal, ... for warm(ADJ) when the sense compas- sionate, favorable, kind, ... already exists; TEMPLATE: substitutes rely excessively on a template-like pattern; TOPIC:",
                "metadata": {
                    "type": "Text",
                    "filename": "1905.12598.pdf",
                    "page_number": 4,
                    "start_index": 1809
                }
            },
            {
                "content": "Table 1: Evaluation Results on the SemEval 2013 Task 13 Dataset. We report our mean (STD) scores over 10 runs. ND: no dynamic patterns. ST(SW): Sense-Topic with embedding similarity weighting.\nModel FNMI FBC AVG Ours 21.4 (0.5) 64.0 (0.5) 37.0 (0.5) Ours:ND 19.3 (0.7) 63.6 (0.2) 35.1 (0.6) LSDP 11.3 57.5 25.4 AutoSense 7.96 61.7 22.16 MCC-S 7.62 55.6 20.58 ST(SW) 7.14 55.4 19.89 AI-KU 6.5 39.0 15.92",
                "metadata": {
                    "filename": "1905.12598.pdf",
                    "page_number": 3,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Evaluation Results on the SemEval 2010 Task 14 Dataset. We report our mean (STD) scores over 10 runs. ND: no dynamic patterns.\nModel F-S V-M AVG Ours 71.3 (0.1) 40.4 (1.8) 53.6 (1.2) Ours:ND 70.9 (0.4) 37.8 (1.5) 51.7 (1.2) AutoSense 61.7 9.8 24.59 SE-WSI-\ufb01x 55.1 9.8 23.24 BNP-HC 23.1 21.4 22.23 LDA 60.7 4.4 16.34",
                "metadata": {
                    "filename": "1905.12598.pdf",
                    "page_number": 3,
                    "type": "Table"
                }
            },
            {
                "content": "No. Class High PMI words 1 OK hug, pick, thank, ., ..., surprise, impressed, marrie, hire, welcome \u2022 You\u2019re going to meet John Speckman! (5) \u2022 So I guess one question might be how I met my wife. (1) \u2022 We are taking you to meet him the day after you arrive. (1) 2 OK qualify, offer, below, violate, comply, accomplish, supply,complete,\ufb01ll,accommodate \u2022 And we need Your help to meet the challenge! (3) \u2022 So, I want to thank you on meeting my \ufb01rst condition. (3) \u2022 They could not meet conditions if their competitors were free to ignore them. (3) 3 SPLIT conversation, summit, friendly, discussion, business, spend, touching, partner, dining \u2022 it\u2019s gonna make the people they\u2019re meeting with feel very uncomfortable ... (5) \u2022 Best wishes until we meet again-perhaps over Volume 9 ... (5) \u2022 He and Atta agreed to meet later at a location to be determined. (7) 4 OK group, convention, weekly, schedule, parliament, convene, celebrate,crowd,originate \u2022 A group called the League of Prizren, named for the Kosovo town where it met, ... (7) \u2022 cat and bagpipean society a society which met at their of\ufb01ce ... (7) \u2022 A summer Antiekmarkt or antique market meets at Nieuwmarkt on Sundays ...(7) 5 OK direct, encounter, dare, oppose, reaction, repulse, cause, underwent, face, react \u2022 They were greeted as liberators by the peasants and met only desultory resistance ... (4,9) \u2022 ... astounded by the funny logic of, say, meeting one\u2019s match ... (9) \u2022 It\u2019s too bad that ... this understanding has to meet with such hostility, don\u2019t you think? (4) 6 OK maximum, phase, interval, curve, origin, converge, respectively, cancel, border, dip \u2022 we can draw a line of those tangencies ..., that meet at the initial apple-pear distributions... (6) 7 TOPIC investigate, cost, fund, ease, budget, recover, shoulder, offset, slash, decrease \u2022 ... appealed to the state government to help meet the cost of burying armed robbers...(3)",
                "metadata": {
                    "filename": "1905.12598.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: Senses induced for the target wait(VERB) on which our method perform poorly. In our manual inspec- tion, sense #6 is classi\ufb01ed as OTHER, this class of found senses usually group up a large portion of unrelated sentence, making their differentiating substitutes an incoherent bag of left-overs. Sense #2\u2019s substitutes aren\u2019t very informative for its sense (\u201dserve as a waiter\u201d) but they do distinguish this sentence from the other sensed sentences.\n1 OK lodge, cool, bray, rock, glide, bend, hush, groom, camp, creep \u2022 The horses wait under the cooling shade for their next customers. (1) 2 \u223cOK bench, staff, guest, bounce, pat, pit, ticket, fare, to, other \u2022 ... offer the best sightlines, roomier seats, and wait staff who peddle gourmet fare. (3) 3 OTHER reasonable, slack, qualify, delivery, short, a, temporary, week, hesitation, due \u2022 ... without the need to wait until everyone is in town for a meeting. (4) \u2022 ... operator will be paid at some average earnings rate during the waiting period. (4) \u2022 ... and uh i would agree a a short waiting period would be appropriate to uh ... (4) 4 TOPIC literally, cooking, everything, pregnant, family, lot, town, money, forever, food \u2022 He had a farm waiting for him right? (2) \u2022 If Clinton, ... , was a time bomb waiting to explode, then ... (1) \u2022 ... as you wouldn\u2019t if you had a wife who looked like that waiting for you. (1) 5 LM ago, they, fade, along, since, drank, though, afterwards, sometimes, uh \u2022 ... i sang in a couple of uh community choirs and then um waited for a while ...(4) 6 OTHER write, reach, argue, appear, bother, act, seem, star, [, wish \u2022 I\u2019m not sure why you waited a week.(4) \u2022 Good things come to those who wait.(4) \u2022 He had been waiting for Oedipa in the bathroom. (1) WordNet senses for wait(VERB) in gold labels: 1. stay in one place and anticipate or expect something 2. look forward to the probable occurrence of 3. serve as a waiter or waitress in a restaurant 4. wait before acting",
                "metadata": {
                    "filename": "1905.12598.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Target Class High PMI words MERGE rally, roar, mobilize, go, picket, dominate, rise, uprising, rebel, riot",
                "metadata": {
                    "filename": "1905.12598.pdf",
                    "page_number": 9,
                    "type": "Table"
                }
            }
        ]
    },
    "1910.13461.pdf": {
        "normalized_output": [
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201916 RO-EN",
                "Metric": "BLEU",
                "Result": "37.96"
            },
            {
                "Task": "Question Answering",
                "Dataset": "SQuAD 1.1",
                "Metric": "F1",
                "Result": "94.6"
            },
            {
                "Task": "Question Answering",
                "Dataset": "SQuAD 2.0",
                "Metric": "F1",
                "Result": "89.2"
            },
            {
                "Task": "Sentiment Analysis",
                "Dataset": "MNLI-m",
                "Metric": "Accuracy",
                "Result": "89.9/90.1"
            },
            {
                "Task": "Sentiment Analysis",
                "Dataset": "SST-2</s>",
                "Metric": "Accuracy",
                "Result": "96.6"
            },
            {
                "Task": "Sentiment Analysis",
                "Dataset": "QQP",
                "Metric": "Accuracy",
                "Result": "92.5"
            },
            {
                "Task": "Sentiment Analysis",
                "Dataset": "QNLI",
                "Metric": "Accuracy",
                "Result": "94.9"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-1</s>",
                "Result": "44.16"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-2",
                "Result": "21.28"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-L",
                "Result": "40.90"
            },
            {
                "Task": "Summarization",
                "Dataset": "XSum",
                "Metric": "ROGUE-1",
                "Result": "45.14"
            },
            {
                "Task": "Summarization",
                "Dataset": "XSum",
                "Metric": "ROGUE-2",
                "Result": "22.27"
            },
            {
                "Task": "Summarization",
                "Dataset": "XSum",
                "Metric": "ROGUE-L",
                "Result": "37.25"
            },
            {
                "Task": "Dialogue Generation",
                "Dataset": "ConvAI2",
                "Metric": "F1",
                "Result": "20.72"
            },
            {
                "Task": "Dialogue Generation",
                "Dataset": "ConvAI2",
                "Metric": "Perplexity",
                "Result": "11.85"
            },
            {
                "Task": "Question Answering",
                "Dataset": "ELI5",
                "Metric": "ROGUE-1",
                "Result": "30.6"
            },
            {
                "Task": "Question Answering",
                "Dataset": "ELI5",
                "Metric": "ROGUE-2",
                "Result": "6.2"
            },
            {
                "Task": "Question Answering",
                "Dataset": "ELI5",
                "Metric": "ROGUE-L</s>",
                "Result": "24.3"
            }
        ],
        "source_documents": [
            {
                "content": "(b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART. The new encoder can use a disjoint vocabulary. re-implement strong pre-training approaches recently proposed for discriminative and generation tasks. We aim, as much as possible, to control for differences un- related to the pre-training objective. However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective). For refer- ence, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data. We compare the following approaches: We experiment with (1) treating the task as a stan- dard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder out- put, or (2) adding the source as pre\ufb01x to the target in the decoder, with a loss only on the target part of the sequence. We \ufb01nd the former works better for BART models, and the latter for other models. To most directly compare our models on their ability to model their \ufb01ne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1. 4.2 Tasks Language Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model. This model is equivalent to the BART decoder, without cross-attention. Permuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and gener- ate them in a random order autoregressively. For con- sistency with other models, we do not implement the relative positional embeddings or attention across seg- ments from XLNet. Masked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens. SQuAD (Rajpurkar et al., 2016)a an extractive ques- tion answering task on Wikipedia paragraphs. Answers are",
                "metadata": {
                    "start_index": 0,
                    "filename": "1910.13461.pdf",
                    "type": "Text",
                    "page_number": 4
                }
            },
            {
                "content": "Table 1: Comparison of pre-training objectives. All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data. Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures. Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1). Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance. Performance of pre-training methods varies signi\ufb01- cantly across tasks The effectiveness of pre-training methods is highly dependent on the task. For exam- ple, a simple language model achieves the best ELI5 performance, but the worst SQUAD results. Token masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation. The successful methods either use token deletion or masking, or self-attention masks. Deletion appears to outperform masking on generation tasks. Pure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplex- ities than other tasks, and is the only generation task where other models outperform BART. A pure lan- guage model performs best, suggesting that BART is less effective when the output is only loosely con- strained by the input. BART achieves the most consistently strong perfor- mance. With the exception of ELI5, BART models using text-in\ufb01lling perform well on all tasks. 5 Large-scale Pre-training Experiments Left-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training. Recent work has shown that downstream performance can dramatically improve when pre-training is",
                "metadata": {
                    "filename": "1910.13461.pdf",
                    "start_index": 0,
                    "type": "Text",
                    "page_number": 5
                }
            },
            {
                "content": "with [MASK] symbols, and train the model to independently predict the original tokens. SQuAD (Rajpurkar et al., 2016)a an extractive ques- tion answering task on Wikipedia paragraphs. Answers are text spans extracted from a given document context. Similar to BERT (Devlin et al., 2019), we use concate- nated question and context as input to the encoder of BART, and additionally pass them to the decoder. The model includes classi\ufb01ers to predict the start and end indices of each token. MNLI (Williams et al., 2017), a bitext classi\ufb01cation task to predict whether one sentence entails another. The \ufb01ne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder. In contrast to BERT, the representation of the EOS token is used to classify the sentences relations. Multitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks. Self at- tention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 un- masked, and 1/3 with the \ufb01rst 50% of tokens unmasked and a left-to-right mask for the remainder. ELI5 (Fan et al., 2019), a long-form abstractive ques- tion answering dataset. Models generate answers con- ditioned on the concatenation of a question and sup- porting documents. XSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries. Masked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens. ConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona. For the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to ef\ufb01ciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right). CNN/DM",
                "metadata": {
                    "start_index": 1804,
                    "type": "Text",
                    "filename": "1910.13461.pdf",
                    "page_number": 4
                }
            },
            {
                "content": "Extractive models do well here, and even the baseline of the \ufb01rst-three source sentences is highly competitive. Nevertheless, BART outperforms all existing work. 5.3 Generation Tasks We also experiment with several text generation tasks. BART is \ufb01ne-tuned as a standard sequence-to-sequence model from the input to the output text. During \ufb01ne- tuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1. During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017). In contrast, XSum is highly abstractive, and extrac- tive models perform poorly. BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a sig- ni\ufb01cant advance in performance on this problem. Qual- itatively, sample quality is high (see \u00a76). Dialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the pre- vious context and a textually-speci\ufb01ed persona. BART outperforms previous work on two automated metrics.",
                "metadata": {
                    "filename": "1910.13461.pdf",
                    "page_number": 6,
                    "start_index": 1810,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Comparison of pre-training objectives. All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data. Entries in the bottom two blocks are trained on identical data using the same code-base, and \ufb01ne-tuned with the same procedures. Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see \u00a74.1). Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most consistently strong performance.\nModel SQuAD 1.1 MNLI ELI5 XSum ConvAI2 CNN/DM F1 Acc PPL PPL PPL PPL BERT Base (Devlin et al., 2019) 88.5 84.3 - - - - Masked Language Model 90.0 83.5 24.77 7.87 12.59 7.06 Masked Seq2seq 87.0 82.1 23.40 6.80 11.43 6.19 Language Model 76.7 80.1 21.40 7.00 11.51 6.56 Permuted Language Model 89.1 83.7 24.03 7.69 12.23 6.96 Multitask Masked Language Model 89.2 82.4 23.73 7.50 12.39 6.74 BART Base w/ Token Masking 90.4 84.1 25.05 7.08 11.73 6.10 w/ Token Deletion 90.4 84.1 24.61 6.90 11.46 5.87 w/ Text In\ufb01lling 90.8 84.0 24.26 6.61 11.05 5.83 w/ Document Rotation 77.2 75.3 53.69 17.14 19.87 10.59 w/ Sentence Shuf\ufb02ing 85.4 81.5 41.87 10.93 16.67 7.89 w/ Text In\ufb01lling + Sentence Shuf\ufb02ing 90.8 83.8 24.17 6.62 11.12 5.41",
                "metadata": {
                    "filename": "1910.13461.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "SQuAD 1.1 SQuAD 2.0 MNLI SST QQP QNLI EM/F1 EM/F1 m/mm Acc Acc Acc Acc Acc Acc Mcc BERT 84.1/90.9 79.0/81.8 86.6/- 93.2 91.3 92.3 90.0 70.4 88.0 60.6 UniLM -/- 80.5/83.4 87.0/85.9 94.5 - 92.7 - 70.9 - 61.1 XLNet 89.0/94.5 86.1/88.8 89.8/- 95.6 91.8 93.9 91.8 83.8 89.2 63.6 RoBERTa 88.9/94.6 86.5/89.4 90.2/90.2 96.4 92.2 94.7 92.4 86.6 90.9 68.0 BART 88.8/94.6 86.1/89.2 89.9/90.1 96.6 92.5 94.9 91.2 87.0 90.4 62.8",
                "metadata": {
                    "filename": "1910.13461.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Results for large models on SQuAD and GLUE tasks. BART performs comparably to RoBERTa and XLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\nCNN/DailyMail XSum R1 R2 RL R1 R2 RL Lead-3 40.42 17.62 36.67 16.30 1.60 11.95 PTGEN (See et al., 2017) 36.44 15.66 33.42 29.70 9.21 23.24 PTGEN+COV (See et al., 2017) 39.53 17.28 36.38 28.10 8.02 21.72 UniLM 43.33 20.21 40.51 - - - BERTSUMABS (Liu & Lapata, 2019) 41.72 19.39 38.76 38.76 16.33 31.15 BERTSUMEXTABS (Liu & Lapata, 2019) 42.13 19.60 39.18 38.81 16.50 31.27 BART 44.16 21.28 40.90 45.14 22.27 37.25",
                "metadata": {
                    "filename": "1910.13461.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "ConvAI2 Valid F1 Valid PPL Seq2Seq + Attention Best System BART 16.02 19.09 20.72 35.07 17.51 11.85",
                "metadata": {
                    "filename": "1910.13461.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset. Comparison models are from Fan et al. (2019).\nELI5 R1 R2 RL Best Extractive 23.5 3.1 17.5 Language Model 27.8 4.7 23.1 Seq2Seq 28.3 5.1 22.8 Seq2Seq Multitask 28.9 5.4 23.1 BART 30.6 6.2 24.3",
                "metadata": {
                    "filename": "1910.13461.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 6: The performance (BLEU) of baseline and BART on WMT\u201916 RO-EN augmented with back- translation data. BART improves over a strong back- translation (BT) baseline by using monolingual English pre-training.\nRO-EN Baseline 36.80 Fixed BART 36.29 Tuned BART 37.96",
                "metadata": {
                    "filename": "1910.13461.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            }
        ]
    },
    "2001.04063.pdf": {
        "normalized_output": [
            {
                "Task": "Question Generation",
                "Dataset": "SQuAD 1.1",
                "Metric": "BLEU-4",
                "Result": "25.80"
            },
            {
                "Task": "Question Generation",
                "Dataset": "SQuAD 1.1",
                "Metric": "METEOR",
                "Result": "27.54"
            },
            {
                "Task": "Question Generation",
                "Dataset": "SQuAD 1.1",
                "Metric": "ROGUE-L</s>",
                "Result": "53.65"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-1",
                "Result": "44.20"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-2",
                "Result": "21.17"
            },
            {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Metric": "ROGUE-L",
                "Result": "41.30"
            },
            {
                "Task": "Summarization",
                "Dataset": "Gigaword",
                "Metric": "ROGUE-1</s>",
                "Result": "39.51"
            },
            {
                "Task": "Summarization",
                "Dataset": "Gigaword",
                "Metric": "ROGUE-2</s>",
                "Result": "20.42"
            },
            {
                "Task": "Summarization",
                "Dataset": "Gigaword",
                "Metric": "ROGUE-L</s>",
                "Result": "36.69"
            }
        ],
        "source_documents": [
            {
                "content": "3: Results on SQuAD 1.1 test set (with reference of Du et al. (2017) tokenized). B4 is short for BLEU- 4, MTR is short for METEOR, and R-L is short for ROUGE-L. The same model is used to evaluate on the two different data splits. The results, according to the references provided by Du et al. (2017) is shown in Table 3. The same model and inference hyper-parameters are used for the two different data split with swapped dev and test set. It can be seen that ProphetNet outperforms all previous methods with signi\ufb01cant improvement. 3.4 Large-scale Pre-training 3.3 Fine-tuning on Question Generation The answer-aware question generation task (Zhou et al., 2017) aims to generate a question that asks towards the given answer span based on a given text passage or document. We conduct experiments on Recent works show that the pre-trained model\u2019s performance on the downstream task can be im- proved when using larger scaled pre-training cor- pora (Lewis et al., 2019; Raffel et al., 2019). We",
                "metadata": {
                    "filename": "2001.04063.pdf",
                    "start_index": 1803,
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "Table 1: Results on the CNN/DailyMail test set. this task to further evaluate the ProphetNet model. Following Du et al. (2017), we split the SQuAD 1.1 (Rajpurkar et al., 2016) dataset into training, de- velopment and test sets. We also report the results on the data split as did in Zhao et al. (2018), which reverses the development set and test set. Table 2: Results on Gigaword test set. R is short for ROUGE. implements the standard Seq2Seq model with at- tention mechanism; Re3Sum (Cao et al., 2018) which employs an extended Seq2Seq model to generate summaries based on the retrieved can- didate summaries. And two pre-training based strong baselines: MASS (Song et al., 2019), and UniLM (Dong et al., 2019). The results are pre- sented in Table 2. It can be observed that Prophet- Net outperforms previous models on all metrics. The question generation task is typically formu- lated as a Seq2Seq problem. The input passage and the answer are packed as \u201canswer [SEP] input passage\u201d as input, and the question is used as the target output sequence. We \ufb01ne-tune the Prophet- Net model 10 epochs in the training set and report the results of the two kinds of data splits as men- tioned above. The \ufb01rst 512 tokens of the passage are fed to the model. The peak learning rate is 1 \u00d7 10\u22125 and the batch size is set to 28. We compare ProphetNet against the following models: CorefNQG (Du and Cardie, 2018) which employs a feature-rich encoder based on Seq2Seq model; MP-GSN (Zhao et al., 2018) which incor- porates a gated self-attention encoder with max- out pointer; SemQG (Zhang and Bansal, 2019) which introduces two semantics-enhanced rewards for Seq2Seq model training. Besides, we also com- pare our model with UniLM (Dong et al., 2019), which is the previous state-of-the-art on this task. Table 3: Results on SQuAD 1.1 test set (with reference of Du et al. (2017) tokenized). B4 is short for BLEU- 4, MTR is short for METEOR, and R-L is short for ROUGE-L. The same model is used to evaluate",
                "metadata": {
                    "start_index": 0,
                    "filename": "2001.04063.pdf",
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "References",
                "metadata": {
                    "filename": "2001.04063.pdf",
                    "start_index": 0,
                    "page_number": 9,
                    "type": "Text"
                }
            },
            {
                "content": "attenuation coef\ufb01cient \u03b3 is set to 0.5. The pre-trained models are then \ufb01ne-tuned on CNN/DailyMail. We report the F1 scores of ROUGE-1, ROUGE-2 and ROUGE-L. The re- sults are shown in Table 6. We can see that the performance of ProphetNetbase-3gram and ProphetNetbase-2gram is comparable. Both of them perform better than MASSbase and ProphetNetbase- 1gram. Considering the computational and time cost, we use ProphetNetbase-2gram in other experi- ments due to its training speed is 15% faster than ProphetNetbase-3gram. Table 6: n-gram comparison results on CNN/DailyMail test set 4 Related Work Unsupervised pre-training has been successfully ap- plied to various natural language processing tasks. GPT (Radford et al., 2018) takes plain text as pre- training data to predict the next tokens with left- ward tokens. It is based on the left-to-right lan- guage model and can be used to generate stories and continue to write for a given text. BERT (De- vlin et al., 2018) and SpanBERT (Joshi et al., 2019) use a Bi-directional language model to recover masked tokens/spans for a given sentence. Bi- directional information \ufb02ow can be used to recover the masked positions, but no left-to-right language model dependency is learned. As a result, BERT and SpanBERT bring signi\ufb01cant improvement for NLU tasks but are not suitable for generation tasks. XLNet (Yang et al., 2019) predicts the tokens with given positions and some tokens with their posi- tions in the sentence in an AR manner. Although it uses AR to build a permuted-ordered language model, it is also not suitable for NLG tasks because it brought too much noise for a left-to-right lan- guage model. MASS (Song et al., 2019) pre-trains the sequence-to-sequence model by dropping a con- tinuous token span to corrupt the original text and learns to recover it. T5 (Raffel et al., 2019) investi- gates different model structures and different pre- training tasks, and is pre-trained on a large scale corpus named C4 which is 750GB. BART",
                "metadata": {
                    "start_index": 0,
                    "page_number": 8,
                    "filename": "2001.04063.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results on the CNN/DailyMail test set.\nMethod ROUGE-1 ROUGE-2 ROUGE-L LEAD-3 (Nallapati et al., 2017) 40.42 17.62 36.67 PTGEN (See et al., 2017) 36.44 15.66 33.42 PTGEN+Coverage (See et al., 2017) 39.53 17.28 36.38 S2S-ELMo (Edunov et al., 2019) 41.56 18.94 38.47 Bottom-Up (Gehrmann et al., 2018) 41.22 18.68 38.34 BERTSUMABS (Liu and Lapata, 2019) 41.72 19.39 38.76 BERTSUMEXTABS (Liu and Lapata, 2019) 42.13 19.60 39.18 MASS (Song et al., 2019) 42.12 19.50 39.01 UniLM (Dong et al., 2019) 43.33 20.21 40.51 ProphetNet 43.68 20.64 40.72",
                "metadata": {
                    "filename": "2001.04063.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Method R-1 R-2 R-L OpenNMT (Klein et al., 2017) 36.73 17.86 33.68 Re3Sum (Cao et al., 2018) MASS (Song et al., 2019) UniLM (Dong et al., 2019) ProphetNet 37.04 38.73 38.45 39.55 19.03 19.71 19.45 20.27 34.46 35.96 35.75 36.57",
                "metadata": {
                    "filename": "2001.04063.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Method CorefNQG (Du and Cardie, 2018) SemQG (Zhang and Bansal, 2019) B4 15.16 18.37 MTR 19.12 22.65 R-L - 46.68 UniLM (Dong et al., 2019) 21.63 25.04 51.09 ProphetNet MP-GSN (Zhao et al., 2018) SemQG (Zhang and Bansal, 2019) 23.91 16.38 20.76 26.60 20.25 24.20 52.26 44.48 48.91 UniLM (Dong et al., 2019) 23.08 25.57 52.03 ProphetNet 25.80 27.54 53.65",
                "metadata": {
                    "filename": "2001.04063.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: Results on the CNN/DailyMail and Gigaword test sets of large-scale pre-training models. R is short for ROUGE, and Corpus denotes the size of the pre-training data.\nDataset Method Corpus R-1 R-2 R-L T5 (Raffel et al., 2019) 750GB 43.52 21.55 40.69 PEGASUSLARGE (C4) (Zhang et al., 2019) 750GB 43.90 21.20 40.76 CNN/DailyMail PEGASUSLARGE (HugeNews) (Zhang et al., 2019) 3800GB 44.17 21.47 41.11 BART (Lewis et al., 2019) 160GB 44.16 21.28 40.90 ProphetNet 160GB 44.20 21.17 41.30 PEGASUSLARGE (C4) (Zhang et al., 2019) 750GB 38.75 19.96 36.14 Gigaword PEGASUSLARGE (HugeNews) (Zhang et al., 2019) 3800GB 39.12 19.86 36.24 ProphetNet 160GB 39.51 20.42 36.69",
                "metadata": {
                    "filename": "2001.04063.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5: Results on CNN/DailyMail dev set without pre-training\nSetting R-1 R-2 R-L Transformer (Raffel et al., 2019) 39.19 17.60 36.69 ProphetNetw/o pre-train 40.66 18.05 37.79",
                "metadata": {
                    "filename": "2001.04063.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 6: n-gram comparison results on CNN/DailyMail test set\nSetting R-1 R-2 R-L MASSbase 42.12 19.50 39.01 ProphetNetbase-1gram 42.21 19.54 39.06 ProphetNetbase-2gram 42.52 19.78 39.59 ProphetNetbase-3gram 42.61 19.83 39.67",
                "metadata": {
                    "filename": "2001.04063.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            }
        ]
    },
    "N19-1373.pdf": {
        "normalized_output": [
            {
                "Task": "Dialogue Act Classification",
                "Dataset": "Switchboard Dialog Act Corpus (SWDA)",
                "Metric": "Accuracy",
                "Result": "82.9"
            },
            {
                "Task": "Dialogue Act Classification",
                "Dataset": "ICSI Meeting Recorder Dialog Act Corpus (MRDA)",
                "Metric": "Accuracy",
                "Result": "91.1"
            }
        ],
        "source_documents": [
            {
                "content": "levels can impact the classi\ufb01cation performance at the higher level. Employing self-attention, which has not previously been applied to this task, en- ables the model to learn richer, more effective ut- terance representations for the task. As future work, we would like to experiment with other attention mechanisms such as multi- head attention (Vaswani et al., 2017), directional self-attention (Shen et al., 2018a), block self- attention (Shen et al., 2018b), or hierarchical at- tention (Yang et al., 2016), since they have been shown to address the limitations of vanilla atten- tion and self-attention by either incorporating in- formation from different representation subspaces at different positions to capture both local and long-range context dependencies, encoding tem- poral order information, or by attending to context dependencies at different levels of granularity. Acknowledgements The authors would like to thank Dimitris Alikanio- tis, Maria Nadejde and Courtney Napoles for their insightful discussions, and the anonymous review- ers for their helpful comments. 3731",
                "metadata": {
                    "type": "Text",
                    "start_index": 1799,
                    "filename": "N19-1373.pdf",
                    "page_number": 5
                }
            },
            {
                "content": "References",
                "metadata": {
                    "page_number": 6,
                    "type": "Text",
                    "start_index": 0,
                    "filename": "N19-1373.pdf"
                }
            },
            {
                "content": "Table 2: Number of utterances by dataset. |C| denotes number of classes and |V| is the vocabulary size. we use the train, validation and test splits as de- \ufb01ned in Lee and Dernoncourt (2016). Table 2 shows the statistics for both datasets. They are highly imbalanced in terms of class distribution, with the DA classes Statement-non-opinion and Acknowledge/Backchannel in SwDA and Statement in MRDA making up over 50% of the labels in each set. 5 Results 5.1 Dialogue Act Classi\ufb01cation We compare the classi\ufb01cation accuracy of our model against several other recent methods (Ta- ble 3).1 Four approaches (Chen et al., 2018; Tran et al., 2017; Ortega and Vu, 2017; Shen and Lee, 2016) use attention in some form to model the con- versations, but none of them have explored self- attention for the task. The last three use CRFs in the \ufb01nal layer of sequence labeling. Only one other method (Chen et al., 2018) uses character- level word embeddings. All models and their vari- ants were trained ten times and we report the av- erage test performance. Our model outperforms state-of-the-art methods by 1.6% on SwDA, the primary dataset for this task, and comes within 0.6% on MRDA. It also beats a TF-IDF GloVe baseline (described in Section 5.2) by 16.4% and 12.2%, respectively. The improvements that the model is able to make over the other methods are signi\ufb01cant, how- ever, the gains on MRDA still fall short of the state-of-the-art by 0.6%. This can mostly be at- tributed to the conversation/context lengths and la- bel noise at the conversation level. Conversations in MRDA (1493 utterances on average) are signi\ufb01- cantly longer than in SwDA (271 utterances on av- erage). In spite of having nearly 12% the number 1Contemporaneous to this submission, (Li et al., 2018; Wan et al., 2018; Ravi and Kozareva, 2018) proposed differ- ent approaches for the task. We do not focus on them here per NAACL 2019 guidelines, however note that our system out- performs the \ufb01rst two. (Ravi and Kozareva, 2018)",
                "metadata": {
                    "start_index": 0,
                    "filename": "N19-1373.pdf",
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "RNN (size of \u2212\u2212\u2192gi\u22121), and b is a vector representing bias. Equation 5 can then be treated as a 2-layer MLP with bias, with da hidden units, Ws1,Ws2 and Ws3 as weight parameters. The scores Si are mapped into a probability matrix Ai by means of a softmax function: which is then used to obtain a 2-d representation Mi of the input utterance, using the GRU hidden states Hi according to the attention weights pro- vided by Ai as follows: This 2-d representation is then projected to a 1-d embedding (denoted as hi), using a fully- connected layer. The conversation-level GRU then operates over this 1-d utterance embedding, and hence, we can represent gi as: gi then provides the conversation-level context used to learn the attention scores and 2-d repre- sentation (Mi+1) for the next utterance in the con- versation (hi+1). 3.3 Conversation-level RNN The utterance representation hi from the previous step is passed on to the conversation-level RNN, which is another bidirectional GRU layer used to encode utterances across a conversation. The hid- den states \u2212\u2192gi and \u2190\u2212gi (Figure 1) are then concate- nated to get the \ufb01nal representation gi of each ut- terance, which is further propagated to a linear chain CRF layer. The CRF layer considers the correlations between labels in context and jointly decodes the optimal sequence of utterance labels for a given conversation, instead of decoding each label independently. 4 Data We evaluate the classi\ufb01cation accuracy of our model on the two standard datasets used for DA classi\ufb01cation: the Switchboard Dialogue Act Cor- pus (SwDA) (Jurafsky et al., 1997) consisting of 43 classes, and the 5-class version of the ICSI Meeting Recorder Dialogue Act Corpus (MRDA) introduced in (Ang et al., 2005). For both datasets,",
                "metadata": {
                    "start_index": 1817,
                    "page_number": 3,
                    "type": "Text",
                    "filename": "N19-1373.pdf"
                }
            },
            {
                "content": "Table 1: A snippet of a conversation sample from the SwDA Corpus. Each utterance has a corresponding di- alogue act label.\nSpeaker Utterance A Okay. DA label Other A B Um, what did you do this weekend? Well, uh, pretty much spent most of my time in the yard. Question Statement B [Throat Clearing] Non Verbal A A B Uh-Huh. What do you have planned for your yard? Well, we\u2019re in the process of, revitalizing it. Backchannel Question Statement",
                "metadata": {
                    "filename": "N19-1373.pdf",
                    "page_number": 1,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: Number of utterances by dataset. |C| denotes number of classes and |V| is the vocabulary size.\nDataset |C| |V| Train Validation Test MRDA 5 12k 78k 16k 15k SwDA 43 20k 193k 23k 5k",
                "metadata": {
                    "filename": "N19-1373.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: DA Classi\ufb01cation Accuracy\nModel SwDA MRDA TF-IDF GloVe 66.5 78.7 Kalchbrenner and Blunsom (2013) 73.9 - Lee and Dernoncourt (2016) 73.9 84.6 Khanpour et al. (2016) 75.8 86.8 Ji et al. (2016) 77.0 - Shen and Lee (2016) 72.6 - Li and Wu (2016) 79.4 - Ortega and Vu (2017) 73.8 84.3 Tran et al. (2017) 74.5 - Kumar et al. (2018) 79.2 90.9 Chen et al. (2018) 81.3 91.7 Our Method 82.9 91.1 Human Agreement 84.0 -",
                "metadata": {
                    "filename": "N19-1373.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: Performance of utterance representation methods when integrated with the hierarchical model\nMethod SwDA MRDA Baseline TF-IDF GloVe 66.5 78.7 Pre-trained on Corpus Skip Thought Vectors 72.6 82.8 Paragraph vectors 72.5 82.6 Joint Learning RNN-Encoder 74.8 85.7 Bi-RNN-LastState 76.2 85.4 Bi-RNN-MaxPool 77.6 86.7 CNN 76.9 84.5 Bi-RNN + Attention 80.1 87.7 + Context 81.8 89.2 Bi-RNN + Self-attention 81.1 88.6 + Context 82.9 91.1",
                "metadata": {
                    "filename": "N19-1373.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5: Hyperparameter space and tuned values\nHyperparams Range of values Final value Word GloVe 100D GloVe 300D + Embeddings GloVe 200D ELMo 1024D GloVe 300D Word2vec 300D Word2vec 200D ELMo 1024D GloVe 300D + ELMo 1024D Word2Vec 300D + ELMo 1024D Sentence GRU 20 - 300 50 Size (u) Utterance GRU 20 - 600 100 Size (k) Learning Rate 0.01 - 2.0 0.015 Dropout 0.1 - 0.8 0.3 Optimizer SGD, Adam RMSProp, Adam",
                "metadata": {
                    "filename": "N19-1373.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            }
        ]
    },
    "1903.07785.pdf": {
        "normalized_output": [
            {
                "Task": "Text Similarity",
                "Dataset": "MRPC",
                "Metric": "F1",
                "Result": "88.3"
            },
            {
                "Task": "Text Similarity",
                "Dataset": "QQP",
                "Metric": "F1",
                "Result": "87.2"
            },
            {
                "Task": "Text Similarity",
                "Dataset": "STS-B",
                "Metric": "Spearman Correlation",
                "Result": "88.8"
            },
            {
                "Task": "Natural Language Inference (NLI)",
                "Dataset": "MNLI-m</s>",
                "Metric": "Accuracy",
                "Result": "82.3"
            },
            {
                "Task": "Natural Language Inference (NLI)",
                "Dataset": "QNLI",
                "Metric": "Accuracy",
                "Result": "86.5"
            },
            {
                "Task": "Natural Language Inference (NLI)",
                "Dataset": "RTE",
                "Metric": "Accuracy",
                "Result": "68.4"
            },
            {
                "Task": "Named Entity Recognition (NER)",
                "Dataset": "CoNLL-2003 - English",
                "Metric": "F1",
                "Result": "93.5"
            }
        ],
        "source_documents": [
            {
                "content": "For tasks involving sentence-pairs, we concatenate them and add a new separator token < sep > be- tween them. We add the output of this token to the \ufb01nal projection W2 \u2208 RC\u00d73d. Structured prediction tasks. For named entity recognition and parsing we use task-speci\ufb01c archi- tectures which we \ufb01ne-tune together with the lan- guage model but with different learning rate. The architectures are detailed in the respective results sections. The input to the architectures are the output representations of the pretrained language model. No Masking. For \ufb01ne-tuning, we found it ben- e\ufb01cial to remove masking of the current token in the \ufb01nal layer that pools the output of the two tow- ers. It is important to have access to information about the token to be classi\ufb01ed for token level clas- si\ufb01cation tasks such as NER but we also found this to perform better for sentence classi\ufb01cation tasks. In practice, we completely disable masking in the combination layer so that it operates over all forward and backward states. However, dis- abling masking below the combination layer does not perform well. Optimization. During \ufb01ne-tuning we use larger learning rates for the new parameters, that is W1, W2, b or the task-speci\ufb01c architecture, compared to the pretrained model. For GLUE tasks, we do so by simply scaling the output of the language model before the W1 and W2 projections by a factor of 16. For structured prediction tasks, we explicitly use different learning rates for the pre- trained model and the task-speci\ufb01c parameters. We \ufb01ne tune with the Adam optimizer (Kingma and Ba, 2015). For GLUE tasks, we disable dropout in the language model and add 0.1 dropout between language model output and the \ufb01nal out- put projection; for structured prediction tasks, we use 0.3 at all levels (within the pretrained model, within the task-speci\ufb01c architecture, and on the weights connecting them). In all settings, we use a batch size of 16 examples. We use a cosine sched- ule to linearly warm up the",
                "metadata": {
                    "start_index": 0,
                    "page_number": 4,
                    "type": "Text",
                    "filename": "1903.07785.pdf"
                }
            },
            {
                "content": "pretrained model, within the task-speci\ufb01c architecture, and on the weights connecting them). In all settings, we use a batch size of 16 examples. We use a cosine sched- ule to linearly warm up the learning rate from 1e- 07 to the target value over the \ufb01rst 10% of train- ing steps, and then anneal the learning rate to 1e- 06, following the cosine curve for the remaining steps. For GLUE tasks, we tuned the learning rate for each task and chose the best value over three settings: 1e-04, 5e-05 and 3e-05. For structured prediction tasks, we tuned on the pairs of learning rate, see the results section for details. For GLUE tasks, we train three seeds for each learning rate value for three epochs and choose the model af- ter each epoch that performs best on the validation set. For structured prediction tasks, we train for up to 25 epochs and stop if the validation loss does not improve over the previous epoch. 5 Experimental setup 5.1 Datasets for pretraining We train the two tower model on several datasets. Common Crawl. We consider various subsets of Common Crawl which is web data. We fol- low the same pre-processing as Grave et al. (2018) which is based on the May 2017 Common Crawl dump. This setup add 20 copies of English Wikipedia resulting in about 14% of the \ufb01nal dataset to be Wikipedia. We subsample up to 18B tokens. All experiments use Common Crawl sub- sampled to 9B tokens, except \u00a76.4. News Crawl. We use up to 4.5B words of En- glish news web data distributed as part of WMT 2018 (Bojar et al., 2018). BooksCorpus + Wikipedia. This is similar to the training data used by BERT which comprises the BooksCorpus (Zhu et al., 2015) of about 800M words plus English Wikipedia data of 2.5B words. 5.2 Pretraining hyper-parameters We adapt the transformer implementation avail- able in the fairseq toolkit to our two tower archi- tecture (Ott et al., 2019). For hyper-parameter and optimization choices we mostly follow Baevski and Auli (2018). Our experiments consider three",
                "metadata": {
                    "filename": "1903.07785.pdf",
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 1799
                }
            },
            {
                "content": "(Table 1). 6 Results There are three tasks assessing sentence sim- ilarity: The Microsoft Research Paragraph Cor- pus (MRPC; Dolan and Brockett, 2015) and the Quora Question Pairs benchmark (QQP); we eval- uate in terms of F1. The Semantic Textual Similar- ity Benchmark (STS-B; Cer et al., 2017) requires predicting a similarity score between 1 and 5 for a sentence pair; we report the Spearman correlation coef\ufb01cient (scc). Finally, there are four natural laguage inference tasks: the Multi-Genre Natural Language Infer- ence (MNLI; Williams et al., 2018), the Stanford Question Answering Dataset (QNLI; Rajpurkar et al., 2016), the Recognizing Textual Entailment (RTE; Dagan et al., 2006, Bar Haim et al., 2006, Ciampiccolo et al., 2007 Bentivogli et al., 2009). We exclude the Winograd NLI task from our re- sults similar to Radford et al. (2018); Devlin et al. (2018) and report accuracy. For MNLI we report both matched (m) and mismatched (mm) accuracy on test. We also report an average over the GLUE met- rics. This \ufb01gure is not comparable to the aver- age on the of\ufb01cial GLUE leaderboard since we ex- clude Winograd and do not report MRPC accuracy STS-B Pearson correlation as well as QQP accu- racy. 6.1 GLUE First, we conduct experiments on the general language understanding evaluation benchmark (GLUE; Wang et al., 2018) and present a short overview of the tasks. More information can be found in Wang et al. (2018). There are two single- sentence classi\ufb01cation tasks: First, the Corpus of Table 2 shows results for three con\ufb01gurations of our approach (cf. Table 1). The BPE model has more parameters than the CNN model but does not perform better in aggregate, however, it is faster to train. All our models outperform the uni-directional transformer (OpenAI GPT) of Radford et al. (2018), however, our model is about",
                "metadata": {
                    "start_index": 1804,
                    "filename": "1903.07785.pdf",
                    "type": "Text",
                    "page_number": 5
                }
            },
            {
                "content": "ni\ufb01cantly better than the bilm loss and that com- bining the two loss types does not improve over the cloze loss by itself. We conjecture that in- dividual left and right context prediction tasks are too different from center word prediction and that their learning signals are not complementary enough. 6.4 Domain and amount of training data Next we investigate how much pretraining bene\ufb01ts from larger training corpora and how the domain of the data in\ufb02uences end-task performance. Figure 3 shows that more training data can sig- ni\ufb01cantly increase accuracy. We train all models with the exact same hyper-parameter settings on Common Crawl data using the CNN base archi- tecture for 600K updates. We train on up to 18B Common Crawl tokens and the results suggest that more training data is likely to further increase per- formance. Table 6 shows a breakdown into individual GLUE tasks. For pretraining on Common Crawl, CoLA and RTE bene\ufb01t most from additional train- ing data. The same table also shows results for News Crawl which contains newswire data. This data generally performs less well than Common Crawl, even on MRPC which is newswire. A",
                "metadata": {
                    "page_number": 7,
                    "filename": "1903.07785.pdf",
                    "start_index": 1799,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Hyper-parameters for our models. Parameter count excludes the (adaptive) softmax layer. Train time as measured on 128 Volta GPUs for the CNN models and 64 Volta GPUs for the BPE model.\nModel Parameters Updates Blocks FFN Dim Attn Heads (\ufb01nal layer) Query formation (\ufb01nal layer) Train time (days) CNN Base 177M 600K 6 4096 12 Sum 6 CNN Large 330M 1M 12 4096 32 Concat 10 BPE Large 370M 1M 12 4096 32 Concat 4.5",
                "metadata": {
                    "filename": "1903.07785.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2 shows results for three con\ufb01gurations of our approach (cf. Table 1). The BPE model has more parameters than the CNN model but does not perform better in aggregate, however, it is faster to train. All our models outperform the uni-directional transformer (OpenAI GPT) of Radford et al. (2018), however, our model is about\nCoLA (mcc) SST-2 (acc) MRPC (F1) STS-B (scc) QQP (F1) MNLI-(m/mm) (acc) QNLI (acc) RTE (acc) Avg OpenAI GPT 45.4 91.3 82.3 80.0 70.3 82.1/81.4 88.1 56.0 75.2 CNN Base 53.1 93.6 81.3 82.2 70.5 82.5/82.2 89.5 64.6 77.7 CNN Large 52.8 94.6 83.7 83.4 71.7 84.3/83.8 89.8 63.7 78.6 BPE Large 51.8 94.0 83.0 84.2 70.6 82.9/82.2 89.3 65.1 78.1 GPT on STILTs 47.2 93.1 87.7 84.8 70.1 80.7/80.6 87.2 69.1 77.8 BERTBASE 52.1 93.5 88.9 85.8 71.2 84.6/83.4 90.1 66.4 79.6 BERTLARGE 60.5 94.9 89.3 86.5 72.1 86.7/85.9 91.1 70.1 81.9",
                "metadata": {
                    "filename": "1903.07785.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Model dev F1 test F1 ELMoBASE 95.7 92.2 CNN Large + ELMo 96.4 93.2 CNN Large + \ufb01ne-tune 96.9 93.5 BERTBASE BERTLARGE 96.4 96.6 92.4 92.8",
                "metadata": {
                    "filename": "1903.07785.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: CoNLL-2003 Named Entity Recognition re- sults. Test result was evaluated on parameter set with the best dev F1.\nModel dev F1 test F1 ELMoBASE 95.2 95.1 CNN Large + ELMo 95.1 95.2 CNN Large + \ufb01ne-tune 95.5 95.6",
                "metadata": {
                    "filename": "1903.07785.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3 shows the results, with comparison to previous published ELMoBASE results (Peters et al., 2018) and the BERT models. Both of our stacking methods outperform the previous state of\nCoLA (mcc) SST-2 (acc) MRPC (F1) STS-B (scc) QQP (F1) MNLI-m (acc) QNLI (acc) RTE (acc) Avg cloze 55.1 92.9 88.3 88.3 87.2 82.3 86.5 66.4 80.9 bilm 50.0 92.4 86.6 87.1 86.1 81.7 84.0 66.4 79.3 cloze + bilm 52.6 93.2 88.9 87.9 87.2 82.1 86.1 65.5 80.4",
                "metadata": {
                    "filename": "1903.07785.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 6 shows a breakdown into individual GLUE tasks. For pretraining on Common Crawl, CoLA and RTE bene\ufb01t most from additional train- ing data. The same table also shows results for News Crawl which contains newswire data. This data generally performs less well than Common Crawl, even on MRPC which is newswire. A\ntrain data (M tok) CoLA (mcc) SST-2 (acc) MRPC (F1) STS-B (scc) QQP (F1) MNLI-m (acc) QNLI (acc) RTE (acc) Avg 562 52.5 92.9 88.2 88.3 87.1 81.7 85.7 63.3 79.9 1125 55.5 93.1 86.1 88.4 87.1 81.9 85.7 65.2 80.4 ccrawl 2250 4500 55.4 56.6 92.4 93.0 87.7 87.3 88.4 88.6 87.2 87.0 82.2 82.0 86.2 86.2 66.9 65.7 80.8 80.8 9000 55.1 92.9 88.3 88.3 87.2 82.3 86.5 66.4 80.9 18000 56.3 93.1 88.0 88.8 87.2 82.3 86.3 68.4 81.3 562 50.9 92.8 81.4 78.2 84.9 79.1 82.0 55.7 75.6 news crawl 1125 2250 51.4 54.8 93.0 92.9 83.0 83.5 82.3 82.8 85.2 85.4 79.7 80.4 82.8 82.4 53.9 54.8 76.4 77.1 4500 53.9 93.6 83.8 83.1 85.5 80.4 83.6 54.2 77.3 BWiki - sent 3300 53.5 91.6 86.4 86.2 86.9 82.3 86.9 63.8 79.7 BWiki - blck 3300 50.6 91.9 86.4 87.1 86.8 81.9 86.2 60.4 78.9",
                "metadata": {
                    "filename": "1903.07785.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            }
        ]
    },
    "1808.08745.pdf": {
        "normalized_output": [
            {
                "Task": "Summarization",
                "Dataset": "XSum",
                "Metric": "ROGUE-1</s>",
                "Result": "31.89"
            },
            {
                "Task": "Summarization",
                "Dataset": "XSum",
                "Metric": "ROGUE-2",
                "Result": "11.54"
            },
            {
                "Task": "Summarization",
                "Dataset": "XSum",
                "Metric": "ROGUE-L",
                "Result": "25.75"
            },
            {
                "Task": "Summarization",
                "Dataset": "XSum",
                "Metric": "Exact Match (EM)",
                "Result": "46.05"
            },
            {
                "Task": "Summarization",
                "Dataset": "XSum",
                "Metric": "Sent-Accuracy",
                "Result": "0.037"
            }
        ],
        "source_documents": [
            {
                "content": "with- out access to the document or the gold summary. The more questions can be answered, the better the corresponding system is at summarizing the docu- ment as a whole. Five participants answered ques- tions for each summary. We followed the scoring mechanism introduced in Clarke and Lapata (2010). A correct answer was marked with a score of one, partially correct answers with a score of 0.5, and zero otherwise. The \ufb01nal score for a system is the average of all its question scores. Answers again were elicited using Amazon\u2019s Mechanical Turk crowdsourcing platform. We uploaded the data in batches (one system at a time) to ensure that the same partic- ipant does not evaluate summaries from different systems on the same set of questions. Table 7 shows the results of the QA evaluation. Based on summaries generated by T-CONVS2S, participants can answer 46.05% of the questions correctly. Summaries generated by CONVS2S, PTGEN and EXT-ORACLE provide answers to 30.90%, 21.40%, and 15.70% of the questions, re- spectively. Pairwise differences between systems are all statistically signi\ufb01cant (p < 0.01) with the exception of PTGEN and EXT-ORACLE. EXT- ORACLE performs poorly on both QA and rating evaluations. The examples in Table 6 indicate that EXT-ORACLE is often misled by selecting a sen- tence with the highest ROUGE (against the gold summary), but ROUGE itself does not ensure that the summary retains the most important informa- tion from the document. The QA evaluation fur- ther emphasizes that in order for the summary to be felicitous, information needs to be embedded in the appropriate context. For example, CONVS2S and PTGEN will fail to answer the question \u201cWho has resigned?\u201d (see Table 6 second block) de- spite containing the correct answer \u201cDick Advo- caat\u201d due to the wrong context. T-CONVS2S is able to extract important entities from the docu- ment with the right theme. 6 Conclusions In this paper we introduced the task of \u201cextreme summarization\u201d together with a",
                "metadata": {
                    "page_number": 9,
                    "filename": "1808.08745.pdf",
                    "type": "Text",
                    "start_index": 1802
                }
            },
            {
                "content": "reduced the learning rate by an order of magnitude after each epoch until it fell below 10\u22124. We also applied a dropout of 0.2 to the embeddings, the decoder outputs and the input of the convolutional blocks. Gradients were normalized by the number of non-padding tokens per mini-batch. We also used weight normalization for all layers except for lookup tables. All neural models, including ours and those based on RNNs (See et al., 2017) had a vocabu- lary of 50,000 words and were trained on a sin- gle Nvidia M40 GPU with a batch size of 32 sen- tences. Summaries at test time were obtained us- ing beam search (with beam size 10). 5 Results Automatic Evaluation We report results us- ing automatic metrics in Table 4. We eval- uated summarization quality using F1 ROUGE (Lin and Hovy, 2003). Unigram and bigram over- lap (ROUGE-1 and ROUGE-2) are a proxy for as- sessing informativeness and the longest common subsequence (ROUGE-L) represents \ufb02uency.4 On the XSum dataset, SEQ2SEQ outper- forms the LEAD and RANDOM baselines by a large margin. PTGEN, a SEQ2SEQ model with a \u201ccopying\u201d mechanism outperforms EXT- ORACLE, a \u201cperfect\u201d extractive system on ROUGE-2 and ROUGE-L. This is in sharp con- trast to the performance of these models on CNN/DailyMail (See et al., 2017) and Newsroom datasets (Grusky et al., 2018), where they fail to outperform the LEAD. The result provides further evidence that XSum is a good testbed for abstrac- tive summarization. PTGEN-COVG, the best per- forming abstractive system on the CNN/DailyMail datasets, does not do well. We believe that the coverage mechanism is more useful when gener- ating multi-line summaries and is basically redun- dant for extreme summarization. CONVS2S, the convolutional variant of SEQ2SEQ, signi\ufb01cantly outperforms all RNN-based abstractive systems. We hypoth- esize that its superior performance stems from the ability to better represent document content (i.e., by capturing long-range dependencies). Table 4 shows several variants",
                "metadata": {
                    "start_index": 0,
                    "filename": "1808.08745.pdf",
                    "type": "Text",
                    "page_number": 7
                }
            },
            {
                "content": "system summaries are shown in Table 6. We ran- domly selected 50 documents from the XSum test set and compared all possible combinations of two out of \ufb01ve systems for each document. We col- lected judgments from three different participants for each comparison. The order of summaries was randomized per document and the order of docu- ments per participant. The score of a system was computed as the percentage of times it was chosen as best mi- nus the percentage of times it was selected as worst. The scores range from -1 (worst) to 1 (best) and are shown in Table 7. Perhaps unsur- prisingly human-authored summaries were con- sidered best, whereas, T-CONVS2S was ranked 2nd followed by EXT-ORACLE and CONVS2S. PTGEN was ranked worst with the lowest score of \u22120.218. We carried out pairwise compar- isons between all models to assess whether sys- tem differences are statistically signi\ufb01cant. GOLD is signi\ufb01cantly different from all other systems and T-CONVS2S is signi\ufb01cantly different from CONVS2S and PTGEN (using a one-way ANOVA with posthoc Tukey HSD tests; p < 0.01). All other differences are not statistically signi\ufb01cant. For our second experiment we used a question- answering (QA) paradigm (Clarke and Lapata, 2010; Narayan et al., 2018b) to assess the degree to which the models retain key information from the document. We used the same 50 documents as in our \ufb01rst elicitation study. We wrote two fact-based questions per document, just by reading the summary, under the assumption that it high- lights the most important content of the news ar- ticle. Questions were formulated so as not to re- veal answers to subsequent questions. We cre- ated 100 questions in total (see Table 6 for exam- ples). Participants read the output summaries and answered the questions as best they could with- out access to the document or the gold summary. The more questions can be answered, the better the corresponding system is at summarizing the docu- ment as a whole. Five participants answered",
                "metadata": {
                    "start_index": 0,
                    "filename": "1808.08745.pdf",
                    "type": "Text",
                    "page_number": 9
                }
            },
            {
                "content": "summaries generated from two out of \ufb01ve systems and were asked to decide which summary was better and which one was worse in order of informativeness (does the summary capture important information in the document?) and \ufb02uency (is the summary written in well-formed English?). Examples of In our \ufb01rst experiment, participants were asked",
                "metadata": {
                    "page_number": 8,
                    "start_index": 1802,
                    "type": "Text",
                    "filename": "1808.08745.pdf"
                }
            },
            {
                "content": "Table 2 provides empirical analysis supporting our claim that XSum is less biased toward ex- tractive methods compared to other summariza- tion datasets. We report the percentage of novel n-grams in the target gold summaries that do not appear in their source documents. There are 36% novel unigrams in the XSum reference summaries compared to 17% in CNN, 17% in DailyMail, and 23% in NY Times. This indicates that XSum summaries are more abstractive. The proportion of novel constructions grows for larger n-grams\navg. document length avg. summary length vocabulary size # docs (train/val/test) Datasets words sentences sentences words document summary CNN 90,266/1,220/1,093 760.50 33.98 45.70 3.59 343,516 89,051 DailyMail 196,961/12,148/10,397 653.33 29.33 54.65 3.86 563,663 179,966 NY Times 589,284/32,736/32,739 800.04 35.55 45.54 2.44 1,399,358 294,011 XSum 204,045/11,332/11,334 431.07 19.77 23.26 1.00 399,147 81,092",
                "metadata": {
                    "filename": "1808.08745.pdf",
                    "page_number": 3,
                    "type": "Table"
                }
            },
            {
                "content": "Table 1: Comparison of summarization datasets with respect to overall corpus size, size of training, validation, and test set, average document (source) and summary (target) length (in terms of words and sentences), and vocabulary size on both on source and target. For CNN and DailyMail, we used the original splits of Hermann et al. (2015) and followed Narayan et al. (2018b) to preprocess them. For NY Times (Sandhaus, 2008), we used the splits and pre-processing steps of Paulus et al. (2018). For the vocabulary, we lowercase tokens.\n% of novel n-grams in gold summary EXT-ORACLE LEAD Datasets trigrams bigrams 4-grams R1 R2 unigrams RL R1 R2 RL 16.75 54.33 72.42 CNN 80.37 29.15 11.13 25.95 50.38 28.55 46.58 DailyMail 17.03 53.78 72.14 80.28 40.68 18.36 37.25 55.12 30.55 51.24 NY Times 22.64 55.59 71.93 80.16 31.85 15.86 23.75 52.08 31.59 46.72 XSum 35.76 83.45 95.50 98.49 16.30 1.61 11.95 29.79 8.81 22.65",
                "metadata": {
                    "filename": "1808.08745.pdf",
                    "page_number": 3,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4: ROUGE results on XSum test set. We re- port ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L (RL) F1 scores. Extractive systems are in the upper block, RNN-based abstractive systems are in the mid- dle block, and convolutional abstractive systems are in the bottom block.\nModels R1 R2 RL Random 15.16 1.78 11.27 LEAD 16.30 1.60 11.95 EXT-ORACLE 29.79 8.81 22.66 SEQ2SEQ 28.42 8.77 22.48 PTGEN 29.70 9.21 23.24 PTGEN-COVG 28.10 8.02 21.72 CONVS2S 31.27 11.07 25.23 T-CONVS2S (enct\u2032) 31.71 11.38 25.56 T-CONVS2S (enct\u2032, dectD) 31.71 11.34 25.61 T-CONVS2S (enc(t\u2032,tD)) 31.61 11.30 25.51 T-CONVS2S (enc(t\u2032,tD), dectD) 31.89 11.54 25.75",
                "metadata": {
                    "filename": "1808.08745.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 5: Proportion of novel n-grams in summaries generated by various models on the XSum test set.\n% of novel n-grams in generated summaries Models unigrams bigrams trigrams 4-grams 0.00 0.00 0.00 0.00 LEAD EXT-ORACLE 0.00 0.00 0.00 0.00 PTGEN 27.40 73.33 90.43 96.04 CONVS2S 31.26 79.50 94.28 98.10 T-CONVS2S 30.73 79.18 94.10 98.03 35.76 83.45 98.49 95.50 GOLD",
                "metadata": {
                    "filename": "1808.08745.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 6: Example output summaries on the XSum test set with [ROUGE-1, ROUGE-2 and ROUGE-L] scores, goldstandard reference, and corresponding questions. Words highlighted in blue are either the right answer or constitute appropriate context for inferring it; words in red lead to the wrong answer.\nEXT-ORACLE Caroline Pidgeon is the Lib Dem candidate, Sian Berry will contest the election for [34.1, 20.5, 34.1] the Greens and UKIP has chosen its culture spokesman Peter Whittle. PTGEN UKIP leader Nigel Goldsmith has been elected as the new mayor of London to elect [45.7, 6.1, 28.6] a new conservative MP. CONVS2S London mayoral candidate Zac Goldsmith has been elected as the new mayor of [53.3, 21.4, 26.7] London. T-CONVS2S Former London mayoral candidate Zac Goldsmith has been chosen to stand in the [50.0, 26.7, 37.5] London mayoral election. GOLD Zac Goldsmith will contest the 2016 London mayoral election for the conservatives, it has been announced. Questions (1) Who will contest for the conservatives? (Zac Goldsmith) (2) For what election will he/she contest? (The London mayoral election) EXT-ORACLE North-east rivals Newcastle are the only team below them in the Premier League [35.3, 18.8, 35.3] table. PTGEN Sunderland have appointed former Sunderland boss Dick Advocaat as manager at [45.0, 10.5, 30.0] the end of the season to sign a new deal. CONVS2S Sunderland have sacked manager Dick Advocaat after less than three months in [25.0, 6.7, 18.8] charge. T-CONVS2S Dick Advocaat has resigned as Sunderland manager until the end of the season. [56.3, 33.3, 56.3] GOLD Dick Advocaat has resigned as Sunderland boss, with the team yet to win in the Premier League this season. Questions (1) Who has resigned? (Dick Advocaat) (2) From what post has he/she resigned? (Sunderland boss) EXT-ORACLE The Greater Ardoyne residents collective (GARC) is protesting against an agree- [26.7, 9.3, 22.2] ment aimed at resolving a long-running dispute in the area. PTGEN A residents\u2019 group has been granted permission for GARC to hold a parade on the [28.6, 5.0, 28.6] outskirts of Crumlin, County Antrim. CONVS2S A protest has been held in the Republic of Ireland calling for an end to parading [42.9, 20.0, 33.3] parading in North Belfast. T-CONVS2S A protest has been held in North Belfast over a protest against the Orange Order in [45.0, 26.3, 45.0] North Belfast. GOLD Church leaders have appealed to a nationalist residents\u2019 group to call off a protest against an Orange Order parade in North Belfast. Questions (1) Where is the protest supposed to happen? (North Belfast) (2) What are they protesting against? (An Orange Order parade)",
                "metadata": {
                    "filename": "1808.08745.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 7: System ranking according to human judg- ments and QA-based evaluation.\nModels Score QA EXT-ORACLE -0.121 15.70 PTGEN -0.218 21.40 CONVS2S -0.130 30.90 T-CONVS2S 0.037 46.05 GOLD 0.431 97.23",
                "metadata": {
                    "filename": "1808.08745.pdf",
                    "page_number": 9,
                    "type": "Table"
                }
            }
        ]
    },
    "wang19f.pdf": {
        "normalized_output": [
            {
                "Task": "Language Modeling",
                "Dataset": "Penn Treebank (PTB)",
                "Metric": "Perplexity",
                "Result": "46.52"
            },
            {
                "Task": "Language Modeling",
                "Dataset": "Wikitext-2",
                "Metric": "Perplexity",
                "Result": "38.65"
            },
            {
                "Task": "Language Modeling",
                "Dataset": "WikiText-103",
                "Metric": "Perplexity",
                "Result": "28.0"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "WMT\u201914 EN-DE",
                "Metric": "BLEU",
                "Result": "29.52"
            },
            {
                "Task": "Machine Translation",
                "Dataset": "IWSLT\u201914 EN-DE",
                "Metric": "BLEU",
                "Result": "35.18"
            }
        ],
        "source_documents": [
            {
                "content": "suggest to use DropConnect (Wan et al., 2013) on the re- current weight matrices and report a series of encouraging benchmark results. Other types of regularization include activation regularization (Merity et al., 2017a), layer nor- malization (Ba et al., 2016), and frequency agnostic train- ing (Gong et al., 2018), etc. Our work is orthogonal to these regularization and optimization techniques and can be eas- ily combined with them to achieve further improvements, as we demonstrate in our experiments. 5. Empirical Results We demonstrate the effectiveness of our method in two ap- plications: neural language modeling and neural machine translation, and compare them with state-of-the-art archi- tectures and learning methods. All models are trained with the weight-tying trick (Press & Wolf, 2016; Inan et al., 2017). Our code is available at: https://github. com/ChengyueGongR/advsoft. 5.1. Experiments on Language Modeling We test our method on three benchmark datasets: Penn Treebank (PTB), Wikitext-2 (WT2) and Wikitext-103 (WT103). Experimental settings For the PTB and WT2 datasets, we closely follow the regularization and optimization tech- niques introduced in AWD-LSTM (Merity et al., 2018a), which stacks a three-layer LSTM and performs optimiza- tion with a bag of tricks. The WT103 corpus contains around 103 million tokens, which is signi\ufb01cantly larger than the PTB and WT2 datasets. In this case, we use Quasi-Recurrent neural networks (QRNN)-based language models (Merity et al., 2018b; Bradbury et al., 2017) as our base model for ef- \ufb01ciency. QRNN allows for parallel computation across both time-step and minibatch dimensions, enabling high throughput and good scaling for long sequences and large datasets. Yang et al. (2018) show that softmax-based language mod- els yield low-rank approximations and do not have enough capacity to model complex natural language. They propose a mixture of softmax (MoS) to break the softmax bottle- neck and achieve signi\ufb01cant",
                "metadata": {
                    "page_number": 5,
                    "filename": "wang19f.pdf",
                    "start_index": 0,
                    "type": "Text"
                }
            },
            {
                "content": "Acknowledgement This work is supported in part by NSF CRII 1830161 and NSF CAREER 1846421. We would like to acknowledge Google Cloud for their support. References",
                "metadata": {
                    "page_number": 9,
                    "type": "Text",
                    "filename": "wang19f.pdf",
                    "start_index": 0
                }
            },
            {
                "content": "\u2192 German (En\u2192De) and IWSLT2014 German \u2192 English (De\u2192En) translation. We use the parallel corpora publicly available at WMT 2014 and IWSLT 2014, which have been widely used for bench- mark neural machine translation tasks (Vaswani et al., 2017; Gehring et al., 2017b). For fair comparison, we fol- low the standard data pre-processing procedures described in Ranzato et al. (2016); Bahdanau et al. (2017). WMT2014 En\u2192De We use the original training set for model training, which consists of 4.5 million sentence pairs. Source and target sentences are encoded by 37K shared sub-word tokens based on byte-pair encoding (BPE) (Sennrich et al., 2016b). We use the concatenation of new- stest2012 and newstest2013 as the validation set and test on newstest2014. IWSLT2014 De\u2192En This dataset contains 160K training sequences pairs and 7K validation sentence pairs. Sen- tences are encoded using BPE with a shared vocabulary of about 33K tokens. We use the concatenation of dev2010, tst2010, tst2011 and tst2011 as the test set, which is widely used in prior works (Bahdanau et al., 2017).",
                "metadata": {
                    "type": "Text",
                    "page_number": 7,
                    "filename": "wang19f.pdf",
                    "start_index": 1806
                }
            },
            {
                "content": "Table 3. Perplexities on validation and test sets on the Wikitext-103 dataset. Experimental settings We choose the Transformer- based state-of-the-art machine translation model (Vaswani et al., 2017) as our base model and use Tensor2Tensor (Vaswani et al., 2018) 1 for implemen- tation. Speci\ufb01cally, to be consistent with prior works, we closely follow the settings reported in Vaswani et al. (2017). We use the Adam optimizer (Kingma & Ba, 2014) and follow the learning rate warm-up strategy in Vaswani et al. (2017). Sentences are pre-processed using byte-pair encoding (Sennrich et al., 2016a) into subword tokens before training, and we measure the \ufb01nal performance with the BLEU score. For the WMT2014 De\u2192En task, we evaluate on the Transformer-Base and Transformer-Big architectures, which consist of a 6-layer encoder and a 6-layer decoder with 512-dimensional and 1024-dimensional hidden units per layer, respectively. For the IWSLT2014 De\u2192En task, we evaluate on two standard con\ufb01gurations: Transformer- Small and Transformer-Base. For Transformer-Small, we stack a 4-layer encoder and a 4-layer decoder with 256- dimensional hidden units per layer. For Transformer-Base, we set the batch size to 6400 and the dropout rate to 0.4 following Wang et al. (2019). For both tasks, we share the BPE subword vocabulary for decoder and encoder. Table 4. BLEU scores on the WMT2014 Ee\u2192De machine trans- lation task. Table 5. BLEU scores on the IWSLT2014 De\u2192En machine translation task. Results From Table 4 and Table 5, we can see that our method improves over the baseline algorithms for all settings. On the WMT2014 De\u2192En translation task, our method reaches 28.43 and 29.52 in BLEU score with the Transformer Base and Transformer Big archi- tectures, respectively; this yields an 1.13/1.12 improve- ment over their corresponding baseline models. On the IWSLT2014 De\u2192En dataset, our method improves the BLEU score from 32.47 to 33.61 and 34.43 to 35.18 for the Transformer-Small and",
                "metadata": {
                    "page_number": 8,
                    "start_index": 0,
                    "filename": "wang19f.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "Improving Neural Language Modeling via Adversarial Training\nMethod Params Valid Test Variational LSTM (Gal & Ghahramani, 2016) 19M - 73.4 Variational LSTM + weight tying (Inan et al., 2017) 51M 71.1 68.5 NAS-RNN (Zoph & Le, 2017) 54M - 62.4 DARTS (Liu et al., 2018a) 23M 58.3 56.1 w/o dynamic evaluation AWD-LSTM (Merity et al., 2018a) 24M 60.00 57.30 AWD-LSTM + Ours 24M 57.15 55.01 AWD-LSTM + MoS (Yang et al., 2018) 22M 56.54 54.44 AWD-LSTM + MoS + Ours 22M 54.98 52.87 AWD-LSTM + MoS + Partial Shuf\ufb02ed (Press, 2019) 22M 55.89 53.92 AWD-LSTM + MoS + Partial Shuf\ufb02ed + Ours 22M 54.10 52.20 + dynamic evaluation (Krause et al., 2018) AWD-LSTM (Merity et al., 2018a) 24M 51.60 51.10 AWD-LSTM +Ours 24M 49.31 48.72 AWD-LSTM + MoS (Yang et al., 2018) 22M 48.33 47.69 AWD-LSTM + MoS + Ours 22M 47.15 46.52 AWD-LSTM + MoS + Partial Shuf\ufb02ed (Press, 2019) 22M 47.93 47.49 AWD-LSTM + MoS + Partial Shuf\ufb02ed + Ours 22M 46.63 46.01",
                "metadata": {
                    "filename": "wang19f.pdf",
                    "page_number": 6,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2. Perplexities on validation and test sets on the Wikitext-2 dataset.\nMethod Params Valid Test Variational LSTM (Inan et al., 2017) (h = 650) 28M 92.3 87.7 Variational LSTM (Inan et al., 2017) (h = 650) + weight tying 28M 91.5 87.0 1-layer LSTM (Mandt et al., 2017) 24M 69.3 65.9 2-layer skip connection LSTM (Mandt et al., 2017) (tied) 24M 69.1 65.9 DARTS (Liu et al., 2018a) 33M 69.5 66.9 w/o \ufb01ne-tune AWD-LSTM (Merity et al., 2018a) 33M 69.10 66.00 AWD-LSTM + Ours 33M 65.76 63.04 AWD-LSTM + MoS (Yang et al., 2018) 35M 66.01 63.33 AWD-LSTM + MoS + Ours 35M 64.07 61.42 + \ufb01ne-tune AWD-LSTM (Merity et al., 2018a) 33M 68.60 65.80 AWD-LSTM + Ours 33M 64.01 61.56 AWD-LSTM + MoS (Yang et al., 2018) 35M 63.88 61.45 AWD-LSTM + MoS + Ours 35M 61.93 59.62 + dynamic evaluation (Krause et al., 2018) AWD-LSTM (Merity et al., 2018a) 33M 46.40 44.30 AWD-LSTM + Ours 33M 42.48 40.71 AWD-LSTM + MoS (Yang et al., 2018) 35M 42.41 40.68 AWD-LSTM + MoS + Ours 35M 40.27 38.65",
                "metadata": {
                    "filename": "wang19f.pdf",
                    "page_number": 7,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3. Perplexities on validation and test sets on the Wikitext-103 dataset.\nMethod Valid Test LSTM (Grave et al., 2017) - 48.7 Temporal CNN (Bai et al., 2018) - 45.2 GCNN (Dauphin et al., 2016) - 37.2 LSTM + Hebbian (Rae et al., 2018) 36.0 36.4 4 layer QRNN (Merity et al., 2018b) 32.0 33.0 4 layer QRNN + Ours 30.6 31.6 + post process (Rae et al., 2018) LSTM + Hebbian + Cache + MbPA (Rae et al., 2018) 29.0 29.2 4 layer QRNN + Ours + dynamic evaluation 27.2 28.0",
                "metadata": {
                    "filename": "wang19f.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Method Local Attention (Luong et al., 2015) ByteNet (Kalchbrenner et al., 2016) ConvS2S (Gehring et al., 2017b) BLEU 20.90 23.75 25.16 Transformer Base (Vaswani et al., 2017) 27.30 Transformer Base + Ours 28.43 Transformer Big (Vaswani et al., 2017) 28.40 Transformer Big + (Gao et al., 2019) Transformer Big + Ours 28.94 29.52",
                "metadata": {
                    "filename": "wang19f.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            },
            {
                "content": "Table 4. BLEU scores on the WMT2014 Ee\u2192De machine trans- lation task.\nMethod BLEU Actor-critic (Bahdanau et al., 2017) 28.53 CNN-a (Gehring et al., 2017a) 30.04 Transformer Small (Vaswani et al., 2017) 32.47 Transformer Small + Ours Transformer Base + (Wang et al., 2019) Transformer Base + Ours 33.61 34.43 35.18",
                "metadata": {
                    "filename": "wang19f.pdf",
                    "page_number": 8,
                    "type": "Table"
                }
            }
        ]
    }
}