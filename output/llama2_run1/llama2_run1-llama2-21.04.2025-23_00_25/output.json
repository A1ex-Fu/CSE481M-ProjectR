{
    "128538.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n\n\n",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Text",
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "page_number": 6,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1804
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "start_index": 1802,
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1511.08308.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n<</SYS>>\n\n[/INST]\n\nInput json: [{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F-score\", \"Result\": 53.48}]\n\nInput json: [{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F-score\", \"Result\": 53.48}]\n\nInput json: [{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F-score\", \"Result\": 50.52}]\n\nInput json: [{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F-score\", \"Result\": 50.52}]\n\nInput json: [{\"Task\": \"CoNLL 2003\", \"Dataset\": \"CoNLL 2003\", \"Metric\": \"F-score\", \"Result\": 89.01}]\n\nInput json: [{\"Task\": \"CoNLL 2003\", \"Dataset\": \"CoNLL 2003\", \"Metric\": \"F-score\", \"Result\": 89.01}]\n\nInput json: [{\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\nInput json: [{\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\nInput json: [{\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\nInput json: [{\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\nInput json: [{\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\nInput json: [{\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\nInput json: [{\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\nInput json: [{\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\nInput json: [{\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\nInput json: [{\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\nInput json: [{\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\nInput json: [{\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\nInput json: [{\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\nInput json: [{\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\nInput json: [{\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\nInput json: [{\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\nInput",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "start_index": 0,
                    "page_number": 4,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "start_index": 3600,
                    "filename": "128538.pdf",
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "page_number": 6,
                    "start_index": 1804,
                    "type": "Text",
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "start_index": 1802,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "page_number": 4
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1603.01354.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\nOutput json: [/INST]\n\\end{code}\n\nComment: I have tried with different parameters like learning rate, dropout rate, embedding size and different architecture of lstm. But I am not getting any improvement.\n\nComment: Please show us the output of your code.\n\nComment: I have added the output of my code.\n\nAnswer: You are not supposed to hard code the dataset path in the code. You can load the dataset using the `train_test_split()` function in `sklearn` and then use the dataset to train and evaluate the model.\n\nFor example:\n\n\\begin{code}\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndata = pd.read_csv(r'C:\\Users\\xyz\\Desktop\\Datasets\\wnut_2016.csv')\nX_train, X_test, y_train, y_test = train_test_split(data['Text'], data['Tagged_Entities'], test_size=0.2, random_state=42)\n\\end{code}\n\nComment: I have tried with train_test_split() function. But still I am not getting any improvement.\n\nComment: @PuneetSingh Please show us the output of your code.\n\nComment: I have added the output of my code.",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "type": "Text",
                    "start_index": 1804
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1702.02098.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 93.09 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 93.09 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1804,
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "start_index": 1802,
                    "filename": "128538.pdf",
                    "page_number": 4
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1703.06345.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[/SYS]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]\n\n]",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "type": "Text",
                    "page_number": 4,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 1804,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "start_index": 1802,
                    "type": "Text",
                    "page_number": 4,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1709.04109.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\nOutput json: [/INST]\n\n\\end{document}\n\\end{code}\n\nAnswer: There are a few things that can be done to improve your code:\n\\begin{itemize}\n\\item Avoid using `\\n` in your document, you should use `\\newline` instead.\n\\item Use `\\nobreak` to avoid line break at the end of the input string.\n\\item Use `\\nobreak` before `\\newline` to avoid line break after `\\newline`.\n\\item If you want to use `\\nobreak` at the end of the input string, you should add `\\par` after `\\newline`.\n\\item Use `\\nobreak` before `\\par` to avoid line break after `\\par`.\n\\item You can use `\\nobreak` at the end of the input string only if the next line is not empty.\n\\item Avoid using `\\nobreak` in your document, you should use `\\nobreak` instead.\n\\item You can use `\\nobreak` in your document only if the next line is not empty.\n\\item Avoid using `\\nobreak` in your document, you should use `\\nobreak` instead.\n\\item You can use `\\nobreak` in your document only if the next line is not empty.\n\\item Avoid using `\\nobreak` in your document, you should use `\\nobreak` instead.\n\\item You can use `\\nobreak` in your document only if the next line is not empty.\n\\item Avoid using `\\nobreak` in your document, you should use `\\nobreak` instead.\n\\item You can use `\\nobreak` in your document only if the next line is not empty.\n\\item Avoid using `\\nobreak` in your document, you should use `\\nobreak` instead.\n\\item You can use `\\nobreak` in your document only if the next line is not empty.\n\\item Avoid using `\\nobreak` in your document, you should use `\\nobreak` instead.\n\\item You can use `\\nobreak` in your document only if the next line is not empty.\n\\item Avoid using `\\nobreak` in your document, you should use `\\nobreak` instead.\n\\item You can use `\\nobreak` in your document only if the next line is not empty.\n\\item Avoid using `\\nobreak` in your document, you should use `\\nobreak` instead.\n\\item You can use `\\nobreak` in your document only if the next line is not empty.\n\\item Avoid using `\\nobreak` in your document, you should use `\\nobreak` instead.\n\\item You can use `\\nobreak` in your document only if the next line is not empty.\n\\item Avoid using `\\nobreak` in your document, you should use `\\nobreak` instead.\n\\item You can use `\\nobreak` in your document only if the next line is not empty.\n\\item Avoid using `\\nobreak` in your document, you should use `\\nobreak` instead.\n\\item You can use `\\nobreak` in your document only if the next line is not empty.\n\\item Avoid using `\\nobreak` in your document, you should use `\\nobreak` instead.\n\\item You can use `\\nobreak` in your document only if the next line is not empty.\n\\item Avoid using `\\nobreak` in your document, you should use `\\nobreak` instead.\n\\item You can use `\\nobreak` in your document only if the next line is not empty.\n\\item Avoid using `\\nobreak` in your document, you should use `\\nobreak` instead.\n\\item You can use `\\nobreak` in your document only if the next line is not empty.\n\\item Avoid using `\\nobreak` in your document, you should use `\\nobreak` instead.\n\\item You can use `\\nobreak` in your document only if the next line is not empty.\n\\item Avoid using `\\nobreak` in your document, you should use `\\nobreak` instead.\n\\item You can use `\\nobreak` in your document only if the next line is not empty.\n\\item Avoid using `\\nobreak` in your document, you should use `\\nobreak` instead.\n\\item You can use `\\nobreak` in your document only if the next line is not empty.\n\\item Avoid using `\\nobreak` in your document, you",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "start_index": 3600,
                    "type": "Text"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "page_number": 6,
                    "type": "Text",
                    "start_index": 1804,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "page_number": 4
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1711.05568.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\nOutput json: [{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-1\",\"Result\":\"53.48\"}]\n\nOutput json: [{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-1\",\"Result\":\"50.52\"}]\n\nOutput json: [{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-1\",\"Result\":\"53.48\"}]\n\nOutput json: [{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-1\",\"Result\":\"50.52\"}]\n\nOutput json: [{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-1\",\"Result\":\"53.48\"}]\n\nOutput json: [{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-1\",\"Result\":\"50.52\"}]\n\nOutput json: [{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-1\",\"Result\":\"53.48\"}]\n\nOutput json: [{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-1\",\"Result\":\"50.52\"}]\n\nOutput json: [{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-1\",\"Result\":\"53.48\"}]\n\nOutput json: [{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-1\",\"Result\":\"50.52\"}]\n\nOutput json: [{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-1\",\"Result\":\"53.48\"}]\n\nOutput json: [{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-1\",\"Result\":\"50.52\"}]\n\nOutput json: [{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-1\",\"Result\":\"53.48\"}]\n\nOutput json: [{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-1\",\"Result\":\"50.52\"}]\n\nOutput json: [{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-1\",\"Result\":\"53.48\"}]\n\nOutput json: [{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-1\",\"Result\":\"50.52\"}]\n\nOutput json: [{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-1\",\"Result\":\"53.48\"}]\n\nOutput json: [{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-1\",\"Result\":\"50.52\"}]\n\nOutput json: [{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-1\",\"Result\":\"53.48\"}]\n\nOutput json: [{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-1\",\"Result\":\"50.52\"}]\n\nOutput json: [{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-1\",\"Result\":\"53.48\"}]\n\nOutput json: [{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-1\",\"Result\":\"50.52\"}]\n\nOutput json: [{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-1\",\"Result\":\"53.48\"}]\n\nOutput json: [{\"Task\":\"WNUT ",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 0,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "start_index": 3600,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1804,
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "type": "Text",
                    "page_number": 4
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1804.09849.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST] [/SYS]]\n",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "page_number": 6,
                    "type": "Text",
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1806.03489.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n>>>\n\n\\end{code}\n\nAnswer: I think you can try something like this:\n\n\\begin{code}\ndef get_json(inst_file):\n    with open(inst_file) as f:\n        data = f.read()\n    data = data.split('[')\n    data = data[1].split(']')\n    data = data[0].split('\\n')\n    data = data[0].split(': ')\n    data = data[1].split(': ')\n    data = data[0].split('|')\n    data = data[0].split('; ')\n    data = data[0].split('| ')\n    data = data[0].split(': ')\n    data = data[0].split('; ')\n    data = data[0].split(': ')\n    data = data[0].split('| ')\n    data = data[0].split(': ')\n    data = data[0].split('| ')\n    data = data[0].split('; ')\n    data = data[0].split('; ')\n    data = data[0].split(': ')\n    data = data[0].split(': ')\n    data = data[0].split('; ')\n    data = data[0].split(': ')\n    data = data[0].split('; ')\n    data = data[0].split(': ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data[0].split('| ')\n    data = data",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "start_index": 3600,
                    "type": "Text"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 1804,
                    "page_number": 6,
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "start_index": 1802,
                    "page_number": 4,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1808.08745.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/INST]\n\nOutput json: [/",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "type": "Text",
                    "start_index": 1804
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "start_index": 1802,
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1809.08370.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[/INST]]\n",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 3600,
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "start_index": 1804,
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1811.05021.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[/SYS]]\n",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "type": "Text"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "start_index": 1804,
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 1802,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1811.09242.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": 53.48}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 50.52}, {\"Task\": \"CoNLL 2003\", \"Dataset\": \"CoNLL 2003 (Eng)\", \"Metric\": \"F1\", \"Result\": 89.01}, {\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal (Partalas et al. 2016)\", \"Metric\": \"F1\", \"Result\": 70.69}, {\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": 46.16}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 40.78}, {\"Task\": \"CoNLL 2003\", \"Dataset\": \"CoNLL 2003 (Eng)\", \"Metric\": \"F1\", \"Result\": 49.59}, {\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal (Partalas et al. 2016)\", \"Metric\": \"F1\", \"Result\": 70.69}, {\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": 46.16}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 40.78}, {\"Task\": \"CoNLL 2003\", \"Dataset\": \"CoNLL 2003 (Eng)\", \"Metric\": \"F1\", \"Result\": 49.59}, {\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal (Partalas et al. 2016)\", \"Metric\": \"F1\", \"Result\": 70.69}, {\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": 46.16}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 40.78}, {\"Task\": \"CoNLL 2003\", \"Dataset\": \"CoNLL 2003 (Eng)\", \"Metric\": \"F1\", \"Result\": 49.59}, {\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal (Partalas et al. 2016)\", \"Metric\": \"F1\", \"Result\": 70.69}, {\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": 46.16}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 40.78}, {\"Task\": \"CoNLL 2003\", \"Dataset\": \"CoNLL 2003 (Eng)\", \"Metric\": \"F1\", \"Result\": 49.59}, {\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal (Partalas et al. 2016)\", \"Metric\": \"F1\", \"Result\": 70.69}, {\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": 46.16}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 40.78}, {\"Task\": \"CoNLL 2003\", \"Dataset\": \"CoNLL 200",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "type": "Text",
                    "page_number": 6,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "start_index": 1804,
                    "page_number": 6,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1812.09471.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST] [{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F1-score\",\"Result\":\"46.16\"}] [{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F1-score\",\"Result\":\"50.52\"}] [{\"Task\":\"CoNLL 2003 (ENG)\",\"Dataset\":\"CoNLL 2003 (ENG)\",\"Metric\":\"F1-score\",\"Result\":\"89.01\"}] [{\"Task\":\"Multimodal (Partalas et al. 2016)\",\"Dataset\":\"Multimodal (Partalas et al. 2016)\",\"Metric\":\"F1-score\",\"Result\":\"70.69\"}] [{\"Task\":\"von Daniken and Cieliebak (2017)\",\"Dataset\":\"von Daniken and Cieliebak (2017)\",\"Metric\":\"F1-score\",\"Result\":\"45.55\"}] [{\"Task\":\"Zhang et al. (2018)\",\"Dataset\":\"Zhang et al. (2018)\",\"Metric\":\"F1-score\",\"Result\":\"40.78\"}] [{\"Task\":\"Aguilar et al. (2019)\",\"Dataset\":\"Aguilar et al. (2019)\",\"Metric\":\"F1-score\",\"Result\":\"49.59\"}] [{\"Task\":\"Baevski et al. (2019)\",\"Dataset\":\"Baevski et al. (2019)\",\"Metric\":\"F1-score\",\"Result\":\"70.69\"}] [{\"Task\":\"Zhou et al. (2019)\",\"Dataset\":\"Zhou et al. (2019)\",\"Metric\":\"F1-score\",\"Result\":\"93.5\"}] [{\"Task\":\"Akbik, Bergmann, and Vollgraf (2019)\",\"Dataset\":\"Akbik, Bergmann, and Vollgraf (2019)\",\"Metric\":\"F1-score\",\"Result\":\"93.09\"}] [{\"Task\":\"Wang et al. (2019)\",\"Dataset\":\"Wang et al. (2019)\",\"Metric\":\"F1-score\",\"Result\":\"74.17\"}]\n\n### 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small training data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs to. Therefore, we tried to use a combination of word and sentence level features to make our model more robust. As shown in Table 2, using word level features with a sentence level feature boosted our performance by 2.8%. We also tried to incorporate character features as in (Limsopatham and Collier 2016) and (Aguilar et al. 2019) but our model was not able to outperform the baseline model. For the WNUT 2017 (Derczynski et al. 2017) dataset, we tried to use a combination of the word level features and sentence level features to achieve the best performance. We observed that using only the word level features with a character level feature did not work. As shown in Table 2, using a combination of word and sentence level features boosted our performance by 2.2%. The WNUT 2017 dataset is larger than the WNUT 2016 dataset and has more overlapping entities. Therefore, we tried to incorporate multimodal features to make our model more robust. As shown in Table 2, using a combination of the word level features and multimodal features boosted our performance by 2.7%. Finally, we tried to use the features proposed by (Zhang et al. 2018) and (Aguilar et al. 2019) and observed that our model was not able to outperform the baseline model. Table 1 shows the performance comparison of our model with the state-of-the-art models on the WNUT 2016, 20",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "start_index": 0,
                    "page_number": 4,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "type": "Text",
                    "page_number": 6,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "page_number": 6,
                    "start_index": 1804,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "type": "Text",
                    "page_number": 4
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1812.10235.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[/INST]]\n\n\n",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 0,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "start_index": 1802,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1901.10430.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[\n    {\n        \"Task\": \"WNUT 2016\",\n        \"Dataset\": \"WNUT 2016\",\n        \"Metric\": \"F1\",\n        \"Result\": 46.16\n    },\n    {\n        \"Task\": \"WNUT 2017\",\n        \"Dataset\": \"WNUT 2017\",\n        \"Metric\": \"F1\",\n        \"Result\": 47.27\n    },\n    {\n        \"Task\": \"CoNLL 2003\",\n        \"Dataset\": \"CoNLL 2003\",\n        \"Metric\": \"F1\",\n        \"Result\": 89.01\n    },\n    {\n        \"Task\": \"Multimodal\",\n        \"Dataset\": \"Multimodal\",\n        \"Metric\": \"F1\",\n        \"Result\": 70.69\n    },\n    {\n        \"Task\": \"WNUT 2016\",\n        \"Dataset\": \"WNUT 2016\",\n        \"Metric\": \"F1\",\n        \"Result\": 53.48\n    },\n    {\n        \"Task\": \"WNUT 2017\",\n        \"Dataset\": \"WNUT 2017\",\n        \"Metric\": \"F1\",\n        \"Result\": 50.52\n    },\n    {\n        \"Task\": \"CoNLL 2003\",\n        \"Dataset\": \"CoNLL 2003\",\n        \"Metric\": \"F1\",\n        \"Result\": 93.76\n    },\n    {\n        \"Task\": \"Multimodal\",\n        \"Dataset\": \"Multimodal\",\n        \"Metric\": \"F1\",\n        \"Result\": 74.17\n    },\n    {\n        \"Task\": \"WNUT 2016\",\n        \"Dataset\": \"WNUT 2016\",\n        \"Metric\": \"F1\",\n        \"Result\": 46.16\n    },\n    {\n        \"Task\": \"WNUT 2017\",\n        \"Dataset\": \"WNUT 2017\",\n        \"Metric\": \"F1\",\n        \"Result\": 47.27\n    },\n    {\n        \"Task\": \"CoNLL 2003\",\n        \"Dataset\": \"CoNLL 2003\",\n        \"Metric\": \"F1\",\n        \"Result\": 89.01\n    },\n    {\n        \"Task\": \"Multimodal\",\n        \"Dataset\": \"Multimodal\",\n        \"Metric\": \"F1\",\n        \"Result\": 70.69\n    },\n    {\n        \"Task\": \"WNUT 2016\",\n        \"Dataset\": \"WNUT 2016\",\n        \"Metric\": \"F1\",\n        \"Result\": 46.16\n    },\n    {\n        \"Task\": \"WNUT 2017\",\n        \"Dataset\": \"WNUT 2017\",\n        \"Metric\": \"F1\",\n        \"Result\": 47.27\n    },\n    {\n        \"Task\": \"CoNLL 2003\",\n        \"Dataset\": \"CoNLL 2003\",\n        \"Metric\": \"F1\",\n        \"Result\": 89.01\n    },\n    {\n        \"Task\": \"Multimodal\",\n        \"Dataset\": \"Multimodal\",\n        \"Metric\": \"F1\",\n        \"Result\": 70.69\n    },\n    {\n        \"Task\": \"WNUT 2016\",\n        \"Dataset\": \"WNUT 2016\",\n        \"Metric\": \"F1\",\n        \"Result\": 46.16\n    },\n    {\n        \"Task\": \"WNUT 2017\",\n        \"Dataset\": \"WNUT 2017\",\n        \"Metric\": \"F1\",\n        \"Result\": 47.27\n    },\n    {\n        \"Task\": \"CoNLL 2003\",\n        \"Dataset\": \"CoNLL ",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 1804,
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "start_index": 1802,
                    "page_number": 4,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1903.07785.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[/SYS]]\n\n\n",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "start_index": 0,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "type": "Text",
                    "start_index": 1804
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1905.03197.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n </SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 93.09 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n </SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 93.09 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiL",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "type": "Text",
                    "start_index": 3600,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "start_index": 1804,
                    "filename": "128538.pdf",
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 1802,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1905.12598.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\nInput json: [{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": \"53.48\"}\n\n]\n\nOutput json: [{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": \"50.52\"}\n\n]\n\nOutput json: [{\"Task\": \"CoNLL 2003 (Eng)\", \"Dataset\": \"CoNLL 2003 (Eng)\", \"Metric\": \"F1\", \"Result\": \"93.76\"}\n\n]\n\nOutput json: [{\"Task\": \"Multimodal (Partalas et al. 2016)\", \"Dataset\": \"Multimodal (Partalas et al. 2016)\", \"Metric\": \"F1\", \"Result\": \"74.17\"}\n\n]\n\nOutput json: [{\"Task\": \"Aguilar et al. 2019\", \"Dataset\": \"Aguilar et al. 2019\", \"Metric\": \"F1\", \"Result\": \"74.17\"}\n\n]\n\nOutput json: [{\"Task\": \"Baevski et al. 2019\", \"Dataset\": \"Baevski et al. 2019\", \"Metric\": \"F1\", \"Result\": \"74.17\"}\n\n]\n\nOutput json: [{\"Task\": \"Wang et al. 2019\", \"Dataset\": \"Wang et al. 2019\", \"Metric\": \"F1\", \"Result\": \"74.17\"}\n\n]\n\nOutput json: [{\"Task\": \"Zhou et al. 2019\", \"Dataset\": \"Zhou et al. 2019\", \"Metric\": \"F1\", \"Result\": \"74.17\"}\n\n]\n\nOutput json: [{\"Task\": \"Zhang et al. 2018\", \"Dataset\": \"Zhang et al. 2018\", \"Metric\": \"F1\", \"Result\": \"74.17\"}\n\n]\n\nOutput json: [{\"Task\": \"Akbik, Bergmann, and Vollgraf 2019\", \"Dataset\": \"Akbik, Bergmann, and Vollgraf 2019\", \"Metric\": \"F1\", \"Result\": \"74.17\"}\n\n]\n\nOutput json: [{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": \"46.16\"}\n\n]\n\nOutput json: [{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": \"40.78\"}\n\n]\n\nOutput json: [{\"Task\": \"CoNLL 2003 (Eng)\", \"Dataset\": \"CoNLL 2003 (Eng)\", \"Metric\": \"F1\", \"Result\": \"89.01\"}\n\n]\n\nOutput json: [{\"Task\": \"Multimodal (Partalas et al. 2016)\", \"Dataset\": \"Multimodal (Partalas et al. 2016)\", \"Metric\": \"F1\", \"Result\": \"93.09\"}\n\n]\n\nOutput json: [{\"Task\": \"Aguilar et al. 2019\", \"Dataset\": \"Aguilar et al. 2019\", \"Metric\": \"F1\", \"Result\": \"93.09\"}\n\n]\n\nOutput json: [{\"Task\": \"Baevski et al. 2019\", \"Dataset\": \"Baevski et al. 2019\", \"Metric\": \"F1\", \"Result\": \"93.09\"}\n\n]\n\nOutput json: [{\"Task\": \"Wang et al. 2019\", \"Dataset\": \"Wang et al. 2019\", \"Metric\": \"F1\", \"Result\": \"93.09\"}\n\n]\n\nOutput json: [{\"Task\": \"Zhou et al",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 3600,
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "start_index": 1804,
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1906.05012.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n<</SYS>>\n\n[{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": \"53.48\"}]\n[{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": \"50.52\"}]\n[{\"Task\": \"CoNLL 2003\", \"Dataset\": \"CoNLL 2003\", \"Metric\": \"F1\", \"Result\": \"89.01\"}]\n[{\"Task\": \"Multimodal (Partalas et al. 2016)\", \"Dataset\": \"Multimodal (Partalas et al. 2016)\", \"Metric\": \"F1\", \"Result\": \"74.17\"}]\n[{\"Task\": \"Aguilar et al. (2019)\", \"Dataset\": \"Aguilar et al. (2019)\", \"Metric\": \"F1\", \"Result\": \"74.17\"}]\n[{\"Task\": \"Baevski et al. (2019)\", \"Dataset\": \"Baevski et al. (2019)\", \"Metric\": \"F1\", \"Result\": \"74.17\"}]\n[{\"Task\": \"Zhou et al. (2019)\", \"Dataset\": \"Zhou et al. (2019)\", \"Metric\": \"F1\", \"Result\": \"74.17\"}]\n[{\"Task\": \"Zhang et al. (2018)\", \"Dataset\": \"Zhang et al. (2018)\", \"Metric\": \"F1\", \"Result\": \"74.17\"}]\n[{\"Task\": \"Akbik, Bergmann, and Vollgraf (2019)\", \"Dataset\": \"Akbik, Bergmann, and Vollgraf (2019)\", \"Metric\": \"F1\", \"Result\": \"74.17\"}]\n\n[{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": \"74.17\"}]\n[{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": \"74.17\"}]\n[{\"Task\": \"CoNLL 2003\", \"Dataset\": \"CoNLL 2003\", \"Metric\": \"F1\", \"Result\": \"74.17\"}]\n[{\"Task\": \"Multimodal (Partalas et al. 2016)\", \"Dataset\": \"Multimodal (Partalas et al. 2016)\", \"Metric\": \"F1\", \"Result\": \"74.17\"}]\n[{\"Task\": \"Aguilar et al. (2019)\", \"Dataset\": \"Aguilar et al. (2019)\", \"Metric\": \"F1\", \"Result\": \"74.17\"}]\n[{\"Task\": \"Baevski et al. (2019)\", \"Dataset\": \"Baevski et al. (2019)\", \"Metric\": \"F1\", \"Result\": \"74.17\"}]\n[{\"Task\": \"Zhou et al. (2019)\", \"Dataset\": \"Zhou et al. (2019)\", \"Metric\": \"F1\", \"Result\": \"74.17\"}]\n[{\"Task\": \"Zhang et al. (2018)\", \"Dataset\": \"Zhang et al. (2018)\", \"Metric\": \"F1\", \"Result\": \"74.17\"}]\n[{\"Task\": \"Akbik, Bergmann, and Vollgraf (2019)\", \"Dataset\": \"Akbik, Bergmann, and Vollgraf (2019)\", \"Metric\": \"F1\", \"Result\": \"74.17\"}]\n\n[{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": \"74.17\"}]\n[{\"Task",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "start_index": 1804,
                    "page_number": 6,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1907.00390.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST] [{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F-Score\", \"Result\": \"47.48\"}]\n\nOutput json: [/INST] [{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F-Score\", \"Result\": \"50.52\"}]\n\nOutput json: [/INST] [{\"Task\": \"CoNLL 2003 (EN)\", \"Dataset\": \"CoNLL 2003 (EN)\", \"Metric\": \"F-Score\", \"Result\": \"89.01\"}]\n\nOutput json: [/INST] [{\"Task\": \"Multimodal (Partalas et al. 2016)\", \"Dataset\": \"Multimodal (Partalas et al. 2016)\", \"Metric\": \"F-Score\", \"Result\": \"74.17\"}]\n\nOutput json: [/INST] [{\"Task\": \"Aguilar et al. 2019\", \"Dataset\": \"Aguilar et al. 2019\", \"Metric\": \"F-Score\", \"Result\": \"74.17\"}]\n\nOutput json: [/INST] [{\"Task\": \"Baevski et al. 2019\", \"Dataset\": \"Baevski et al. 2019\", \"Metric\": \"F-Score\", \"Result\": \"93.5\"}]\n\nOutput json: [/INST] [{\"Task\": \"Zhou et al. 2019\", \"Dataset\": \"Zhou et al. 2019\", \"Metric\": \"F-Score\", \"Result\": \"93.5\"}]\n\nOutput json: [/INST] [{\"Task\": \"Wang et al. 2019\", \"Dataset\": \"Wang et al. 2019\", \"Metric\": \"F-Score\", \"Result\": \"74.17\"}]\n\nOutput json: [/INST] [{\"Task\": \"Zhang et al. 2018\", \"Dataset\": \"Zhang et al. 2018\", \"Metric\": \"F-Score\", \"Result\": \"74.17\"}]\n\nOutput json: [/INST] [{\"Task\": \"Akbik, Bergmann, and Vollgraf 2019\", \"Dataset\": \"Akbik, Bergmann, and Vollgraf 2019\", \"Metric\": \"F-Score\", \"Result\": \"93.47\"}]\n\nOutput json: [/INST] [{\"Task\": \"Zhou et al. 2019\", \"Dataset\": \"Zhou et al. 2019\", \"Metric\": \"F-Score\", \"Result\": \"93.5\"}]\n\nOutput json: [/INST] [{\"Task\": \"Wang et al. 2019\", \"Dataset\": \"Wang et al. 2019\", \"Metric\": \"F-Score\", \"Result\": \"93.47\"}]\n\nOutput json: [/INST] [{\"Task\": \"Akbik, Bergmann, and Vollgraf 2019\", \"Dataset\": \"Akbik, Bergmann, and Vollgraf 2019\", \"Metric\": \"F-Score\", \"Result\": \"93.47\"}]\n\nOutput json: [/INST] [{\"Task\": \"Akbik, Bergmann, and Vollgraf 2019\", \"Dataset\": \"Akbik, Bergmann, and Vollgraf 2019\", \"Metric\": \"F-Score\", \"Result\": \"93.47\"}]\n\nOutput json: [/INST] [{\"Task\": \"Zhou et al. 2019\", \"Dataset\": \"Zhou et al. 2019\", \"Metric\": \"F-Score\", \"Result\": \"93.5\"}]\n\nOutput json: [/INST] [{\"Task\": \"Wang et al. 2019\", \"Dataset\": \"Wang et al. 2019\", \"Metric\": \"F-Score\", \"Result\": \"93.47\"}]\n\nOutput json: [/INST] [{\"Task\": \"Akbik, Berg",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 3600,
                    "page_number": 6,
                    "type": "Text"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "type": "Text",
                    "page_number": 6,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1908.08345.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n </SYS>>\n\n\n\n\n",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 3600,
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "type": "Text",
                    "page_number": 6,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "type": "Text",
                    "page_number": 4
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1909.02188.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n#!/bin/bash # script to extract data from research paper # 1. extract task, dataset and evaluation metrics from paper # 2. extract best results from paper # 3. output as json # 4. use to train the model # #1. extract task, dataset and evaluation metrics from paper # extract_data.sh <paper-link> # extract_data.sh https://arxiv.org/abs/1903.08049 # 2. extract best results from paper # extract_results.sh <paper-link> # extract_results.sh https://arxiv.org/abs/1903.08049 # 3. output as json # 4. use to train the model # extract_json.sh <paper-link> # extract_json.sh https://arxiv.org/abs/1903.08049 # # extract task, dataset and evaluation metrics from paper # extract_data.sh <paper-link> # extract_data.sh https://arxiv.org/abs/1903.08049 # extract_results.sh <paper-link> # extract_results.sh https://arxiv.org/abs/1903.08049 # extract_json.sh <paper-link> # extract_json.sh https://arxiv.org/abs/1903.08049\n\n#!/bin/bash # script to extract data from research paper # 1. extract task, dataset and evaluation metrics from paper # 2. extract best results from paper # 3. output as json # 4. use to train the model # #1. extract task, dataset and evaluation metrics from paper # extract_data.sh <paper-link> # extract_data.sh https://arxiv.org/abs/1903.08049 # extract_results.sh <paper-link> # extract_results.sh https://arxiv.org/abs/1903.08049 # extract_json.sh <paper-link> # extract_json.sh https://arxiv.org/abs/1903.08049 # # extract task, dataset and evaluation metrics from paper # extract_data.sh <paper-link> # extract_data.sh https://arxiv.org/abs/1903.08049 # extract_results.sh <paper-link> # extract_results.sh https://arxiv.org/abs/1903.08049 # extract_json.sh <paper-link> # extract_json.sh https://arxiv.org/abs/1903.08049\n\\end{code}\n\nComment: I am trying to extract only best results obtained by proposed methods of the paper not baselines.\n\nComment: Then you should be able to use the `grep` command to extract only the lines that contain the word \"best\" in the paper.\n\nComment: I am getting the error for the following line `extract_results.sh <paper-link> # extract_results.sh https://arxiv.org/abs/1903.08049`\n\nComment: I have updated my answer with the code I've used to extract the results.\n\nComment: Thanks for your help. But I am getting error for the following line `extract_json.sh <paper-link> # extract_json.sh https://arxiv.org/abs/1903.08049`\n\nComment: I've updated the answer with the code I used. I've tested it with the link you provided and it worked for me.\n\nComment: I am getting the following error when I am trying to run `extract_results.sh <paper-link> # extract_results.sh https://arxiv.org/abs/1903.08049`\n\nComment: ```bash: extract_results.sh: command not found```\n\nComment: I've updated the answer with a working version.\n\nComment: Thanks for your help. But I am getting error for the following line `extract_json.sh <paper-link> # extract_json.sh https://arxiv.org/abs/1903.08049`\n\nComment: I've updated the answer with the code I've used to extract the results.",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "start_index": 1804
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1910.13461.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"46.16\"}]\n[{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-score\",\"Result\":\"45.55\"}]\n[{\"Task\":\"CoNLL 2003 (ENG)\",\"Dataset\":\"CoNLL 2003 (ENG)\",\"Metric\":\"F-score\",\"Result\":\"89.01\"}]\n[{\"Task\":\"Multimodal (Partalas et al. 2016)\",\"Dataset\":\"Multimodal (Partalas et al. 2016)\",\"Metric\":\"F-score\",\"Result\":\"70.69\"}]\n[{\"Task\":\"von Daniken and Cieliebak 2017\",\"Dataset\":\"von Daniken and Cieliebak 2017\",\"Metric\":\"F-score\",\"Result\":\"45.55\"}]\n[{\"Task\":\"Aguilar et al. 2019\",\"Dataset\":\"Aguilar et al. 2019\",\"Metric\":\"F-score\",\"Result\":\"49.59\"}]\n[{\"Task\":\"Baevski et al. 2019\",\"Dataset\":\"Baevski et al. 2019\",\"Metric\":\"F-score\",\"Result\":\"49.59\"}]\n[{\"Task\":\"Wang et al. 2019\",\"Dataset\":\"Wang et al. 2019\",\"Metric\":\"F-score\",\"Result\":\"50.03\"}]\n[{\"Task\":\"Zhou et al. 2019\",\"Dataset\":\"Zhou et al. 2019\",\"Metric\":\"F-score\",\"Result\":\"53.43\"}]\n[{\"Task\":\"Ours\",\"Dataset\":\"Ours\",\"Metric\":\"F-score\",\"Result\":\"53.48\"}]\n\n[{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"47.64\"}]\n[{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-score\",\"Result\":\"47.27\"}]\n[{\"Task\":\"CoNLL 2003 (ENG)\",\"Dataset\":\"CoNLL 2003 (ENG)\",\"Metric\":\"F-score\",\"Result\":\"89.01\"}]\n[{\"Task\":\"Multimodal (Partalas et al. 2016)\",\"Dataset\":\"Multimodal (Partalas et al. 2016)\",\"Metric\":\"F-score\",\"Result\":\"70.69\"}]\n[{\"Task\":\"von Daniken and Cieliebak 2017\",\"Dataset\":\"von Daniken and Cieliebak 2017\",\"Metric\":\"F-score\",\"Result\":\"45.55\"}]\n[{\"Task\":\"Aguilar et al. 2019\",\"Dataset\":\"Aguilar et al. 2019\",\"Metric\":\"F-score\",\"Result\":\"49.59\"}]\n[{\"Task\":\"Baevski et al. 2019\",\"Dataset\":\"Baevski et al. 2019\",\"Metric\":\"F-score\",\"Result\":\"49.59\"}]\n[{\"Task\":\"Wang et al. 2019\",\"Dataset\":\"Wang et al. 2019\",\"Metric\":\"F-score\",\"Result\":\"50.03\"}]\n[{\"Task\":\"Zhou et al. 2019\",\"Dataset\":\"Zhou et al. 2019\",\"Metric\":\"F-score\",\"Result\":\"53.43\"}]\n[{\"Task\":\"Ours\",\"Dataset\":\"Ours\",\"Metric\":\"F-score\",\"Result\":\"53.48\"}]\n\n[{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"47.64\"}]\n[{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-score\",\"Result\":\"47.27\"}]\n[{\"Task\":\"CoNLL 2003 (ENG)",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "start_index": 1802,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1911.09483.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n\n\n",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "start_index": 1802,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1911.10390.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\nOutput json: [{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": \"53.48\"}]\n\nOutput json: [{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": \"53.48\"}]\n\nOutput json: [{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": \"50.52\"}]\n\nOutput json: [{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": \"50.52\"}]\n\nOutput json: [{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": \"53.48\"}]\n\nOutput json: [{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": \"53.48\"}]\n\nOutput json: [{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": \"50.52\"}]\n\nOutput json: [{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": \"50.52\"}]\n\nOutput json: [{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": \"53.48\"}]\n\nOutput json: [{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": \"53.48\"}]\n\nOutput json: [{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": \"50.52\"}]\n\nOutput json: [{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": \"50.52\"}]\n\nOutput json: [{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": \"53.48\"}]\n\nOutput json: [{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": \"53.48\"}]\n\nOutput json: [{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": \"50.52\"}]\n\nOutput json: [{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": \"50.52\"}]\n\nOutput json: [{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": \"53.48\"}]\n\nOutput json: [{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": \"53.48\"}]\n\nOutput json: [{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": \"50.52\"}]\n\nOutput json: [{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": \"50.52\"}]\n\nOutput json: [{\"Task\": \"WNUT 2016\", \"Dataset\": \"WN",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "start_index": 3600,
                    "type": "Text"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "1912.08777.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[/INST]\n\n\n",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "start_index": 3600,
                    "page_number": 6,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "page_number": 6,
                    "type": "Text",
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 1802,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "2001.04063.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n<</INST>>\n\nWang et al. 2019) - - - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n<</INST>>\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n<</INST>>\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n<</INST>>\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n<</INST>>\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n<</INST>>\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3. The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up... 5 if Leicester",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "start_index": 0,
                    "page_number": 4,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "type": "Text",
                    "page_number": 6,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "start_index": 1802,
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "2002.03184.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F1\",\"Result\":\"53.48\"}]\n\n[{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F1\",\"Result\":\"50.52\"}]\n\n[{\"Task\":\"CoNLL 2003\",\"Dataset\":\"CoNLL 2003\",\"Metric\":\"F1\",\"Result\":\"89.01\"}]\n\n[{\"Task\":\"Multimodal\",\"Dataset\":\"Multimodal\",\"Metric\":\"F1\",\"Result\":\"70.69\"}]\n\n[{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F1\",\"Result\":\"46.16\"}]\n\n[{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F1\",\"Result\":\"40.78\"}]\n\n[{\"Task\":\"CoNLL 2003 (Eng)\",\"Dataset\":\"CoNLL 2003 (Eng)\",\"Metric\":\"F1\",\"Result\":\"74.17\"}]\n\n[{\"Task\":\"Multimodal (Partalas et al. 2016)\",\"Dataset\":\"Multimodal (Partalas et al. 2016)\",\"Metric\":\"F1\",\"Result\":\"93.09\"}]\n\n[{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F1\",\"Result\":\"47.27\"}]\n\n[{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F1\",\"Result\":\"48.13\"}]\n\n[{\"Task\":\"CoNLL 2003 (Eng)\",\"Dataset\":\"CoNLL 2003 (Eng)\",\"Metric\":\"F1\",\"Result\":\"93.09\"}]\n\n[{\"Task\":\"Multimodal (Partalas et al. 2016)\",\"Dataset\":\"Multimodal (Partalas et al. 2016)\",\"Metric\":\"F1\",\"Result\":\"93.5\"}]\n\n[{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F1\",\"Result\":\"47.64\"}]\n\n[{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F1\",\"Result\":\"49.87\"}]\n\n[{\"Task\":\"CoNLL 2003 (Eng)\",\"Dataset\":\"CoNLL 2003 (Eng)\",\"Metric\":\"F1\",\"Result\":\"93.09\"}]\n\n[{\"Task\":\"Multimodal (Partalas et al. 2016)\",\"Dataset\":\"Multimodal (Partalas et al. 2016)\",\"Metric\":\"F1\",\"Result\":\"93.5\"}]\n\n[{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F1\",\"Result\":\"47.64\"}]\n\n[{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F1\",\"Result\":\"49.87\"}]\n\n[{\"Task\":\"CoNLL 2003 (Eng)\",\"Dataset\":\"CoNLL 2003 (Eng)\",\"Metric\":\"F1\",\"Result\":\"93.09\"}]\n\n[{\"Task\":\"Multimodal (Partalas et al. 2016)\",\"Dataset\":\"Multimodal (Partalas et al. 2016)\",\"Metric\":\"F1\",\"Result\":\"93.5\"}]\n\n[{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F1\",\"Result\":\"47.64\"}]\n\n[{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F1\",\"Result\":\"49.87\"}]\n\n[{\"Task\":\"CoNLL 2003 (Eng)\",\"Dataset\":\"CoNLL 2003 (Eng)\",\"Metric\":\"F1\",\"",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "type": "Text",
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 3600,
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "2008.07772.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "page_number": 6,
                    "type": "Text",
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "start_index": 1802,
                    "page_number": 4,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "2020.emnlp-main.152.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\nInput json: [{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F-score\", \"Result\": 53.48}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F-score\", \"Result\": 50.52}, {\"Task\": \"CoNLL 2003\", \"Dataset\": \"CoNLL 2003 (Eng)\", \"Metric\": \"F-score\", \"Result\": 74.17}, {\"Task\": \"Multimodal (Partalas et al. 2016)\", \"Dataset\": \"Multimodal (Partalas et al. 2016)\", \"Metric\": \"F-score\", \"Result\": 93.76}, {\"Task\": \"Aguilar et al. (2019)\", \"Dataset\": \"Aguilar et al. (2019)\", \"Metric\": \"F-score\", \"Result\": 93.5}, {\"Task\": \"Baevski et al. (2019)\", \"Dataset\": \"Baevski et al. (2019)\", \"Metric\": \"F-score\", \"Result\": 93.5}, {\"Task\": \"Zhou et al. (2019)\", \"Dataset\": \"Zhou et al. (2019)\", \"Metric\": \"F-score\", \"Result\": 93.47}, {\"Task\": \"Zhang et al. (2018)\", \"Dataset\": \"Zhang et al. (2018)\", \"Metric\": \"F-score\", \"Result\": 70.69}, {\"Task\": \"Akbik et al. (2019)\", \"Dataset\": \"Akbik et al. (2019)\", \"Metric\": \"F-score\", \"Result\": 89.01}, {\"Task\": \"von Daniken and Cieliebak (2017)\", \"Dataset\": \"von Daniken and Cieliebak (2017)\", \"Metric\": \"F-score\", \"Result\": 49.59}, {\"Task\": \"Limsopatham and Collier (2016)\", \"Dataset\": \"Limsopatham and Collier (2016)\", \"Metric\": \"F-score\", \"Result\": 52.41}, {\"Task\": \"Peters et al. (2018)\", \"Dataset\": \"Peters et al. (2018)\", \"Metric\": \"F-score\", \"Result\": 52.31}, {\"Task\": \"Strauss et al. (2016)\", \"Dataset\": \"Strauss et al. (2016)\", \"Metric\": \"F-score\", \"Result\": 46.16}, {\"Task\": \"Wang et al. (2019)\", \"Dataset\": \"Wang et al. (2019)\", \"Metric\": \"F-score\", \"Result\": 74.17}, {\"Task\": \"Zhang et al. (2019)\", \"Dataset\": \"Zhang et al. (2019)\", \"Metric\": \"F-score\", \"Result\": 93.5}, {\"Task\": \"Baevski et al. (2019)\", \"Dataset\": \"Baevski et al. (2019)\", \"Metric\": \"F-score\", \"Result\": 93.47}, {\"Task\": \"Wang et al. (2019)\", \"Dataset\": \"Wang et al. (2019)\", \"Metric\": \"F-score\", \"Result\": 93.47}, {\"Task\": \"Zhou et al. (2019)\", \"Dataset\": \"Zhou et al. (2019)\", \"Metric\": \"F-score\", \"Result\": 93.47}, {\"Task\": \"Zhang et al. (2018)\", \"Dataset\": \"Zhang et al. (2018)\", \"Metric\": \"F-score\", \"Result\": 70.69}, {\"Task\": \"Akbik et al. (2019)\", \"Dataset\": \"Akbik et al. (2019)\", \"Metric\": \"F-score\", \"Result\": 8",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 3600,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "type": "Text"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "start_index": 1802,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 4
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "2020.emnlp-main.523.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[\n  {\n    \"Task\": \"WNUT 2016\",\n    \"Dataset\": \"WNUT 2016\",\n    \"Metric\": \"F1\",\n    \"Result\": 0.5348\n  },\n  {\n    \"Task\": \"WNUT 2017\",\n    \"Dataset\": \"WNUT 2017\",\n    \"Metric\": \"F1\",\n    \"Result\": 0.5052\n  },\n  {\n    \"Task\": \"CoNLL 2003 (EN)\",\n    \"Dataset\": \"CoNLL 2003 (EN)\",\n    \"Metric\": \"F1\",\n    \"Result\": 0.9376\n  },\n  {\n    \"Task\": \"Multimodal (Partalas et al. 2016)\",\n    \"Dataset\": \"Multimodal (Partalas et al. 2016)\",\n    \"Metric\": \"F1\",\n    \"Result\": 0.7417\n  }\n]\n\nOutput json: [/INST]\n\n[\n  {\n    \"Task\": \"WNUT 2016\",\n    \"Dataset\": \"WNUT 2016\",\n    \"Metric\": \"F1\",\n    \"Result\": 0.5348\n  },\n  {\n    \"Task\": \"WNUT 2017\",\n    \"Dataset\": \"WNUT 2017\",\n    \"Metric\": \"F1\",\n    \"Result\": 0.5052\n  },\n  {\n    \"Task\": \"CoNLL 2003 (EN)\",\n    \"Dataset\": \"CoNLL 2003 (EN)\",\n    \"Metric\": \"F1\",\n    \"Result\": 0.9376\n  },\n  {\n    \"Task\": \"Multimodal (Partalas et al. 2016)\",\n    \"Dataset\": \"Multimodal (Partalas et al. 2016)\",\n    \"Metric\": \"F1\",\n    \"Result\": 0.7417\n  }\n]\n\nOutput json: [/INST]\n\n[\n  {\n    \"Task\": \"WNUT 2016\",\n    \"Dataset\": \"WNUT 2016\",\n    \"Metric\": \"F1\",\n    \"Result\": 0.5348\n  },\n  {\n    \"Task\": \"WNUT 2017\",\n    \"Dataset\": \"WNUT 2017\",\n    \"Metric\": \"F1\",\n    \"Result\": 0.5052\n  },\n  {\n    \"Task\": \"CoNLL 2003 (EN)\",\n    \"Dataset\": \"CoNLL 2003 (EN)\",\n    \"Metric\": \"F1\",\n    \"Result\": 0.9376\n  },\n  {\n    \"Task\": \"Multimodal (Partalas et al. 2016)\",\n    \"Dataset\": \"Multimodal (Partalas et al. 2016)\",\n    \"Metric\": \"F1\",\n    \"Result\": 0.7417\n  }\n]\n\nOutput json: [/INST]\n\n[\n  {\n    \"Task\": \"WNUT 2016\",\n    \"Dataset\": \"WNUT 2016\",\n    \"Metric\": \"F1\",\n    \"Result\": 0.5348\n  },\n  {\n    \"Task\": \"WNUT 2017\",\n    \"Dataset\": \"WNUT 2017\",\n    \"Metric\": \"F1\",\n    \"Result\": 0.5052\n  },\n  {\n    \"Task\": \"CoNLL 2003 (EN)\",\n    \"Dataset\": \"CoNLL 2003 (EN)\",\n    \"Metric\": \"F1\",\n    \"Result\": 0.9376\n  },\n  {\n    \"Task\": \"Multimodal (Partalas et al. 2016)\",\n    \"Dataset\": \"Multimodal (Partalas et al. 2",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 3600,
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "page_number": 6,
                    "start_index": 1804,
                    "type": "Text",
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "2020.findings-emnlp.378.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\nOutput json: [{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": 53.48}, {\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": 50.52}, {\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": 93.76}, {\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F1\", \"Result\": 74.17}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 50.52}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 48.13}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 93.47}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 70.69}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 89.01}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 93.5}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 93.09}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 74.17}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 59.56}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 50.52}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 93.5}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 93.09}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 74.17}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 59.56}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 50.52}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 93.5}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 93.09}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 74.17}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1\", \"Result\": 59.56}, {\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F1",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 6,
                    "type": "Text",
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 1804,
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "start_index": 1802,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "2105.03654.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n<</SYS>>\n\n; and a company. We use this dataset for training and testing. The WNUT 2017 (Derczynski et al. 2017) dataset is a novel extension of WNUT 2016 dataset. In this dataset, the authors introduced 4 new entity types: (1) product, (2) location, (3) organization and (4) event. Moreover, the dataset is split into train- ing and test set in a 70:30 ratio. This dataset also includes a new evaluation metric called NER-100. This metric is a 100-fold cross-validation metric. We use the official WNUT scripts to evaluate the performance of the models. CoNLL 2003 (CoNLL 2003) dataset is a standard dataset for NER task. This dataset includes 4 entity types: (1) person, (2) organization, (3) location and (4) miscellaneous. In this dataset, we only use the training data and use the official CoNLL evaluation script to evaluate the performance of the models. Multimodal NER dataset (Partalas et al. 2016) is another dataset we use for evaluating our model. This dataset includes 10 entity types: (1) person, (2) organization, (3) location, (4) musicartist, (5) facility, (6) company, (7) product, (8) time, (9) number and (10) miscellaneous. We use the official evaluation script to evaluate the performance of the models. WNUT 2016 results. Table 1 presents the results of different models on WNUT 2016 dataset. We use 2-Stacked Word BiLSTM as our baseline model and report the results after using the features described in Section 4.1. We also present the results of other state-of-the-art models in the table. As we can see, our model outperforms all the baseline models. Moreover, our model outperforms the state-of-the-art models on WNUT 2016 dataset. WNUT 2017 results. We also report the results of different models on WNUT 2017 dataset in Table 1. We observe that our model outperforms all the baseline models on this dataset. Moreover, our model outperforms the state-of-the-art models. CoNLL 2003 results. We also report the results of different models on CoNLL 2003 dataset in Table 1. We observe that our model outperforms all the baseline models. Moreover, our model outperforms the state-of-the-art models. Multimodal NER results. Table 1 also presents the results of different models on multimodal NER dataset. We observe that our model outperforms all the baseline models. Moreover, our model outperforms the state-of-the-art models.\n\nIn this section, we present the results of our model on the WNUT 2016 and WNUT 2017 datasets. We compare our results with the baseline models and the state-of-the-art models. Table 1 presents the results of different models on WNUT 2016 dataset. We observe that our model outperforms all the baseline models. Moreover, our model outperforms the state-of-the-art models. We also report the results of different models on WNUT 2017 dataset in Table 1. We observe that our model outperforms all the baseline models. Moreover, our model outperforms the state-of-the-art models. Table 1 also presents the results of different models on multimodal NER dataset. We observe that our model outperforms all the baseline models. Moreover, our model outperforms the state-of-the-art models.\n\nIn this section, we present the results of our model on the WNUT 2016 and WNUT 2017 datasets. We compare our results with the baseline models and the state-of-the-art models. Table 1 presents the results of different models on WNUT 2016 dataset. We observe that our model outperforms all the baseline models. Moreover, our model outperforms the state-of-the-art models. We also report the results of different models on WNUT 2017 dataset in Table 1. We observe that our model outperforms all the baseline models.",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Text",
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 3600,
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "start_index": 1804,
                    "filename": "128538.pdf",
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Text",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "2203.16804.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[SYS] <<SYS>>\n\n[INST]\n\n{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"53.48\"}\n\n{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"50.52\"}\n\n{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"47.64\"}\n\n{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"47.27\"}\n\n{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"49.81\"}\n\n{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"48.13\"}\n\n{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"49.87\"}\n\n{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"53.48\"}\n\n{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"50.52\"}\n\n{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-score\",\"Result\":\"47.64\"}\n\n{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-score\",\"Result\":\"47.27\"}\n\n{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-score\",\"Result\":\"49.81\"}\n\n{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-score\",\"Result\":\"48.13\"}\n\n{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-score\",\"Result\":\"49.87\"}\n\n{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-score\",\"Result\":\"53.48\"}\n\n{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-score\",\"Result\":\"50.52\"}\n\n{\"Task\":\"CoNLL 2003\",\"Dataset\":\"CoNLL 2003\",\"Metric\":\"F-score\",\"Result\":\"74.17\"}\n\n{\"Task\":\"Multimodal (Partalas et al. 2016)\",\"Dataset\":\"Multimodal (Partalas et al. 2016)\",\"Metric\":\"F-score\",\"Result\":\"93.76\"}\n\n{\"Task\":\"(Limsopatham and Collier 2016)\",\"Dataset\":\"(Limsopatham and Collier 2016)\",\"Metric\":\"F-score\",\"Result\":\"93.09\"}\n\n{\"Task\":\"(von Daniken and Cieliebak 2017)\",\"Dataset\":\"(von Daniken and Cieliebak 2017)\",\"Metric\":\"F-score\",\"Result\":\"70.69\"}\n\n{\"Task\":\"(Aguilar et al. 2019)\",\"Dataset\":\"(Aguilar et al. 2019)\",\"Metric\":\"F-score\",\"Result\":\"45.55\"}\n\n{\"Task\":\"(Akbik, Bergmann, and Vollgraf 2019)\",\"Dataset\":\"(Akbik, Bergmann, and Vollgraf 2019)\",\"Metric\":\"F-score\",\"Result\":\"93.5\"}\n\n{\"Task\":\"(Baevski et al. 2019)\",\"Dataset\":\"(Baevski et al. 2019)\",\"Metric\":\"F-score\",\"Result\":\"93.5\"}\n\n{\"Task\":\"(Zhou et al. ",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "type": "Text",
                    "page_number": 4,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 1804
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "C18-1139.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F-score\", \"Result\": 46.16}]\n\n[{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F-score\", \"Result\": 40.78}]\n\n[{\"Task\": \"CoNLL 2003\", \"Dataset\": \"CoNLL 2003 (Eng)\", \"Metric\": \"F-score\", \"Result\": 89.01}]\n\n[{\"Task\": \"Multimodal (Partalas et al. 2016)\", \"Dataset\": \"Multimodal (Partalas et al. 2016)\", \"Metric\": \"F-score\", \"Result\": 93.09}]\n\n[{\"Task\": \"von Daniken and Cieliebak (2017)\", \"Dataset\": \"von Daniken and Cieliebak (2017)\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\n[{\"Task\": \"Aguilar et al. (2019)\", \"Dataset\": \"Aguilar et al. (2019)\", \"Metric\": \"F-score\", \"Result\": 93.47}]\n\n[{\"Task\": \"Baevski et al. (2019)\", \"Dataset\": \"Baevski et al. (2019)\", \"Metric\": \"F-score\", \"Result\": 93.5}]\n\n[{\"Task\": \"Zhou et al. (2019)\", \"Dataset\": \"Zhou et al. (2019)\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\n[{\"Task\": \"Zhou et al. (2019)\", \"Dataset\": \"Zhou et al. (2019)\", \"Metric\": \"F-score\", \"Result\": 93.5}]\n\n[{\"Task\": \"Wang et al. (2019)\", \"Dataset\": \"Wang et al. (2019)\", \"Metric\": \"F-score\", \"Result\": 93.47}]\n\n[{\"Task\": \"Akbik, Bergmann, and Vollgraf (2019)\", \"Dataset\": \"Akbik, Bergmann, and Vollgraf (2019)\", \"Metric\": \"F-score\", \"Result\": 93.5}]\n\n[{\"Task\": \"Ours\", \"Dataset\": \"Ours\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\n[{\"Task\": \"Ours\", \"Dataset\": \"Ours\", \"Metric\": \"F-score\", \"Result\": 93.47}]\n\n[{\"Task\": \"Ours\", \"Dataset\": \"Ours\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\n[{\"Task\": \"Ours\", \"Dataset\": \"Ours\", \"Metric\": \"F-score\", \"Result\": 93.47}]\n\n[{\"Task\": \"Ours\", \"Dataset\": \"Ours\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\n[{\"Task\": \"Ours\", \"Dataset\": \"Ours\", \"Metric\": \"F-score\", \"Result\": 93.47}]\n\n[{\"Task\": \"Ours\", \"Dataset\": \"Ours\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\n[{\"Task\": \"Ours\", \"Dataset\": \"Ours\", \"Metric\": \"F-score\", \"Result\": 93.47}]\n\n[{\"Task\": \"Ours\", \"Dataset\": \"Ours\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\n[{\"Task\": \"Ours\", \"Dataset\": \"Ours\", \"Metric\": \"F-score\", \"Result\": 93.47}]\n\n[{\"Task\": \"Ours\", \"Dataset\": \"Ours\", \"Metric\": \"F-score\", \"Result\": 74.17}]\n\n[{\"Task\": \"Ours\", \"Dataset\": \"",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 1804,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "D18-1105.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n [{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-1\",\"Result\":\"53.48\"}]\n\n [{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-1\",\"Result\":\"50.52\"}]\n\n [{\"Task\":\"CoNLL 2003 (ENG)\",\"Dataset\":\"CoNLL 2003 (ENG)\",\"Metric\":\"F-1\",\"Result\":\"46.16\"}]\n\n [{\"Task\":\"Multimodal (Partalas et al. 2016)\",\"Dataset\":\"Multimodal (Partalas et al. 2016)\",\"Metric\":\"F-1\",\"Result\":\"46.16\"}]\n\n [{\"Task\":\"Limsopatham and Collier (2016)\",\"Dataset\":\"Limsopatham and Collier (2016)\",\"Metric\":\"F-1\",\"Result\":\"46.16\"}]\n\n [{\"Task\":\"von Daniken and Cieliebak (2017)\",\"Dataset\":\"von Daniken and Cieliebak (2017)\",\"Metric\":\"F-1\",\"Result\":\"46.16\"}]\n\n [{\"Task\":\"Zhang et al. (2018)\",\"Dataset\":\"Zhang et al. (2018)\",\"Metric\":\"F-1\",\"Result\":\"46.16\"}]\n\n [{\"Task\":\"Aguilar et al. (2019)\",\"Dataset\":\"Aguilar et al. (2019)\",\"Metric\":\"F-1\",\"Result\":\"46.16\"}]\n\n [{\"Task\":\"Akbik, Bergmann, and Vollgraf (2019)\",\"Dataset\":\"Akbik, Bergmann, and Vollgraf (2019)\",\"Metric\":\"F-1\",\"Result\":\"46.16\"}]\n\n [{\"Task\":\"Baevski et al. (2019)\",\"Dataset\":\"Baevski et al. (2019)\",\"Metric\":\"F-1\",\"Result\":\"46.16\"}]\n\n [{\"Task\":\"Wang et al. (2019)\",\"Dataset\":\"Wang et al. (2019)\",\"Metric\":\"F-1\",\"Result\":\"46.16\"}]\n\n [{\"Task\":\"Zhou et al. (2019)\",\"Dataset\":\"Zhou et al. (2019)\",\"Metric\":\"F-1\",\"Result\":\"46.16\"}]\n\n [{\"Task\":\"Ours\",\"Dataset\":\"Ours\",\"Metric\":\"F-1\",\"Result\":\"53.48\"}]\n\n [{\"Task\":\"Ours\",\"Dataset\":\"Ours\",\"Metric\":\"F-1\",\"Result\":\"50.52\"}]\n\n [{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-1\",\"Result\":\"47.64\"}]\n\n [{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-1\",\"Result\":\"47.27\"}]\n\n [{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-1\",\"Result\":\"49.81\"}]\n\n [{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-1\",\"Result\":\"48.13\"}]\n\n [{\"Task\":\"CoNLL 2003 (ENG)\",\"Dataset\":\"CoNLL 2003 (ENG)\",\"Metric\":\"F-1\",\"Result\":\"46.16\"}]\n\n [{\"Task\":\"Multimodal (Partalas et al. 2016)\",\"Dataset\":\"Multimodal (Partalas et al. 2016)\",\"Metric\":\"F-1\",\"Result\":\"46.16\"}]\n\n [{\"Task\":\"Limsopatham and Collier (2016)\",\"Dataset\":\"Limsopatham and Collier (2016)\",\"Metric\":\"F-1\",\"Result\":\"46.16\"}]\n\n [{\"Task\":\"von Daniken and Cieliebak (",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 3600,
                    "page_number": 6
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 1804
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "start_index": 1802,
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "D18-1279.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[</SYS>>]\n\nTable 4: Example of word-level BiLSTM with sentence-level dense layer.\n\n| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "start_index": 3600,
                    "filename": "128538.pdf",
                    "type": "Text"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "start_index": 1804,
                    "type": "Text",
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1802
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "D18-1523.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\nInput json: [{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"53.48\"}\n{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-score\",\"Result\":\"50.52\"}\n{\"Task\":\"CoNLL 2003\",\"Dataset\":\"CoNLL 2003\",\"Metric\":\"F-score\",\"Result\":\"46.16\"}\n{\"Task\":\"Multimodal\",\"Dataset\":\"Multimodal\",\"Metric\":\"F-score\",\"Result\":\"40.78\"}\n{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"46.16\"}\n{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-score\",\"Result\":\"40.78\"}\n{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"46.16\"}\n{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-score\",\"Result\":\"40.78\"}\n{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"46.16\"}\n{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-score\",\"Result\":\"40.78\"}\n{\"Task\":\"CoNLL 2003\",\"Dataset\":\"CoNLL 2003\",\"Metric\":\"F-score\",\"Result\":\"46.16\"}\n{\"Task\":\"Multimodal\",\"Dataset\":\"Multimodal\",\"Metric\":\"F-score\",\"Result\":\"40.78\"}\n{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"46.16\"}\n{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-score\",\"Result\":\"40.78\"}\n{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"46.16\"}\n{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-score\",\"Result\":\"40.78\"}\n{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"46.16\"}\n{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-score\",\"Result\":\"40.78\"}\n{\"Task\":\"CoNLL 2003\",\"Dataset\":\"CoNLL 2003\",\"Metric\":\"F-score\",\"Result\":\"46.16\"}\n{\"Task\":\"Multimodal\",\"Dataset\":\"Multimodal\",\"Metric\":\"F-score\",\"Result\":\"40.78\"}\n{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"46.16\"}\n{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-score\",\"Result\":\"40.78\"}\n{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"46.16\"}\n{\"Task\":\"WNUT 2017\",\"Dataset\":\"WNUT 2017\",\"Metric\":\"F-score\",\"Result\":\"40.78\"}\n{\"Task\":\"CoNLL 2003\",\"Dataset\":\"CoNLL 2003\",\"Metric\":\"F-score\",\"Result\":\"46.16\"}\n{\"Task\":\"Multimodal\",\"Dataset\":\"Multimodal\",\"Metric\":\"F-score\",\"Result\":\"40.78\"}\n{\"Task\":\"WNUT 2016\",\"Dataset\":\"WNUT 2016\",\"Metric\":\"F-score\",\"Result\":\"46.16\"}\n{\"Task\":\"WNUT 20",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "page_number": 6,
                    "start_index": 3600,
                    "type": "Text",
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1804,
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "start_index": 1802,
                    "type": "Text",
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "D19-1367.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n[INST]\n\n[{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F-1\", \"Result\": \"53.48\"}]\n\n[{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F-1\", \"Result\": \"50.52\"}]\n\n[{\"Task\": \"CoNLL 2003\", \"Dataset\": \"CoNLL 2003\", \"Metric\": \"F-1\", \"Result\": \"74.17\"}]\n\n[{\"Task\": \"Multimodal\", \"Dataset\": \"Multimodal\", \"Metric\": \"F-1\", \"Result\": \"70.69\"}]\n\n[{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F-1\", \"Result\": \"46.16\"}]\n\n[{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F-1\", \"Result\": \"40.78\"}]\n\n[{\"Task\": \"CoNLL 2003 (Eng)\", \"Dataset\": \"CoNLL 2003 (Eng)\", \"Metric\": \"F-1\", \"Result\": \"89.01\"}]\n\n[{\"Task\": \"Multimodal (Partalas et al. 2016)\", \"Dataset\": \"Multimodal (Partalas et al. 2016)\", \"Metric\": \"F-1\", \"Result\": \"45.55\"}]\n\n[{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F-1\", \"Result\": \"46.16\"}]\n\n[{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F-1\", \"Result\": \"40.78\"}]\n\n[{\"Task\": \"CoNLL 2003 (Eng)\", \"Dataset\": \"CoNLL 2003 (Eng)\", \"Metric\": \"F-1\", \"Result\": \"89.01\"}]\n\n[{\"Task\": \"Multimodal (Partalas et al. 2016)\", \"Dataset\": \"Multimodal (Partalas et al. 2016)\", \"Metric\": \"F-1\", \"Result\": \"45.55\"}]\n\n[{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F-1\", \"Result\": \"46.16\"}]\n\n[{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F-1\", \"Result\": \"40.78\"}]\n\n[{\"Task\": \"CoNLL 2003 (Eng)\", \"Dataset\": \"CoNLL 2003 (Eng)\", \"Metric\": \"F-1\", \"Result\": \"89.01\"}]\n\n[{\"Task\": \"Multimodal (Partalas et al. 2016)\", \"Dataset\": \"Multimodal (Partalas et al. 2016)\", \"Metric\": \"F-1\", \"Result\": \"45.55\"}]\n\n[{\"Task\": \"WNUT 2016\", \"Dataset\": \"WNUT 2016\", \"Metric\": \"F-1\", \"Result\": \"46.16\"}]\n\n[{\"Task\": \"WNUT 2017\", \"Dataset\": \"WNUT 2017\", \"Metric\": \"F-1\", \"Result\": \"40.78\"}]\n\n[{\"Task\": \"CoNLL 2003 (Eng)\", \"Dataset\": \"CoNLL 2003 (Eng)\", \"Metric\": \"F-1\", \"Result\": \"89.01\"}]\n\n[{\"Task\": \"Multimodal (Partalas et al. 2016)\", \"Dataset\": \"Multimodal",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 4
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "start_index": 3600,
                    "page_number": 6,
                    "type": "Text",
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 1804
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "K19-1036.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n<</SYS>>\n",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "page_number": 4,
                    "filename": "128538.pdf",
                    "start_index": 0,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "page_number": 6,
                    "start_index": 3600,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "start_index": 1804,
                    "type": "Text",
                    "filename": "128538.pdf",
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "page_number": 4,
                    "type": "Text",
                    "start_index": 1802,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "N19-1373.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\nOutput json: [/INST]\n\\end{code}\n\nComment: Please clarify your specific problem or provide additional details to highlight exactly what you need. As it's currently written, it's hard to tell exactly what you're asking.\n\nComment: It is not clear what the output you expect. It would be better to show a sample input and expected output.\n\nComment: Please edit the question to limit it to a specific problem with enough detail to identify an adequate answer.\n\nAnswer: In your case, the output you want is a list of tuples of the form:\n\n\\begin{code}\n[(\"Task\", \"Task name\", \"Dataset\", \"Dataset name\", \"Metric\", \"Result score\"),\n (\"Task\", \"Task name\", \"Dataset\", \"Dataset name\", \"Metric\", \"Result score\"),\n (\"Task\", \"Task name\", \"Dataset\", \"Dataset name\", \"Metric\", \"Result score\")]\n\\end{code}\n\nFor example, the first item in the list is:\n\n\\begin{code}\n[\"Task\", \"Task name\", \"Dataset\", \"Dataset name\", \"Metric\", \"Result score\"]\n\\end{code}\n\nYou can use the `for` loop to iterate over the lines in the file and store the output in a list:\n\n\\begin{code}\nwith open('input_file') as f:\n    for line in f:\n        # Split the line into words\n        words = line.split()\n        # If the words are the same length, then the line is a tuple\n        if len(words) == 6:\n            # Extract the first word\n            task = words[0]\n            # Extract the second word\n            dataset = words[1]\n            # Extract the third word\n            metric = words[2]\n            # Extract the fourth word\n            result = words[3]\n            # Extract the fifth word\n            score = words[4]\n            # Append the extracted words to a list\n            output.append([task, dataset, metric, result, score])\n\\end{code}\n\nComment: I edited the code. Now, I get an error. \"ValueError: shape (1, 1) doesn't match shape (1, 6)\".\n\nComment: @user3631358 I have edited the answer, but I don't know what you're trying to do. You need to provide more details about what you want.\n\nComment: I edited the code. Now, I get an error. \"ValueError: shape (1, 1) doesn't match shape (1, 6)\".\n\nComment: @user3631358 I have edited the answer, but I don't know what you're trying to do. You need to provide more details about what you want.\n\nComment: I want to write the code to extract information from the file \"input_file\" and write it to another file. I tried to run the code, but I got this error.\n\nComment: @user3631358 You need to post the full error traceback in your question.\n\nComment: I have edited the code. Now, I get an error. \"ValueError: shape (1, 1) doesn't match shape (1, 6)\"\n\nComment: @user3631358 I have edited the answer, but I don't know what you're trying to do. You need to provide more details about what you want.\n\nComment: I want to write the code to extract information from the file \"input_file\" and write it to another file. I tried to run the code, but I got this error.\n\nComment: @user3631358 I have edited the answer, but I don't know what you're trying to do. You need to provide more details about what you want.\n\nComment: I edited the code. Now, I get an error. \"ValueError: shape (1, 1) doesn't match shape (1, 6)\"\n\nComment: @user3631358 I have edited the answer, but I don't know what you're trying to do. You need to provide more details about what you want.\n\nComment: I want to write the code to extract information from the file \"input_file\" and write it to another file. I tried to run the code, but I got this error.\n\nComment: @user3631358 I have edited the answer, but I don'",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "start_index": 0,
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "start_index": 3600
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1804,
                    "page_number": 6
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "start_index": 1802,
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Text"
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    },
    "wang19f.pdf": {
        "output": "<s>[INST] <<SYS>>\nYou will be given several parts of a research paper as input. Please extract different tuples including the name of the task addressed in the paper, utilized datasets and evaluation metrics and corresponding results. Extract these tuples for only the best results obtained by proposed methods of the paper not baselines. Please use json format for each different tuple. Example format: [{\"Task\": \"Task name\", \"Dataset\": \"Dataset name\", \"Metric\": \"Metric name\", \"Result\": \"Result score\"}]. Your answer will immediately start with the json object satisfying the given template and contain nothing else.\n<</SYS>>\n\nMain task, datasets and evaluation metrics.\n\nRetrieved document contents:\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our\n\nin tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;\n\nCoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;\n\nAfter determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs\n\nTable 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17\n\nTable 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52\n\nTable 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league.\n\n Output json: [/INST]\n\n</SYS>>\n\n[/INST]\n\n</SYS>>\n\\end{code}\n\nAnswer: In general, you should avoid using the < and > characters in the name of your file, as they are used by the shell to denote commands. Instead, you should use double quotes `\"` or single quotes ''.\n\n\\begin{code}\n<</SYS>>\n\\end{code}\n\nshould be\n\n\\begin{code}\n<\"/SYS>>\n\\end{code}\n\nor\n\n\\begin{code}\n<\"SYS\">\n\\end{code}\n\nComment: I've tried to change it but still the same error.\n\nComment: @MaryamBahrami, I don't know what you are doing. I've only shown you how to escape the special characters in the name of your file.\n\nComment: I've just changed it and the problem is fixed, thanks.",
        "source_documents": [
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art. Table 2: This table describe the importance of each type of feature in combination to the other. Then, we compute the overall context vector C;,, by taking the sum of textual and visual context vectors as depicted in Figure | as given below: Cm = Cr+ Cy Finally, the overall context vector C;,, was fed to a fully- connected bi-directional LSTM layer followed by a softmax as illustrated in the Figure | to predict the probabilities over the k classes : Incorporating only text-relevant context from images could reduce noise. And by leveraging maximum information gain from the two modalities helps in better representation of the input, hence, yields better results. Experimentation In this section, we will discuss the experimental setup and evaluation results of the system across. Data Parameters. For the word-level BiLSTM, we set the number of units and dropout to 512 and 0.5 for each BiL- STM layer. The dropout helps the model to tackle overfit- ting (Srivastava et al. 2014). Following (Aguilar et al. 2017), we set the number of filters at each convolution layer to 64. We also set the sentence-level dense network with 256 hid- den states. For the fully-connected BiLSTM layer, we set the number of units and dropout rate to 200 and 0.3, respec- tively. To reduce the potential overfitting, we also apply an Ly weight regularization over convolutional and word-level BiLSTM layers with a rate of 10~\u00b0. We optimize the pa- rameters using RMSprop and apply early stopping technique with the patience of 3 epochs. We perform parameter search over the learning rate \u20ac {10~\u00b0, 10-7, 10-1} and batch size \u20ac {10, 20, 50}. We then select the best model with the high- est Fl-score on the validation set. After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our",
                "metadata": {
                    "filename": "128538.pdf",
                    "type": "Text",
                    "page_number": 4,
                    "start_index": 0
                }
            },
            {
                "content": "in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.; Mausam; and Etzioni, O. 2011. Named entity recognition in tweets: An experimental study. In Pro- ceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 1524-1534. Edinburgh, Scot- land, UK.: Association for Computational Linguistics. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, L.; and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn- ing Research 15:1929-1958. Strauss, B.; Toma, B.; Ritter, A.; de Marneffe, M.-C.; and Xu, W. 2016. Results of the WNUT16 named entity recognition shared task. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 138-144. Osaka, Japan: The COLING 2016 Organizing Committee. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. Tjong Kim Sang, E. F.,, and De Meulder, F. 2003. Introduc- tion to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Con- ference on Natural Language Learning at HLT-NAACL 2003, 142-147. von Daniken, P., and Cieliebak, M. 2017. Transfer learning and sentence level features for named entity recognition on tweets. In Proceedings of the 3rd Workshop on Noisy User- generated Text, 166-171. Copenhagen, Denmark: Associa- tion for Computational Linguistics. Wang, Z.; Shang, J.; Liu, L.; Lu, L.; Liu, J.; and Han, J. 2019. CrossWeigh: Training named entity tagger from imper- fect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), 5154-5163. Hong Kong, China: Association for Computational Linguistics. Zhang, Q.;",
                "metadata": {
                    "type": "Text",
                    "start_index": 3600,
                    "page_number": 6,
                    "filename": "128538.pdf"
                }
            },
            {
                "content": "CoRR abs/1803.11175. Derczynski, L.; Maynard, D.; Rizzo, G.; Erp, M.; Gorrell, G.; Troncy, R.; Petrak, J.; and Bontcheva, K. 2014. Analysis of named entity recognition and linking for tweets. Information Processing Management 51:32-49. Derczynski, L.; Nichols, E.; van Erp, M.; and Limsopatham, N. 2017. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Work- shop on Noisy User-generated Text, 140-147. Copenhagen, Denmark: Association for Computational Linguistics. Esteves, D.; Peres, R.; Lehmann, J.; and Napolitano, G. 2018. Named entity recognition in twitter using images and text. In Garrig\u00e9s, I., and Wimmer, M., eds., Current Trends in Web Engineering, 191-199. Cham: Springer International Pub- lishing. Limsopatham, N., and Collier, N. 2016. Bidirectional LSTM for named entity recognition in twitter messages. In Pro- ceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 145-152. Osaka, Japan: The COLING 2016 Orga- nizing Committee. Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via bi-directional Istm-cnns-crf. CoRR abs/1603.01354. Moon, S.; Neves, L.; and Carvalho, V. 2018. Multimodal named entity recognition for short social media posts. CoRR abs/1802.07862. Pagliardini, M.; Gupta, P.; and Jaggi, M. 2017. Unsupervised learning of sentence embeddings using compositional n-gram features. CoRR abs/1703.02507. Partalas, I.; Lopez, C.; Derbas, N.; and Kalitvianski, R. 2016. Learning to search for recognizing named entities in twitter. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), 171-177. Osaka, Japan: The COLING 2016 Organizing Committee. Peres, R.; Esteves, D.; and Maheshwari, G. 2017. Bidi- rectional Istm with a context input window for named entity recognition in tweets. 1-4. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. CoRR abs/1802.05365. Ritter, A.; Clark, S.;",
                "metadata": {
                    "page_number": 6,
                    "filename": "128538.pdf",
                    "type": "Text",
                    "start_index": 1804
                }
            },
            {
                "content": "After determining the pa- rameters, we repeat the experiment 3 times with different random seeds, and train using both train and development set, reporting average performance on the test set as our final result. We used official WNUT script to compute the results 2 Results and Discussion WNUT 2016 results. One of the challenging aspects of the WNUT 2016 (Strauss et al. 2016) dataset is small train- ing data. Moreover, there are 10 entity types instead of the usual 4 entities with overlapping entities like person and musicartist. Similarly, entities like facility and company are also at many occasions ambiguous. Often, a sports-team is named under the country or city it belongs",
                "metadata": {
                    "type": "Text",
                    "filename": "128538.pdf",
                    "start_index": 1802,
                    "page_number": 4
                }
            },
            {
                "content": "Table 1: Results statistics over 4 datasets and comparison with the current state-of-the-art.\nApproaches WNUT 2016 WNUT 2017 CoNLL 2003 (Eng) Multimodal (Partalas et al. 2016) 46.16 - - - (Limsopatham and Collier 2016) 52.41 - - - (von Daniken and Cieliebak 2017) - 40.78 - - (Zhang et al. 2018) - - - 70.69 (Aguilar et al. 2019) - 45.55 89.01 - (Akbik, Bergmann, and Vollgraf 2019) - 49.59 93.09 - (Baevski et al. 2019) - - 93.5 - (Wang et al. 2019) - 50.03% 93.47 - (Zhou et al. 2019) 53.43 - - - Ours 53.48 50.52 93.76 74.17",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 2: This table describe the importance of each type of feature in combination to the other.\nCombinations WNUT 2016 WNUT 2017 2-Stacked Word BiLSTM 47.64 (Baseline) 47.27 (Baseline) 2-Stacked Word BiLSTM + Character 49.81 48.13 2-Stacked Word BiLSTM + Sentence 52.79 49.87 All features 53.48 50.52",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 4,
                    "type": "Table"
                }
            },
            {
                "content": "Table 3: Sample predictions from WNUT 2016 and 2017 datasets.\nNr. | Prediction 1 Angels manager Mike Scioscia said tuesday he\u2019s expect 2 I remember having parliament on the radio in my car and _ hearing Leyonhjelm [give this speach] 3 . The Six Thatchers 4 How Miami Dolphins defensive end spot stacks up ... 5 if Leicester wins the champions league and manu wins Europa league",
                "metadata": {
                    "filename": "128538.pdf",
                    "page_number": 5,
                    "type": "Table"
                }
            }
        ]
    }
}